This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2024-11-30T06:37:30.072Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Repository structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

For more information about Repomix, visit: https://github.com/yamadashy/repomix

================================================================
Repository Structure
================================================================
.github/
  workflows/
    presumit.yml
analysis/
  analyzer_test.go
  analyzer.go
  filters.go
document/
  document_test.go
  document.go
elastic/
  api_test.go
  api.go
  dsl_test.go
  dsl.go
index/
  index_management_test.go
  index_test.go
  index_txlog_test.go
  index.go
logger/
  logger_test.go
  logger.go
query/
  parser_test.go
  parser.go
  query_test.go
  query.go
router/
  bulk.go
  router_test.go
  router.go
  validation.go
search/
  es_formatter.go
  query_executor_test.go
  query_executor.go
  search_test.go
  search.go
storage/
  storage_test.go
  storage.go
txlog/
  txlog_test.go
  txlog.go
.dockerignore
.gitignore
.repomixignore
Dockerfile
elasticsearch_api_ref.md
fix_tests.sh
go.mod
main.go
Makefile
README.md
steps.txt

================================================================
Repository Files
================================================================

================
File: .github/workflows/presumit.yml
================
name: Presubmit.ai

permissions:
  contents: read
  pull-requests: write
  issues: write

concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

on:
  pull_request:
    paths-ignore:
      - '**.md'
      - 'docs/**'
  pull_request_review_comment:
    types: [created]
    paths-ignore:
      - '**.md'
      - 'docs/**'

jobs:
  review:
    runs-on: ubuntu-latest
    steps:
      - uses: presubmit/ai-reviewer@latest
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
          LLM_API_KEY: ${{ secrets.LLM_API_KEY }}
          LLM_MODEL: "claude-3-5-sonnet-20241022"

================
File: analysis/analyzer_test.go
================
package analysis

import (
	"reflect"
	"testing"
)

func TestStandardAnalyzer(t *testing.T) {
	analyzer := NewStandardAnalyzer()

	tests := []struct {
		name     string
		input    string
		expected []Token
	}{
		{
			name:     "Empty string",
			input:    "",
			expected: []Token{},
		},
		{
			name:     "Only spaces",
			input:    "   ",
			expected: []Token{},
		},
		{
			name:  "Simple text",
			input: "Hello World",
			expected: []Token{
				{Text: "hello", Position: 0, StartByte: 0, EndByte: 5},
				{Text: "world", Position: 1, StartByte: 6, EndByte: 11},
			},
		},
		{
			name:  "Text with punctuation",
			input: "Hello, World!",
			expected: []Token{
				{Text: "hello", Position: 0, StartByte: 0, EndByte: 5},
				{Text: "world", Position: 1, StartByte: 7, EndByte: 12},
			},
		},
		{
			name:  "Multiple spaces",
			input: "Hello    World",
			expected: []Token{
				{Text: "hello", Position: 0, StartByte: 0, EndByte: 5},
				{Text: "world", Position: 1, StartByte: 9, EndByte: 14},
			},
		},
		{
			name:  "Mixed case",
			input: "HeLLo WoRLD",
			expected: []Token{
				{Text: "hello", Position: 0, StartByte: 0, EndByte: 5},
				{Text: "world", Position: 1, StartByte: 6, EndByte: 11},
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := analyzer.Analyze(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Analyze() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestCustomAnalyzer(t *testing.T) {
	filters := []TokenFilter{
		NewLowercaseFilter(),
		NewPunctuationFilter(),
		NewTrimSpaceFilter(),
	}
	analyzer := NewCustomAnalyzer(filters)

	tests := []struct {
		name     string
		input    string
		expected []Token
	}{
		{
			name:     "Empty string",
			input:    "",
			expected: []Token{},
		},
		{
			name:  "Complex text",
			input: "Hello, World! This is a TEST.",
			expected: []Token{
				{Text: "hello", Position: 0, StartByte: 0, EndByte: 5},
				{Text: "world", Position: 1, StartByte: 7, EndByte: 12},
				{Text: "this", Position: 2, StartByte: 14, EndByte: 18},
				{Text: "is", Position: 3, StartByte: 19, EndByte: 21},
				{Text: "a", Position: 4, StartByte: 22, EndByte: 23},
				{Text: "test", Position: 5, StartByte: 24, EndByte: 28},
			},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := analyzer.Analyze(tt.input)
			if !reflect.DeepEqual(got, tt.expected) {
				t.Errorf("Analyze() = %v, want %v", got, tt.expected)
			}
		})
	}
}

func TestFilters(t *testing.T) {
	tests := []struct {
		name     string
		filter   TokenFilter
		input    string
		expected string
	}{
		{
			name:     "Lowercase filter",
			filter:   NewLowercaseFilter(),
			input:    "HeLLo",
			expected: "hello",
		},
		{
			name:     "Punctuation filter",
			filter:   NewPunctuationFilter(),
			input:    "Hello, World!",
			expected: "Hello World",
		},
		{
			name:     "Trim space filter",
			filter:   NewTrimSpaceFilter(),
			input:    "  hello  ",
			expected: "hello",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got := tt.filter.Filter(tt.input)
			if got != tt.expected {
				t.Errorf("Filter() = %v, want %v", got, tt.expected)
			}
		})
	}
}

================
File: analysis/analyzer.go
================
package analysis

import (
	"strings"
	"unicode"
)

// Token represents a single token in the text
type Token struct {
	Text      string
	Position  int  // Position in the original text
	StartByte int  // Start byte offset
	EndByte   int  // End byte offset
}

// Analyzer defines the interface for text analysis
type Analyzer interface {
	Analyze(text string) []Token
}

// StandardAnalyzer implements a basic analyzer that splits on whitespace,
// converts to lowercase, and removes punctuation
type StandardAnalyzer struct{}

// NewStandardAnalyzer creates a new StandardAnalyzer
func NewStandardAnalyzer() *StandardAnalyzer {
	return &StandardAnalyzer{}
}

// Analyze performs the text analysis process:
// 1. Splits text into tokens based on whitespace
// 2. Converts tokens to lowercase
// 3. Removes punctuation
func (a *StandardAnalyzer) Analyze(text string) []Token {
	if len(strings.TrimSpace(text)) == 0 {
		return []Token{}
	}

	var tokens []Token
	position := 0
	startByte := 0

	// Split on whitespace first
	words := strings.Fields(text)

	for _, word := range words {
		// Skip empty words
		if len(word) == 0 {
			continue
		}

		// Process the word
		cleanWord := strings.Map(func(r rune) rune {
			if unicode.IsPunct(r) || unicode.IsSymbol(r) {
				return -1 // Remove punctuation and symbols
			}
			return unicode.ToLower(r)
		}, word)

		// Skip if the word became empty after cleaning
		if len(cleanWord) == 0 {
			continue
		}

		// Calculate byte offsets
		wordStartByte := startByte
		if startByte > 0 {
			// Find the start of the current word in the original text
			for i := startByte; i < len(text); i++ {
				if strings.HasPrefix(strings.ToLower(text[i:]), strings.ToLower(word)) {
					wordStartByte = i
					break
				}
			}
		}
		wordEndByte := wordStartByte + len(cleanWord)

		tokens = append(tokens, Token{
			Text:      cleanWord,
			Position:  position,
			StartByte: wordStartByte,
			EndByte:   wordEndByte,
		})

		position++
		startByte = wordStartByte + len(word)
	}

	return tokens
}

// CustomAnalyzer allows for configurable analysis with custom filters
type CustomAnalyzer struct {
	filters []TokenFilter
}

// TokenFilter defines an interface for token filtering
type TokenFilter interface {
	Filter(token string) string
}

// NewCustomAnalyzer creates a new CustomAnalyzer with the specified filters
func NewCustomAnalyzer(filters []TokenFilter) *CustomAnalyzer {
	return &CustomAnalyzer{
		filters: filters,
	}
}

// Analyze performs text analysis using the configured filters
func (a *CustomAnalyzer) Analyze(text string) []Token {
	if len(strings.TrimSpace(text)) == 0 {
		return []Token{}
	}

	var tokens []Token
	position := 0
	startByte := 0

	words := strings.Fields(text)

	for _, word := range words {
		if len(word) == 0 {
			continue
		}

		processedWord := word
		for _, filter := range a.filters {
			processedWord = filter.Filter(processedWord)
		}

		if len(processedWord) == 0 {
			continue
		}

		// Calculate byte offsets
		wordStartByte := startByte
		if startByte > 0 {
			// Find the start of the current word in the original text
			for i := startByte; i < len(text); i++ {
				if strings.HasPrefix(strings.ToLower(text[i:]), strings.ToLower(word)) {
					wordStartByte = i
					break
				}
			}
		}
		wordEndByte := wordStartByte + len(processedWord)

		tokens = append(tokens, Token{
			Text:      processedWord,
			Position:  position,
			StartByte: wordStartByte,
			EndByte:   wordEndByte,
		})

		position++
		startByte = wordStartByte + len(word)
	}

	return tokens
}

================
File: analysis/filters.go
================
package analysis

import (
	"strings"
	"unicode"
)

// LowercaseFilter converts tokens to lowercase
type LowercaseFilter struct{}

func NewLowercaseFilter() *LowercaseFilter {
	return &LowercaseFilter{}
}

func (f *LowercaseFilter) Filter(token string) string {
	return strings.ToLower(token)
}

// PunctuationFilter removes punctuation from tokens
type PunctuationFilter struct{}

func NewPunctuationFilter() *PunctuationFilter {
	return &PunctuationFilter{}
}

func (f *PunctuationFilter) Filter(token string) string {
	return strings.Map(func(r rune) rune {
		if unicode.IsPunct(r) || unicode.IsSymbol(r) {
			return -1
		}
		return r
	}, token)
}

// TrimSpaceFilter removes leading and trailing whitespace
type TrimSpaceFilter struct{}

func NewTrimSpaceFilter() *TrimSpaceFilter {
	return &TrimSpaceFilter{}
}

func (f *TrimSpaceFilter) Filter(token string) string {
	return strings.TrimSpace(token)
}

================
File: document/document_test.go
================
package document

import (
	"testing"
)

func TestNewDocument(t *testing.T) {
	doc := NewDocument()
	if doc == nil {
		t.Error("NewDocument() returned nil")
	}
	if doc.fields == nil {
		t.Error("Document fields map not initialized")
	}
}

func TestAddField(t *testing.T) {
	tests := []struct {
		name      string
		fieldName string
		value     interface{}
		wantErr   bool
	}{
		{"string field", "title", "test document", false},
		{"integer field", "count", 42, false},
		{"float field", "score", 3.14, false},
		{"invalid type", "invalid", []string{"test"}, true},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			doc := NewDocument()
			err := doc.AddField(tt.fieldName, tt.value)
			
			if (err != nil) != tt.wantErr {
				t.Errorf("AddField() error = %v, wantErr %v", err, tt.wantErr)
				return
			}

			if !tt.wantErr {
				field, err := doc.GetField(tt.fieldName)
				if err != nil {
					t.Errorf("GetField() error = %v", err)
					return
				}

				if field.Name != tt.fieldName {
					t.Errorf("Field name = %v, want %v", field.Name, tt.fieldName)
				}

				if field.Value != tt.value {
					t.Errorf("Field value = %v, want %v", field.Value, tt.value)
				}
			}
		})
	}
}

func TestGetField(t *testing.T) {
	doc := NewDocument()
	fieldName := "test"
	fieldValue := "test value"

	// Test getting non-existent field
	_, err := doc.GetField(fieldName)
	if err == nil {
		t.Error("GetField() should return error for non-existent field")
	}

	// Add field and test retrieval
	err = doc.AddField(fieldName, fieldValue)
	if err != nil {
		t.Errorf("AddField() error = %v", err)
	}

	field, err := doc.GetField(fieldName)
	if err != nil {
		t.Errorf("GetField() error = %v", err)
	}

	if field.Name != fieldName {
		t.Errorf("Field name = %v, want %v", field.Name, fieldName)
	}

	if field.Value != fieldValue {
		t.Errorf("Field value = %v, want %v", field.Value, fieldValue)
	}
}

================
File: document/document.go
================
package document

import (
	"encoding/json"
	"fmt"
	"sync"
	"time"
)

// FieldType represents the type of a field value
type FieldType int

const (
	// StringType represents string field values
	StringType FieldType = iota
	// IntType represents integer field values
	IntType
	// FloatType represents floating-point field values
	FloatType
	// TimeType represents time.Time field values
	TimeType
)

// Field represents a single field in a document
type Field struct {
	Name     string
	Type     FieldType
	Value    interface{}
}

// Document represents a searchable document with multiple fields
type Document struct {
	mu     sync.RWMutex
	ID     int
	fields map[string]Field
}

// NewDocument creates a new Document instance
func NewDocument() *Document {
	return &Document{
		fields: make(map[string]Field),
	}
}

// AddField adds a new field to the document
func (d *Document) AddField(name string, value interface{}) error {
	d.mu.Lock()
	defer d.mu.Unlock()

	fieldType, err := determineFieldType(value)
	if err != nil {
		return fmt.Errorf("failed to add field: %w", err)
	}

	d.fields[name] = Field{
		Name:  name,
		Type:  fieldType,
		Value: value,
	}
	return nil
}

// GetField retrieves a field by name
func (d *Document) GetField(name string) (Field, error) {
	d.mu.RLock()
	defer d.mu.RUnlock()

	field, exists := d.fields[name]
	if !exists {
		return Field{}, fmt.Errorf("field %s not found", name)
	}
	return field, nil
}

// GetFields returns a map of all fields in the document
func (d *Document) GetFields() map[string]Field {
	d.mu.RLock()
	defer d.mu.RUnlock()

	// Create a copy of the fields map to prevent concurrent modification
	fields := make(map[string]Field, len(d.fields))
	for k, v := range d.fields {
		fields[k] = v
	}
	return fields
}

// determineFieldType infers the FieldType from a value
func determineFieldType(value interface{}) (FieldType, error) {
	switch value.(type) {
	case string:
		return StringType, nil
	case int, int8, int16, int32, int64, uint, uint8, uint16, uint32, uint64:
		return IntType, nil
	case float32, float64:
		return FloatType, nil
	case time.Time:
		return TimeType, nil
	default:
		return 0, fmt.Errorf("unsupported field type for value: %v", value)
	}
}

// MarshalJSON implements json.Marshaler interface
func (d *Document) MarshalJSON() ([]byte, error) {
	d.mu.RLock()
	defer d.mu.RUnlock()

	// Create a map of field name to field value for JSON serialization
	fields := make(map[string]interface{})
	for name, field := range d.fields {
		fields[name] = field.Value
	}

	return json.Marshal(fields)
}

// UnmarshalJSON implements json.Unmarshaler interface
func (d *Document) UnmarshalJSON(data []byte) error {
	d.mu.Lock()
	defer d.mu.Unlock()

	// Initialize fields map if not already initialized
	if d.fields == nil {
		d.fields = make(map[string]Field)
	}

	// Unmarshal into a temporary map
	var fields map[string]interface{}
	if err := json.Unmarshal(data, &fields); err != nil {
		return err
	}

	// Convert each field into a Document Field
	for name, value := range fields {
		// Determine field type based on the value
		var fieldType FieldType
		switch value.(type) {
		case string:
			fieldType = StringType
		case float64:
			fieldType = FloatType
		case int, int64:
			fieldType = IntType
		default:
			return fmt.Errorf("unsupported field type for field %s", name)
		}

		// Add the field to the document
		d.fields[name] = Field{
			Name:  name,
			Type:  fieldType,
			Value: value,
		}
	}

	return nil
}

================
File: elastic/api_test.go
================
package elastic

import (
	"context"
	"encoding/json"
	"fmt"
	"sync"
	"testing"
)

// MockAPI implements the API interface for testing
type MockAPI struct {
	documents map[string]*Document
	mu        sync.RWMutex
}

func NewMockAPI() *MockAPI {
	return &MockAPI{
		documents: make(map[string]*Document),
	}
}

func (m *MockAPI) Index(ctx context.Context, doc *Document) (*Document, error) {
	if doc.Source == nil || len(doc.Source) == 0 {
		return nil, fmt.Errorf("document source is empty")
	}
	
	// Check for malformed document
	if _, hasSource := doc.Source["_source"]; hasSource {
		return nil, fmt.Errorf("malformed document: contains reserved field '_source'")
	}
	
	m.mu.Lock()
	defer m.mu.Unlock()
	key := doc.Index + ":" + doc.ID
	m.documents[key] = doc
	return doc, nil
}

func (m *MockAPI) Get(ctx context.Context, index, id string) (*Document, error) {
	m.mu.RLock()
	defer m.mu.RUnlock()
	key := index + ":" + id
	if doc, ok := m.documents[key]; ok {
		return doc, nil
	}
	return nil, nil
}

func (m *MockAPI) Update(ctx context.Context, doc *Document) (*Document, error) {
	return m.Index(ctx, doc)
}

func (m *MockAPI) Delete(ctx context.Context, index, id string) error {
	m.mu.Lock()
	defer m.mu.Unlock()
	key := index + ":" + id
	delete(m.documents, key)
	return nil
}

func (m *MockAPI) Search(ctx context.Context, query map[string]interface{}) (*SearchResponse, error) {
	if query == nil {
		query = map[string]interface{}{
			"query": map[string]interface{}{
				"match_all": map[string]interface{}{},
			},
		}
	}
	
	// If no query specified, treat as match_all
	if _, hasQuery := query["query"]; !hasQuery {
		query["query"] = map[string]interface{}{
			"match_all": map[string]interface{}{},
		}
	}

	// Validate query structure
	queryObj, ok := query["query"].(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid query format: 'query' must be an object")
	}

	// Check for empty query object
	if len(queryObj) == 0 {
		return nil, fmt.Errorf("invalid query format: empty query object")
	}

	// Must have exactly one query type
	validTypes := []string{"match", "match_all", "term", "range", "bool"}
	foundType := ""
	for _, qType := range validTypes {
		if queryValue, exists := queryObj[qType]; exists {
			if foundType != "" {
				return nil, fmt.Errorf("invalid query format: multiple query types specified")
			}
			// Validate the query value is a map
			if _, ok := queryValue.(map[string]interface{}); !ok {
				return nil, fmt.Errorf("invalid query format: %s value must be an object", qType)
			}
			foundType = qType
		}
	}
	
	// Check if any invalid query types are present
	for qType := range queryObj {
		isValid := false
		for _, validType := range validTypes {
			if qType == validType {
				isValid = true
				break
			}
		}
		if !isValid {
			return nil, fmt.Errorf("invalid query format: unsupported query type '%s'", qType)
		}
	}

	if foundType == "" {
		return nil, fmt.Errorf("invalid query format: no valid query type found")
	}
	m.mu.RLock()
	defer m.mu.RUnlock()
	// Simple mock implementation that returns all documents
	hits := make([]Document, 0)
	for _, doc := range m.documents {
		hits = append(hits, *doc)
	}
	return &SearchResponse{
		Took: 1,
		Hits: SearchHits{
			Total: Total{Value: int64(len(hits)), Relation: TotalRelationEq},
			Hits:  hits,
		},
	}, nil
}

func (m *MockAPI) MultiSearch(ctx context.Context, queries []map[string]interface{}) ([]*SearchResponse, error) {
	responses := make([]*SearchResponse, len(queries))
	for i, query := range queries {
		resp, err := m.Search(ctx, query)
		if err != nil {
			return nil, err
		}
		responses[i] = resp
	}
	return responses, nil
}

func (m *MockAPI) ListIndices(ctx context.Context) ([]string, error) {
	m.mu.RLock()
	defer m.mu.RUnlock()
	indices := make(map[string]bool)
	for _, doc := range m.documents {
		indices[doc.Index] = true
	}
	result := make([]string, 0, len(indices))
	for index := range indices {
		result = append(result, index)
	}
	return result, nil
}

func (m *MockAPI) Bulk(ctx context.Context, operations []json.RawMessage) error {
	return nil
}

func TestElasticAPI(t *testing.T) {
	ctx := context.Background()
	api := NewMockAPI()

	// Test document operations
	doc := &Document{
		Index:  "test",
		ID:     "1",
		Source: map[string]interface{}{"field": "value"},
	}

	// Test Index
	indexed, err := api.Index(ctx, doc)
	if err != nil {
		t.Fatalf("Index failed: %v", err)
	}
	if indexed.ID != doc.ID {
		t.Errorf("Expected ID %s, got %s", doc.ID, indexed.ID)
	}

	// Test Get
	retrieved, err := api.Get(ctx, "test", "1")
	if err != nil {
		t.Fatalf("Get failed: %v", err)
	}
	if retrieved.ID != doc.ID {
		t.Errorf("Expected ID %s, got %s", doc.ID, retrieved.ID)
	}

	// Test Search
	searchResp, err := api.Search(ctx, map[string]interface{}{
		"query": map[string]interface{}{
			"match_all": map[string]interface{}{},
		},
	})
	if err != nil {
		t.Fatalf("Search failed: %v", err)
	}
	if searchResp.Hits.Total.Value != 1 {
		t.Errorf("Expected 1 hit, got %d", searchResp.Hits.Total.Value)
	}

	// Test Delete
	err = api.Delete(ctx, "test", "1")
	if err != nil {
		t.Fatalf("Delete failed: %v", err)
	}

	// Verify deletion
	deleted, err := api.Get(ctx, "test", "1")
	if err != nil {
		t.Fatalf("Get after delete failed: %v", err)
	}
	if deleted != nil {
		t.Error("Document still exists after deletion")
	}
}

func TestTotalRelation(t *testing.T) {
	tests := []struct {
		name     string
		relation TotalRelation
		want     bool
	}{
		{
			name:     "Valid eq relation",
			relation: TotalRelationEq,
			want:     true,
		},
		{
			name:     "Valid gte relation",
			relation: TotalRelationGte,
			want:     true,
		},
		{
			name:     "Invalid relation",
			relation: TotalRelation("invalid"),
			want:     false,
		},
		{
			name:     "Empty relation",
			relation: TotalRelation(""),
			want:     false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := tt.relation.IsValid(); got != tt.want {
				t.Errorf("TotalRelation.IsValid() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestElasticSearchCompatibility(t *testing.T) {
	ctx := context.Background()
	api := NewMockAPI()

	t.Run("ElasticSearch Request Format", func(t *testing.T) {
		// Test standard ElasticSearch query format
		query := map[string]interface{}{
			"query": map[string]interface{}{
				"match": map[string]interface{}{
					"field": "value",
				},
			},
			"size": 10,
			"from": 0,
		}
		
		resp, err := api.Search(ctx, query)
		if err != nil {
			t.Fatalf("Search failed: %v", err)
		}
		
		// Validate ElasticSearch response format
		if resp.Took == 0 {
			t.Error("Expected non-zero took value")
		}
		if resp.Hits.Total.Relation != TotalRelationEq {
			t.Error("Expected eq relation for total")
		}
	})

	t.Run("Concurrent Operations", func(t *testing.T) {
		const numOps = 100
		errCh := make(chan error, numOps)
		doneCh := make(chan bool, numOps)

		for i := 0; i < numOps; i++ {
			go func(idx int) {
				doc := &Document{
					Index:  "concurrent-test",
					ID:     fmt.Sprintf("doc-%d", idx),
					Source: map[string]interface{}{"value": idx},
				}
				_, err := api.Index(ctx, doc)
				if err != nil {
					errCh <- err
					return
				}
				doneCh <- true
			}(i)
		}

		// Wait for all operations
		for i := 0; i < numOps; i++ {
			select {
			case err := <-errCh:
				t.Errorf("Concurrent operation failed: %v", err)
			case <-doneCh:
				// Operation successful
			}
		}
	})

	t.Run("Large Payload", func(t *testing.T) {
		// Create a large document (>1MB)
		largeData := make([]string, 100000)
		for i := range largeData {
			largeData[i] = "test data string that takes up space"
		}

		doc := &Document{
			Index:  "large-test",
			ID:     "large-1",
			Source: map[string]interface{}{"data": largeData},
		}

		indexed, err := api.Index(ctx, doc)
		if err != nil {
			t.Fatalf("Large document indexing failed: %v", err)
		}

		// Verify the indexed document has the correct ID
		if indexed.ID != "large-1" {
			t.Errorf("Expected document ID 'large-1', got '%s'", indexed.ID)
		}

		// Verify we can retrieve the large document
		retrieved, err := api.Get(ctx, "large-test", "large-1")
		if err != nil {
			t.Fatalf("Failed to retrieve large document: %v", err)
		}
		if retrieved == nil {
			t.Error("Large document not found after indexing")
		}
	})

	t.Run("Error Cases", func(t *testing.T) {
		// Test invalid query format
		invalidQuery := map[string]interface{}{
			"query": map[string]interface{}{
				"invalid_type": map[string]interface{}{
					"field": "value",
				},
			},
		}
		_, err := api.Search(ctx, invalidQuery)
		if err == nil {
			t.Error("Expected error for invalid query format")
		}

		// Test multiple query types
		multipleTypesQuery := map[string]interface{}{
			"query": map[string]interface{}{
				"match": map[string]interface{}{
					"field": "value",
				},
				"term": map[string]interface{}{
					"field": "value",
				},
			},
		}
		_, err = api.Search(ctx, multipleTypesQuery)
		if err == nil {
			t.Error("Expected error for multiple query types")
		}

		// Test invalid query value type
		invalidValueQuery := map[string]interface{}{
			"query": map[string]interface{}{
				"match": "invalid_value",
			},
		}
		_, err = api.Search(ctx, invalidValueQuery)
		if err == nil {
			t.Error("Expected error for invalid query value type")
		}

		// Test empty document source
		emptyDoc := &Document{
			Index:  "test",
			ID:     "empty",
			Source: map[string]interface{}{},
		}
		_, err = api.Index(ctx, emptyDoc)
		if err == nil {
			t.Error("Expected error for empty document source")
		}

		// Test malformed document
		malformedDoc := &Document{
			Index: "test",
			ID:    "malformed",
			Source: map[string]interface{}{
				"_source": nil,
			},
		}
		_, err = api.Index(ctx, malformedDoc)
		if err == nil {
			t.Error("Expected error for malformed document")
		}
	})
}

================
File: elastic/api.go
================
package elastic

import (
	"context"
	"encoding/json"
)

// Document represents an Elasticsearch document
type Document struct {
	Index      string                 `json:"_index"`
	ID         string                 `json:"_id,omitempty"`
	Source     map[string]interface{} `json:"_source"`
	Version    int64                  `json:"_version,omitempty"`
	SeqNo      int64                  `json:"_seq_no,omitempty"`
	PrimaryTerm int64                 `json:"_primary_term,omitempty"`
}

// SearchResponse represents an Elasticsearch search response
type SearchResponse struct {
	Took     int64      `json:"took"`
	TimedOut bool       `json:"timed_out"`
	Hits     SearchHits `json:"hits"`
}

// SearchHits contains search results and handles null max_score values
type SearchHits struct {
	Total    Total      `json:"total"`
	MaxScore *float64   `json:"max_score,omitempty"`
	Hits     []Document `json:"hits"`
}

// TotalRelation represents the possible values for Total.Relation
type TotalRelation string

const (
	// TotalRelationEq indicates the exact total count
	TotalRelationEq TotalRelation = "eq"
	// TotalRelationGte indicates the total count is greater than or equal
	TotalRelationGte TotalRelation = "gte"
)

// Total represents the total number of hits with type-safe relation field
type Total struct {
	Value    int64         `json:"value"`
	Relation TotalRelation `json:"relation"`
}

// IsValid checks if the TotalRelation value is valid
func (tr TotalRelation) IsValid() bool {
	switch tr {
	case TotalRelationEq, TotalRelationGte:
		return true
	default:
		return false
	}
}

// API defines the Elasticsearch-compatible API interface
type API interface {
	// Document APIs
	Index(ctx context.Context, doc *Document) (*Document, error)
	Get(ctx context.Context, index, id string) (*Document, error)
	Update(ctx context.Context, doc *Document) (*Document, error)
	Delete(ctx context.Context, index, id string) error

	// Search APIs
	Search(ctx context.Context, query map[string]interface{}) (*SearchResponse, error)
	MultiSearch(ctx context.Context, queries []map[string]interface{}) ([]*SearchResponse, error)

	// Index APIs
	ListIndices(ctx context.Context) ([]string, error)

	// Bulk Operations
	Bulk(ctx context.Context, operations []json.RawMessage) error
}

================
File: elastic/dsl_test.go
================
package elastic

import (
	"encoding/json"
	"testing"
)

func TestParseQuery(t *testing.T) {
	tests := []struct {
		name    string
		query   string
		wantErr bool
	}{
		{
			name: "match query",
			query: `{
				"query": {
					"match": {
						"title": {
							"query": "golang programming"
						}
					}
				}
			}`,
			wantErr: false,
		},
		{
			name: "term query",
			query: `{
				"query": {
					"term": {
						"status": "active"
					}
				}
			}`,
			wantErr: false,
		},
		{
			name: "range query",
			query: `{
				"query": {
					"range": {
						"age": {
							"gt": 18,
							"lt": 65
						}
					}
				}
			}`,
			wantErr: false,
		},
		{
			name: "bool query",
			query: `{
				"query": {
					"bool": {
						"must": [
							{
								"match": {
									"title": {
										"query": "golang"
									}
								}
							}
						],
						"filter": [
							{
								"term": {
									"status": {
										"value": "published"
									}
								}
							}
						]
					}
				}
			}`,
			wantErr: false,
		},
		{
			name: "invalid query structure",
			query: `{
				"invalid": {}
			}`,
			wantErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			query, err := ParseQuery([]byte(tt.query))
			if (err != nil) != tt.wantErr {
				t.Errorf("ParseQuery() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if !tt.wantErr && query == nil {
				t.Error("ParseQuery() returned nil query for valid input")
			}

			// Additional type-specific validations
			if !tt.wantErr {
				switch q := query.(type) {
				case *MatchQueryClause:
					if q.Type() != MatchQuery {
						t.Errorf("Expected MatchQuery type, got %v", q.Type())
					}
				case *TermQueryClause:
					if q.Type() != TermQuery {
						t.Errorf("Expected TermQuery type, got %v", q.Type())
					}
				case *RangeQueryClause:
					if q.Type() != RangeQuery {
						t.Errorf("Expected RangeQuery type, got %v", q.Type())
					}
				case *BoolQueryClause:
					if q.Type() != BoolQuery {
						t.Errorf("Expected BoolQuery type, got %v", q.Type())
					}
				}
			}
		})
	}
}

func TestParseComplexQueries(t *testing.T) {
	tests := []struct {
		name    string
		query   string
		wantErr bool
		validate func(*testing.T, Query)
	}{
		{
			name: "Valid complex bool query",
			query: `{
				"query": {
					"bool": {
						"must": [
							{
								"match": {
									"title": "golang"
								}
							},
							{
								"range": {
									"year": {
										"gte": 2020
									}
								}
							}
						],
						"should": [
							{
								"term": {
									"tags": "programming"
								}
							}
						],
						"must_not": [
							{
								"term": {
									"status": "draft"
								}
							}
						],
						"filter": [
							{
								"term": {
									"published": true
								}
							}
						]
					}
				}
			}`,
			wantErr: false,
			validate: func(t *testing.T, q Query) {
				boolQuery, ok := q.(*BoolQueryClause)
				if !ok {
					t.Fatal("Expected BoolQueryClause")
				}

				// Validate must clauses
				if len(boolQuery.Must) != 2 {
					t.Errorf("Expected 2 must clauses, got %d", len(boolQuery.Must))
				}

				// Validate match query in must clause
				if matchQuery, ok := boolQuery.Must[0].(*MatchQueryClause); ok {
					if matchQuery.Field != "title" || matchQuery.Value != "golang" {
						t.Errorf("Expected match query with field='title', value='golang', got field='%s', value='%v'",
							matchQuery.Field, matchQuery.Value)
					}
				} else {
					t.Error("First must clause should be a MatchQueryClause")
				}

				// Validate range query in must clause
				if rangeQuery, ok := boolQuery.Must[1].(*RangeQueryClause); ok {
					if rangeQuery.Field != "year" {
						t.Errorf("Expected range query with field='year', got field='%s'", rangeQuery.Field)
					}
					if rangeQuery.GTE != float64(2020) {
						t.Errorf("Expected range query with gte=2020, got %v", rangeQuery.GTE)
					}
				} else {
					t.Error("Second must clause should be a RangeQueryClause")
				}

				// Validate should clause
				if len(boolQuery.Should) != 1 {
					t.Errorf("Expected 1 should clause, got %d", len(boolQuery.Should))
				}
				if termQuery, ok := boolQuery.Should[0].(*TermQueryClause); ok {
					if termQuery.Field != "tags" || termQuery.Value != "programming" {
						t.Errorf("Expected term query with field='tags', value='programming', got field='%s', value='%v'",
							termQuery.Field, termQuery.Value)
					}
				} else {
					t.Error("Should clause should be a TermQueryClause")
				}

				// Validate must_not clause
				if len(boolQuery.MustNot) != 1 {
					t.Errorf("Expected 1 must_not clause, got %d", len(boolQuery.MustNot))
				}
				if termQuery, ok := boolQuery.MustNot[0].(*TermQueryClause); ok {
					if termQuery.Field != "status" || termQuery.Value != "draft" {
						t.Errorf("Expected term query with field='status', value='draft', got field='%s', value='%v'",
							termQuery.Field, termQuery.Value)
					}
				} else {
					t.Error("Must_not clause should be a TermQueryClause")
				}

				// Validate filter clause
				if len(boolQuery.Filter) != 1 {
					t.Errorf("Expected 1 filter clause, got %d", len(boolQuery.Filter))
				}
				if termQuery, ok := boolQuery.Filter[0].(*TermQueryClause); ok {
					if termQuery.Field != "published" || termQuery.Value != true {
						t.Errorf("Expected term query with field='published', value=true, got field='%s', value=%v",
							termQuery.Field, termQuery.Value)
					}
				} else {
					t.Error("Filter clause should be a TermQueryClause")
				}
			},
		},
		{
			name: "Invalid - duplicate must clauses",
			query: `{
				"query": {
					"bool": {
						"must": [
							{
								"match": {
									"title": "golang"
								}
							},
							{
								"match": {
									"title": "golang"
								}
							}
						]
					}
				}
			}`,
			wantErr: true,
		},
		{
			name: "Invalid - nested bool exceeds depth",
			query: `{
				"query": {
					"bool": {
						"must": [
							{
								"bool": {
									"must": [
										{
											"bool": {
												"must": [
													{
														"match": {
															"title": "golang"
														}
													}
												]
											}
										}
									]
								}
							}
						]
					}
				}
			}`,
			wantErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			q, err := ParseQuery([]byte(tt.query))
			if (err != nil) != tt.wantErr {
				t.Errorf("ParseQuery() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if !tt.wantErr && tt.validate != nil {
				tt.validate(t, q)
			}
		})
	}
}

func TestQueryToJSON(t *testing.T) {
	// Create a complex query
	query := &BoolQueryClause{
		BaseQuery: BaseQuery{queryType: BoolQuery},
		Must: []Query{
			&MatchQueryClause{
				BaseQuery: BaseQuery{queryType: MatchQuery},
				Field:     "title",
				Value:     "golang",
			},
		},
		Filter: []Query{
			&TermQueryClause{
				BaseQuery: BaseQuery{queryType: TermQuery},
				Field:     "status",
				Value:     "active",
			},
		},
	}

	// Marshal to JSON
	data, err := json.Marshal(query)
	if err != nil {
		t.Fatalf("Failed to marshal query: %v", err)
	}

	// Parse back
	parsedQuery, err := ParseQuery(data)
	if err != nil {
		t.Fatalf("Failed to parse marshaled query: %v", err)
	}

	// Validate parsed query
	boolQuery, ok := parsedQuery.(*BoolQueryClause)
	if !ok {
		t.Fatal("Expected BoolQueryClause")
	}

	if len(boolQuery.Must) != 1 {
		t.Errorf("Expected 1 must clause, got %d", len(boolQuery.Must))
	}

	if len(boolQuery.Filter) != 1 {
		t.Errorf("Expected 1 filter clause, got %d", len(boolQuery.Filter))
	}
}

func TestDSLCompatibility(t *testing.T) {
	t.Run("Match Query", func(t *testing.T) {
		query := map[string]interface{}{
			"query": map[string]interface{}{
				"match": map[string]interface{}{
					"title": "search text",
				},
			},
		}
		
		queryBytes, err := json.Marshal(query)
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}
		
		parsed, err := ParseQuery(queryBytes)
		if err != nil {
			t.Fatalf("Failed to parse match query: %v", err)
		}
		
		if parsed.Type() != MatchQuery {
			t.Errorf("Expected match query type, got %s", parsed.Type())
		}
	})

	t.Run("Bool Query", func(t *testing.T) {
		query := map[string]interface{}{
			"query": map[string]interface{}{
				"bool": map[string]interface{}{
					"must": []interface{}{
						map[string]interface{}{
							"match": map[string]interface{}{
								"title": "search",
							},
						},
					},
					"must_not": []interface{}{
						map[string]interface{}{
							"match": map[string]interface{}{
								"status": "draft",
							},
						},
					},
					"should": []interface{}{
						map[string]interface{}{
							"match": map[string]interface{}{
								"category": "tech",
							},
						},
					},
					"filter": []interface{}{
						map[string]interface{}{
							"range": map[string]interface{}{
								"date": map[string]interface{}{
									"gte": "2020-01-01",
								},
							},
						},
					},
				},
			},
		}
		
		queryBytes, err := json.Marshal(query)
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}
		
		parsed, err := ParseQuery(queryBytes)
		if err != nil {
			t.Fatalf("Failed to parse bool query: %v", err)
		}
		
		if parsed.Type() != BoolQuery {
			t.Errorf("Expected bool query type, got %s", parsed.Type())
		}
	})

	t.Run("Range Query", func(t *testing.T) {
		query := map[string]interface{}{
			"query": map[string]interface{}{
				"range": map[string]interface{}{
					"age": map[string]interface{}{
						"gte": 20,
						"lt":  30,
					},
				},
			},
		}
		
		queryBytes, err := json.Marshal(query)
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}
		
		parsed, err := ParseQuery(queryBytes)
		if err != nil {
			t.Fatalf("Failed to parse range query: %v", err)
		}
		
		if parsed.Type() != RangeQuery {
			t.Errorf("Expected range query type, got %s", parsed.Type())
		}
	})

	t.Run("Aggregations", func(t *testing.T) {
		query := map[string]interface{}{
			"query": map[string]interface{}{
				"match_all": map[string]interface{}{},
			},
			"aggs": map[string]interface{}{
				"avg_age": map[string]interface{}{
					"avg": map[string]interface{}{
						"field": "age",
					},
				},
			},
		}
		
		queryBytes, err := json.Marshal(query)
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}
		
		_, err = ParseQuery(queryBytes)
		if err != nil {
			t.Fatalf("Failed to parse aggregation query: %v", err)
		}
	})

	t.Run("Invalid_Queries", func(t *testing.T) {
		invalidQueries := []map[string]interface{}{
			// Missing query field
			{
				"match": map[string]interface{}{
					"field": "value",
				},
			},
			// Invalid query type
			{
				"query": map[string]interface{}{
					"invalid_type": map[string]interface{}{
						"field": "value",
					},
				},
			},
			// Invalid bool query structure
			{
				"query": map[string]interface{}{
					"bool": map[string]interface{}{
						"invalid": []interface{}{
							map[string]interface{}{
								"match": map[string]interface{}{
									"field": "value",
								},
							},
						},
					},
				},
			},
		}

		for i, query := range invalidQueries {
			queryBytes, err := json.Marshal(query)
			if err != nil {
				t.Fatalf("Failed to marshal invalid query %d: %v", i, err)
			}
			
			_, err = ParseQuery(queryBytes)
			if err == nil {
				t.Errorf("Expected error for invalid query %d", i)
			}
		}
	})
}

func TestMatchAllQueryClause(t *testing.T) {
	t.Run("Type", func(t *testing.T) {
		query := &MatchAllQueryClause{
			BaseQuery: BaseQuery{queryType: MatchAllQuery},
		}
		if query.Type() != MatchAllQuery {
			t.Errorf("Expected query type %s, got %s", MatchAllQuery, query.Type())
		}
	})

	t.Run("MarshalJSON", func(t *testing.T) {
		query := &MatchAllQueryClause{
			BaseQuery: BaseQuery{queryType: MatchAllQuery},
		}

		data, err := query.MarshalJSON()
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}

		// Verify the JSON structure
		var result map[string]interface{}
		if err := json.Unmarshal(data, &result); err != nil {
			t.Fatalf("Failed to unmarshal result: %v", err)
		}

		// Check query structure
		queryObj, ok := result["query"].(map[string]interface{})
		if !ok {
			t.Error("Expected 'query' field to be an object")
			return
		}

		matchAll, ok := queryObj["match_all"].(map[string]interface{})
		if !ok {
			t.Error("Expected 'match_all' field to be an object")
			return
		}

		if len(matchAll) != 0 {
			t.Error("Expected 'match_all' to be an empty object")
		}

		// Test that it produces valid JSON
		expectedJSON := `{"query":{"match_all":{}}}`
		actualJSON := string(data)
		if actualJSON != expectedJSON {
			t.Errorf("Expected JSON %s, got %s", expectedJSON, actualJSON)
		}
	})

	t.Run("ParseQuery", func(t *testing.T) {
		// Test parsing a match_all query
		jsonQuery := []byte(`{"match_all":{}}`)
		query, err := parseQueryClause(jsonQuery, newQueryContext())
		if err != nil {
			t.Fatalf("Failed to parse match_all query: %v", err)
		}

		matchAllQuery, ok := query.(*MatchAllQueryClause)
		if !ok {
			t.Error("Expected query to be a MatchAllQueryClause")
			return
		}

		if matchAllQuery.Type() != MatchAllQuery {
			t.Errorf("Expected query type %s, got %s", MatchAllQuery, matchAllQuery.Type())
		}

		// Test parsing with additional fields (should be ignored)
		jsonQuery = []byte(`{"match_all":{"boost":1.0}}`)
		query, err = parseQueryClause(jsonQuery, newQueryContext())
		if err != nil {
			t.Fatalf("Failed to parse match_all query with boost: %v", err)
		}

		matchAllQuery, ok = query.(*MatchAllQueryClause)
		if !ok {
			t.Error("Expected query to be a MatchAllQueryClause")
		}
	})

	t.Run("Integration", func(t *testing.T) {
		// Create a match_all query
		query := &MatchAllQueryClause{
			BaseQuery: BaseQuery{queryType: MatchAllQuery},
		}

		// Test that it matches any document
		data, err := query.MarshalJSON()
		if err != nil {
			t.Fatalf("Failed to marshal query: %v", err)
		}

		var queryMap map[string]interface{}
		if err := json.Unmarshal(data, &queryMap); err != nil {
			t.Fatalf("Failed to unmarshal query: %v", err)
		}

		// Verify that the query matches the document
		// In a real implementation, this would use the search functionality
		// Here we just verify the query structure is correct
		queryObj := queryMap["query"].(map[string]interface{})
		if _, ok := queryObj["match_all"]; !ok {
			t.Error("Expected query to have 'match_all' field")
		}
	})
}

================
File: elastic/dsl.go
================
package elastic

import (
	"encoding/json"
	"fmt"
	"time"
)

// QueryType represents the type of query
type QueryType string

const (
	// Match query for full text search
	MatchQuery QueryType = "match"
	// Term query for exact term matches
	TermQuery QueryType = "term"
	// Range query for numeric/date ranges
	RangeQuery QueryType = "range"
	// Bool query for combining multiple queries
	BoolQuery QueryType = "bool"
	// MatchAll query that matches all documents
	MatchAllQuery QueryType = "match_all"
)

// Query represents the base query interface
type Query interface {
	Type() QueryType
	MarshalJSON() ([]byte, error)
}

// BaseQuery provides common query fields
type BaseQuery struct {
	queryType QueryType
}

func (q BaseQuery) Type() QueryType {
	return q.queryType
}

// MatchQueryClause represents a full text query
type MatchQueryClause struct {
	BaseQuery
	Field string      // Field to search in
	Value interface{} // Value to search for (must be a string)
}

func (q *MatchQueryClause) MarshalJSON() ([]byte, error) {
	// Validate that Value is a string
	switch v := q.Value.(type) {
	case string:
		// Valid type, proceed with marshaling
	case fmt.Stringer:
		// Also accept types that implement String() string
		q.Value = v.String()
	default:
		return nil, fmt.Errorf("match query value must be a string, got %T", q.Value)
	}

	return json.Marshal(map[string]interface{}{
		"match": map[string]interface{}{
			q.Field: map[string]interface{}{
				"query": q.Value,
			},
		},
	})
}

// TermQueryClause represents an exact term query
type TermQueryClause struct {
	BaseQuery
	Field string
	Value interface{}
}

func (q *TermQueryClause) MarshalJSON() ([]byte, error) {
	return json.Marshal(map[string]interface{}{
		"term": map[string]interface{}{
			q.Field: map[string]interface{}{
				"value": q.Value,
			},
		},
	})
}

// RangeQueryClause represents a range query
type RangeQueryClause struct {
	BaseQuery
	Field string
	GT    interface{} `json:"gt,omitempty"`   // Greater than value (must be numeric or time.Time)
	GTE   interface{} `json:"gte,omitempty"`  // Greater than or equal value (must be numeric or time.Time)
	LT    interface{} `json:"lt,omitempty"`   // Less than value (must be numeric or time.Time)
	LTE   interface{} `json:"lte,omitempty"`  // Less than or equal value (must be numeric or time.Time)
}

// validateRangeValue checks if a value is valid for range queries (numeric or time.Time)
func (q *RangeQueryClause) validateRangeValue(val interface{}) error {
	if val == nil {
		return nil
	}

	switch v := val.(type) {
	case int, int8, int16, int32, int64,
		uint, uint8, uint16, uint32, uint64,
		float32, float64:
		return nil
	case json.Number:
		// Try to parse as float64 to validate it's a valid number
		if _, err := v.Float64(); err != nil {
			return fmt.Errorf("invalid numeric value: %v", err)
		}
		return nil
	case string:
		// Check if it's a valid time string
		if _, err := time.Parse(time.RFC3339, v); err != nil {
			return fmt.Errorf("string value must be a valid RFC3339 time format: %v", err)
		}
		return nil
	case time.Time:
		return nil
	default:
		return fmt.Errorf("range value must be numeric or time.Time, got %T", val)
	}
}

func (q *RangeQueryClause) MarshalJSON() ([]byte, error) {
	conditions := make(map[string]interface{})
	if q.GT != nil {
		conditions["gt"] = q.GT
	}
	if q.GTE != nil {
		conditions["gte"] = q.GTE
	}
	if q.LT != nil {
		conditions["lt"] = q.LT
	}
	if q.LTE != nil {
		conditions["lte"] = q.LTE
	}

	return json.Marshal(map[string]interface{}{
		"range": map[string]interface{}{
			q.Field: conditions,
		},
	})
}

// BoolQueryClause represents a boolean combination of queries
type BoolQueryClause struct {
	BaseQuery
	Must    []Query `json:"must,omitempty"`
	Should  []Query `json:"should,omitempty"`
	MustNot []Query `json:"must_not,omitempty"`
	Filter  []Query `json:"filter,omitempty"`
}

func (q *BoolQueryClause) MarshalJSON() ([]byte, error) {
	boolQuery := make(map[string]interface{})

	if len(q.Must) > 0 {
		must := make([]interface{}, len(q.Must))
		for i, query := range q.Must {
			data, err := query.MarshalJSON()
			if err != nil {
				return nil, err
			}
			var decoded interface{}
			if err := json.Unmarshal(data, &decoded); err != nil {
				return nil, err
			}
			must[i] = decoded
		}
		boolQuery["must"] = must
	}

	if len(q.Should) > 0 {
		should := make([]interface{}, len(q.Should))
		for i, query := range q.Should {
			data, err := query.MarshalJSON()
			if err != nil {
				return nil, err
			}
			var decoded interface{}
			if err := json.Unmarshal(data, &decoded); err != nil {
				return nil, err
			}
			should[i] = decoded
		}
		boolQuery["should"] = should
	}

	if len(q.MustNot) > 0 {
		mustNot := make([]interface{}, len(q.MustNot))
		for i, query := range q.MustNot {
			data, err := query.MarshalJSON()
			if err != nil {
				return nil, err
			}
			var decoded interface{}
			if err := json.Unmarshal(data, &decoded); err != nil {
				return nil, err
			}
			mustNot[i] = decoded
		}
		boolQuery["must_not"] = mustNot
	}

	if len(q.Filter) > 0 {
		filter := make([]interface{}, len(q.Filter))
		for i, query := range q.Filter {
			data, err := query.MarshalJSON()
			if err != nil {
				return nil, err
			}
			var decoded interface{}
			if err := json.Unmarshal(data, &decoded); err != nil {
				return nil, err
			}
			filter[i] = decoded
		}
		boolQuery["filter"] = filter
	}

	return json.Marshal(map[string]interface{}{
		"bool": boolQuery,
	})
}

const maxBoolNestingDepth = 2 // Maximum allowed nesting depth for bool queries

type queryContext struct {
	depth int
	seenFields map[string]map[string]bool // clause type -> field -> seen
}

func newQueryContext() *queryContext {
	return &queryContext{
		depth: 0,
		seenFields: make(map[string]map[string]bool),
	}
}

func (ctx *queryContext) checkAndAddField(clauseType, field string) error {
	if _, exists := ctx.seenFields[clauseType]; !exists {
		ctx.seenFields[clauseType] = make(map[string]bool)
	}
	if ctx.seenFields[clauseType][field] {
		return fmt.Errorf("duplicate field '%s' in clause type '%s'", field, clauseType)
	}
	ctx.seenFields[clauseType][field] = true
	return nil
}

// MatchAllQueryClause represents a query that matches all documents
type MatchAllQueryClause struct {
	BaseQuery
}

func (q *MatchAllQueryClause) MarshalJSON() ([]byte, error) {
	return json.Marshal(map[string]interface{}{
		"query": map[string]interface{}{
			"match_all": map[string]interface{}{},
		},
	})
}

func ParseQuery(data []byte) (Query, error) {
	var wrapper struct {
		Query json.RawMessage `json:"query"`
	}
	if err := json.Unmarshal(data, &wrapper); err != nil {
		return nil, fmt.Errorf("failed to parse query wrapper: %v", err)
	}

	if len(wrapper.Query) == 0 {
		return nil, fmt.Errorf("query field is required")
	}

	ctx := newQueryContext()
	return parseQueryClause(wrapper.Query, ctx)
}

func parseQueryClause(data []byte, ctx *queryContext) (Query, error) {
	var raw map[string]interface{}
	if err := json.Unmarshal(data, &raw); err != nil {
		return nil, fmt.Errorf("failed to parse query clause: %v", err)
	}

	// Check bool nesting depth
	if _, ok := raw["bool"]; ok {
		ctx.depth++
		if ctx.depth > maxBoolNestingDepth {
			return nil, fmt.Errorf("bool query nesting depth exceeds maximum of %d", maxBoolNestingDepth)
		}
	}

	// Check for query wrapper
	if queryWrapper, ok := raw["query"]; ok {
		queryBytes, err := json.Marshal(queryWrapper)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal query wrapper: %v", err)
		}
		return parseQueryClause(queryBytes, ctx)
	}

	for queryType, value := range raw {
		valueBytes, err := json.Marshal(value)
		if err != nil {
			return nil, fmt.Errorf("failed to marshal query value: %v", err)
		}

		switch queryType {
		case "match":
			return parseMatchQuery(valueBytes, ctx)
		case "term":
			return parseTermQuery(valueBytes, ctx)
		case "range":
			return parseRangeQuery(valueBytes, ctx)
		case "bool":
			return parseBoolQuery(raw, ctx)
		case "match_all":
			return parseMatchAllQuery(valueBytes, ctx)
		}
	}

	return nil, fmt.Errorf("invalid or unsupported query type")
}

func parseMatchQuery(data []byte, ctx *queryContext) (Query, error) {
	var raw map[string]interface{}
	if err := json.Unmarshal(data, &raw); err != nil {
		return nil, err
	}

	if len(raw) != 1 {
		return nil, fmt.Errorf("match query must have exactly one field")
	}

	var field string
	var value interface{}

	for f, v := range raw {
		field = f
		switch val := v.(type) {
		case string:
			value = val
		case map[string]interface{}:
			if q, ok := val["query"]; ok {
				value = q
			} else {
				// If no query field is present, use the value directly
				value = val
			}
		default:
			value = val
		}
	}

	if err := ctx.checkAndAddField("match", field); err != nil {
		return nil, err
	}

	return &MatchQueryClause{
		BaseQuery: BaseQuery{queryType: MatchQuery},
		Field:     field,
		Value:     value,
	}, nil
}

func parseTermQuery(data []byte, ctx *queryContext) (Query, error) {
	var raw map[string]interface{}
	if err := json.Unmarshal(data, &raw); err != nil {
		return nil, err
	}

	if len(raw) != 1 {
		return nil, fmt.Errorf("term query must have exactly one field")
	}

	var field string
	var value interface{}

	for f, v := range raw {
		field = f
		switch val := v.(type) {
		case map[string]interface{}:
			if v, ok := val["value"]; ok {
				value = v
			} else {
				// If no value field is present, use the value directly
				value = val
			}
		default:
			value = val
		}
	}

	if err := ctx.checkAndAddField("term", field); err != nil {
		return nil, err
	}

	return &TermQueryClause{
		BaseQuery: BaseQuery{queryType: TermQuery},
		Field:     field,
		Value:     value,
	}, nil
}

func parseRangeQuery(data []byte, ctx *queryContext) (Query, error) {
	var raw map[string]interface{}
	if err := json.Unmarshal(data, &raw); err != nil {
		return nil, err
	}

	if len(raw) != 1 {
		return nil, fmt.Errorf("range query must have exactly one field")
	}

	var field string
	var rangeValues map[string]interface{}

	for f, v := range raw {
		field = f
		if values, ok := v.(map[string]interface{}); ok {
			rangeValues = values
		} else {
			return nil, fmt.Errorf("range query values must be an object")
		}
	}

	if err := ctx.checkAndAddField("range", field); err != nil {
		return nil, err
	}

	clause := &RangeQueryClause{
		BaseQuery: BaseQuery{queryType: RangeQuery},
		Field:     field,
	}

	if gt, ok := rangeValues["gt"]; ok {
		clause.GT = gt
	}
	if gte, ok := rangeValues["gte"]; ok {
		clause.GTE = gte
	}
	if lt, ok := rangeValues["lt"]; ok {
		clause.LT = lt
	}
	if lte, ok := rangeValues["lte"]; ok {
		clause.LTE = lte
	}

	return clause, nil
}

func parseBoolQuery(data map[string]interface{}, ctx *queryContext) (Query, error) {
	boolQuery := &BoolQueryClause{
		BaseQuery: BaseQuery{queryType: BoolQuery},
	}

	boolClauses, ok := data["bool"].(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid bool query structure")
	}

	// Validate bool query structure
	for key := range boolClauses {
		switch key {
		case "must", "should", "must_not", "filter":
			// Valid keys
		default:
			return nil, fmt.Errorf("invalid bool query clause: %s", key)
		}
	}
	if !ok {
		return nil, fmt.Errorf("invalid bool query structure")
	}

	// Process must clauses
	if mustClauses, ok := boolClauses["must"].([]interface{}); ok {
		for _, clause := range mustClauses {
			clauseBytes, err := json.Marshal(clause)
			if err != nil {
				return nil, fmt.Errorf("failed to marshal must clause: %v", err)
			}
			query, err := parseQueryClause(clauseBytes, ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to parse must clause: %v", err)
			}
			boolQuery.Must = append(boolQuery.Must, query)
		}
	}

	// Process should clauses
	if shouldClauses, ok := boolClauses["should"].([]interface{}); ok {
		for _, clause := range shouldClauses {
			clauseBytes, err := json.Marshal(clause)
			if err != nil {
				return nil, fmt.Errorf("failed to marshal should clause: %v", err)
			}
			query, err := parseQueryClause(clauseBytes, ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to parse should clause: %v", err)
			}
			boolQuery.Should = append(boolQuery.Should, query)
		}
	}

	// Process must_not clauses
	if mustNotClauses, ok := boolClauses["must_not"].([]interface{}); ok {
		for _, clause := range mustNotClauses {
			clauseBytes, err := json.Marshal(clause)
			if err != nil {
				return nil, fmt.Errorf("failed to marshal must_not clause: %v", err)
			}
			query, err := parseQueryClause(clauseBytes, ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to parse must_not clause: %v", err)
			}
			boolQuery.MustNot = append(boolQuery.MustNot, query)
		}
	}

	// Process filter clauses
	if filterClauses, ok := boolClauses["filter"].([]interface{}); ok {
		for _, clause := range filterClauses {
			clauseBytes, err := json.Marshal(clause)
			if err != nil {
				return nil, fmt.Errorf("failed to marshal filter clause: %v", err)
			}
			query, err := parseQueryClause(clauseBytes, ctx)
			if err != nil {
				return nil, fmt.Errorf("failed to parse filter clause: %v", err)
			}
			boolQuery.Filter = append(boolQuery.Filter, query)
		}
	}

	return boolQuery, nil
}

func parseMatchAllQuery(data []byte, ctx *queryContext) (Query, error) {
	return &MatchAllQueryClause{
		BaseQuery: BaseQuery{queryType: MatchAllQuery},
	}, nil
}

================
File: index/index_management_test.go
================
package index

import (
	"my-indexer/document"
	"sync"
	"testing"
)

func TestDocumentUpdate(t *testing.T) {
	idx := NewIndex(nil)

	// Add initial document
	doc1 := document.NewDocument()
	doc1.AddField("title", "initial title")
	doc1.AddField("content", "initial content")
	docID, err := idx.AddDocument(doc1)
	if err != nil {
		t.Fatalf("Failed to add document: %v", err)
	}

	// Update document
	doc2 := document.NewDocument()
	doc2.AddField("title", "updated title")
	doc2.AddField("content", "updated content")
	err = idx.UpdateDocument(docID, doc2)
	if err != nil {
		t.Fatalf("Failed to update document: %v", err)
	}

	// Verify update
	updatedDoc, err := idx.GetDocument(docID)
	if err != nil {
		t.Fatalf("Failed to get document: %v", err)
	}
	if title, _ := updatedDoc.GetField("title"); title.Value != "updated title" {
		t.Errorf("Expected updated title, got %v", title.Value)
	}

	// Verify term frequencies are updated
	tf, err := idx.GetTermFrequency("initial", docID)
	if err == nil && tf > 0 {
		t.Error("Old terms should not exist in updated document")
	}
	tf, err = idx.GetTermFrequency("updated", docID)
	if err != nil || tf == 0 {
		t.Error("New terms should exist in updated document")
	}
}

func TestDocumentDeletion(t *testing.T) {
	idx := NewIndex(nil)

	// Add document
	doc := document.NewDocument()
	doc.AddField("title", "test title")
	doc.AddField("content", "test content")
	docID, err := idx.AddDocument(doc)
	if err != nil {
		t.Fatalf("Failed to add document: %v", err)
	}

	// Delete document
	err = idx.DeleteDocument(docID)
	if err != nil {
		t.Fatalf("Failed to delete document: %v", err)
	}

	// Verify deletion
	_, err = idx.GetDocument(docID)
	if err == nil {
		t.Error("Document should not exist after deletion")
	}

	// Verify posting lists are updated
	tf, err := idx.GetTermFrequency("test", docID)
	if err == nil && tf > 0 {
		t.Error("Terms from deleted document should not exist in index")
	}
}

func TestIndexOptimization(t *testing.T) {
	t.Log("Starting index optimization test")
	idx := NewIndex(nil)

	// Add and delete documents to create gaps
	t.Log("Adding and deleting documents to create gaps...")
	for i := 0; i < 10; i++ {
		doc := document.NewDocument()
		doc.AddField("content", "test content")
		docID, err := idx.AddDocument(doc)
		if err != nil {
			t.Fatalf("Failed to add document %d: %v", i, err)
		}
		t.Logf("Added document with ID: %d", docID)
		
		if i%2 == 0 {
			if err := idx.DeleteDocument(docID); err != nil {
				t.Fatalf("Failed to delete document %d: %v", docID, err)
			}
			t.Logf("Deleted document with ID: %d", docID)
		}
	}

	t.Logf("Pre-optimization document count: %d", idx.GetDocumentCount())

	// Optimize index
	t.Log("Starting index optimization...")
	err := idx.Optimize()
	if err != nil {
		t.Fatalf("Failed to optimize index: %v", err)
	}
	t.Log("Index optimization completed")

	// Verify optimization
	finalCount := idx.GetDocumentCount()
	t.Logf("Post-optimization document count: %d", finalCount)
	if finalCount != 5 {
		t.Errorf("Expected 5 documents after optimization, got %d", finalCount)
	}
}

func TestConcurrentModifications(t *testing.T) {
	idx := NewIndex(nil)
	var wg sync.WaitGroup
	numOps := 100

	// Concurrent additions
	wg.Add(numOps)
	for i := 0; i < numOps; i++ {
		go func(i int) {
			defer wg.Done()
			doc := document.NewDocument()
			doc.AddField("content", "test content")
			idx.AddDocument(doc)
		}(i)
	}
	wg.Wait()

	// Verify document count
	if count := idx.GetDocumentCount(); count != numOps {
		t.Errorf("Expected %d documents, got %d", numOps, count)
	}
}

================
File: index/index_test.go
================
package index

import (
	"fmt"
	"sync"
	"testing"
	"time"

	"my-indexer/analysis"
	"my-indexer/document"
)

func TestIndexOperations(t *testing.T) {
	// Create analyzer and index
	analyzer := analysis.NewStandardAnalyzer()
	idx := NewIndex(analyzer)

	// Test document 1
	doc1 := document.NewDocument()
	doc1.AddField("title", "The quick brown fox")
	doc1.AddField("content", "jumps over the lazy dog")

	docID1, err := idx.AddDocument(doc1)
	if err != nil {
		t.Fatalf("Failed to add document 1: %v", err)
	}

	// Test document 2
	doc2 := document.NewDocument()
	doc2.AddField("title", "Quick brown foxes")
	doc2.AddField("content", "are quick and brown")

	docID2, err := idx.AddDocument(doc2)
	if err != nil {
		t.Fatalf("Failed to add document 2: %v", err)
	}

	// Test term frequency
	tests := []struct {
		term   string
		docID  int
		expect int
	}{
		{"quick", docID1, 1},  // appears once in doc1
		{"quick", docID2, 2},  // appears twice in doc2
		{"brown", docID1, 1},  // appears once in doc1
		{"brown", docID2, 2},  // appears twice in doc2
		{"fox", docID1, 1},    // appears once in doc1
		{"foxes", docID2, 1},  // appears once in doc2
		{"nonexistent", docID1, 0},
	}

	for _, tt := range tests {
		tf, err := idx.GetTermFrequency(tt.term, tt.docID)
		if err != nil {
			t.Errorf("GetTermFrequency(%q, %d) returned error: %v", tt.term, tt.docID, err)
			continue
		}
		if tf != tt.expect {
			t.Errorf("GetTermFrequency(%q, %d) = %d, want %d", tt.term, tt.docID, tf, tt.expect)
		}
	}

	// Test document frequency
	dfTests := []struct {
		term   string
		expect int
	}{
		{"quick", 2},  // appears in both docs
		{"brown", 2},  // appears in both docs
		{"fox", 1},    // appears only in doc1
		{"foxes", 1},  // appears only in doc2
		{"nonexistent", 0},
	}

	for _, tt := range dfTests {
		df, err := idx.GetDocumentFrequency(tt.term)
		if err != nil {
			t.Errorf("GetDocumentFrequency(%q) returned error: %v", tt.term, err)
			continue
		}
		if df != tt.expect {
			t.Errorf("GetDocumentFrequency(%q) = %d, want %d", tt.term, df, tt.expect)
		}
	}

	// Test document count
	if count := idx.GetDocumentCount(); count != 2 {
		t.Errorf("GetDocumentCount() = %d, want 2", count)
	}

	// Test nil document
	if _, err := idx.AddDocument(nil); err == nil {
		t.Error("AddDocument(nil) should return error")
	}
}

func TestConcurrentAccess(t *testing.T) {
	startTime := time.Now()
	t.Log("Initializing test...")
	idx := NewIndex(analysis.NewStandardAnalyzer())
	var wg sync.WaitGroup
	var errors []string
	var errorMu sync.Mutex

	// Add initial document
	t.Log("Creating initial document...")
	doc := document.NewDocument()
	err := doc.AddField("content", "test document")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}
	
	t.Log("Attempting to add document to index...")
	var docID int
	done := make(chan struct{})
	go func() {
		defer close(done)
		var err error
		t.Log("Starting AddDocument operation...")
		docID, err = idx.AddDocument(doc)
		if err != nil {
			t.Errorf("Failed to add document: %v", err)
			return
		}
		t.Log("AddDocument operation completed successfully")
	}()

	select {
	case <-done:
		t.Logf("Successfully added document with ID: %d", docID)
	case <-time.After(5 * time.Second):
		t.Fatal("AddDocument operation timed out after 5 seconds")
	}

	t.Log("Starting concurrent operations...")

	// Concurrent reads
	for i := 0; i < 10; i++ {
		wg.Add(1)
		go func(routineNum int) {
			defer wg.Done()
			
			startGet := time.Now()
			t.Logf("Routine %d: Starting GetDocument at %v", routineNum, startGet)
			_, err := idx.GetDocument(docID)
			if err != nil {
				errorMu.Lock()
				errors = append(errors, fmt.Sprintf("GetDocument failed in routine %d: %v", routineNum, err))
				errorMu.Unlock()
				return
			}
			t.Logf("Routine %d: Completed GetDocument in %v", routineNum, time.Since(startGet))
			
			startFreq := time.Now()
			t.Logf("Routine %d: Starting GetTermFrequency at %v", routineNum, startFreq)
			_, err = idx.GetTermFrequency("test", docID)
			if err != nil {
				errorMu.Lock()
				errors = append(errors, fmt.Sprintf("GetTermFrequency failed in routine %d: %v", routineNum, err))
				errorMu.Unlock()
				return
			}
			t.Logf("Routine %d: Completed GetTermFrequency in %v", routineNum, time.Since(startFreq))
		}(i)
	}

	t.Log("All routines launched, waiting for completion...")

	// Wait with timeout
	waitDone := make(chan struct{})
	go func() {
		wg.Wait()
		close(waitDone)
	}()

	select {
	case <-waitDone:
		t.Logf("All routines completed successfully in %v", time.Since(startTime))
		if len(errors) > 0 {
			for _, err := range errors {
				t.Error(err)
			}
		}
	case <-time.After(5 * time.Second):
		t.Fatal("Test timed out - possible deadlock")
	}
}

================
File: index/index_txlog_test.go
================
package index

import (
	"os"
	"testing"

	"my-indexer/document"
)

func TestTransactionLogIntegration(t *testing.T) {
	t.Log("Starting transaction log integration test")
	
	// Create temporary directory for test logs
	t.Log("Creating temporary directory for test logs")
	tmpDir, err := os.MkdirTemp("", "index_txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)
	t.Logf("Created temporary directory: %s", tmpDir)

	// Create index with transaction log
	t.Log("Creating new index with transaction log")
	idx := NewIndex(nil)
	err = idx.InitTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to initialize transaction log: %v", err)
	}
	t.Log("Successfully initialized transaction log")

	// Test document addition with transaction logging
	t.Log("Testing document addition with transaction logging")
	doc1 := document.NewDocument()
	doc1.AddField("title", "test document 1")
	docID, err := idx.AddDocument(doc1)
	if err != nil {
		t.Fatalf("Failed to add document: %v", err)
	}
	t.Logf("Successfully added document with ID: %d", docID)

	// Verify document was added
	t.Log("Verifying document addition")
	retrievedDoc, err := idx.GetDocument(docID)
	if err != nil {
		t.Fatalf("Failed to retrieve document: %v", err)
	}
	if title, _ := retrievedDoc.GetField("title"); title.Value != "test document 1" {
		t.Fatalf("Expected title 'test document 1', got '%v'", title.Value)
	}
	t.Log("Successfully verified document addition")

	// Test document update with transaction logging
	t.Log("Testing document update with transaction logging")
	doc2 := document.NewDocument()
	doc2.AddField("title", "updated document 1")
	err = idx.UpdateDocument(docID, doc2)
	if err != nil {
		t.Fatalf("Failed to update document: %v", err)
	}
	t.Log("Successfully updated document")

	// Verify update
	t.Log("Verifying document update")
	retrievedDoc, err = idx.GetDocument(docID)
	if err != nil {
		t.Fatalf("Failed to retrieve updated document: %v", err)
	}
	if title, _ := retrievedDoc.GetField("title"); title.Value != "updated document 1" {
		t.Fatalf("Expected title 'updated document 1', got '%v'", title.Value)
	}
	t.Log("Successfully verified document update")

	// Test crash recovery
	t.Log("Testing crash recovery - closing index")
	idx.Close()
	t.Log("Index closed, creating new instance")

	// Create new index instance
	newIdx := NewIndex(nil)
	err = newIdx.InitTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to initialize transaction log for recovery: %v", err)
	}
	t.Log("Successfully initialized new index instance")

	// Verify document state after recovery
	t.Log("Verifying document state after recovery")
	retrievedDoc, err = newIdx.GetDocument(docID)
	if err != nil {
		t.Fatalf("Failed to retrieve document after recovery: %v", err)
	}
	if title, _ := retrievedDoc.GetField("title"); title.Value != "updated document 1" {
		t.Fatalf("Expected title 'updated document 1' after recovery, got '%v'", title.Value)
	}
	t.Log("Successfully verified document state after recovery")

	// Test document deletion with transaction logging
	t.Log("Testing document deletion")
	err = newIdx.DeleteDocument(docID)
	if err != nil {
		t.Fatalf("Failed to delete document: %v", err)
	}
	t.Log("Successfully deleted document")

	// Verify deletion
	t.Log("Verifying document deletion")
	_, err = newIdx.GetDocument(docID)
	if err == nil {
		t.Fatal("Document should not exist after deletion")
	}
	t.Log("Successfully verified document deletion")

	// Test recovery after deletion
	t.Log("Testing recovery after deletion - closing index")
	newIdx.Close()
	t.Log("Creating final index instance")

	finalIdx := NewIndex(nil)
	err = finalIdx.InitTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to initialize transaction log for final recovery: %v", err)
	}
	defer finalIdx.Close()
	t.Log("Successfully initialized final index instance")

	// Verify document remains deleted after recovery
	t.Log("Verifying document remains deleted after recovery")
	_, err = finalIdx.GetDocument(docID)
	if err == nil {
		t.Fatal("Document should not exist after recovery of deletion")
	}
	t.Log("Successfully verified document deletion persistence")
	t.Log("Transaction log integration test completed successfully")
}

func TestConcurrentTransactions(t *testing.T) {
	tmpDir, err := os.MkdirTemp("", "index_txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)

	idx := NewIndex(nil)
	err = idx.InitTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to initialize transaction log: %v", err)
	}

	// Test concurrent additions
	done := make(chan bool)
	for i := 0; i < 10; i++ {
		go func(i int) {
			doc := document.NewDocument()
			doc.AddField("title", "concurrent document")
			_, err := idx.AddDocument(doc)
			if err != nil {
				t.Errorf("Failed to add document concurrently: %v", err)
			}
			done <- true
		}(i)
	}

	// Wait for all operations to complete
	for i := 0; i < 10; i++ {
		<-done
	}

	// Verify document count
	if count := idx.GetDocumentCount(); count != 10 {
		t.Errorf("Expected 10 documents after concurrent additions, got %d", count)
	}

	// Close and recover
	idx.Close()

	// Create new index instance
	newIdx := NewIndex(nil)
	err = newIdx.InitTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to initialize transaction log for recovery: %v", err)
	}
	defer newIdx.Close()

	// Verify document count after recovery
	if count := newIdx.GetDocumentCount(); count != 10 {
		t.Errorf("Expected 10 documents after recovery, got %d", count)
	}
}

================
File: index/index.go
================
package index

import (
	"fmt"
	"sync"

	"my-indexer/analysis"
	"my-indexer/document"
	"my-indexer/txlog"
)

// PostingList represents a list of documents containing a term
type PostingList struct {
	DocFreq int                    // Number of documents containing the term
	Postings map[int]*PostingEntry // Map of document ID to posting entry
}

// PostingEntry represents a single document entry in a posting list
type PostingEntry struct {
	DocID         int       // Document ID
	TermFreq      int       // Frequency of term in document
	Positions     []int     // Positions of term in document
	FieldName     string    // Name of the field containing the term
	Fields        []string  // Names of the fields containing the term
}

// Index represents an inverted index
type Index struct {
	mu            sync.RWMutex
	terms         map[string]*PostingList
	docCount      int
	analyzer      analysis.Analyzer
	nextDocID     int
	docIDMap      map[int]*document.Document // Maps document IDs to documents
	txLog         *txlog.TransactionLog      // Transaction log for crash recovery
}

// NewIndex creates a new inverted index
func NewIndex(analyzer analysis.Analyzer) *Index {
	if analyzer == nil {
		analyzer = analysis.NewStandardAnalyzer()
	}
	return &Index{
		terms:     make(map[string]*PostingList),
		analyzer:  analyzer,
		docIDMap:  make(map[int]*document.Document),
	}
}

// InitTransactionLog initializes the transaction log
func (idx *Index) InitTransactionLog(logDir string) error {
	txLog, err := txlog.NewTransactionLog(logDir)
	if err != nil {
		return fmt.Errorf("failed to create transaction log: %v", err)
	}
	idx.txLog = txLog

	// Recover any pending operations
	return idx.recover()
}

// recover processes any pending operations from the transaction log
func (idx *Index) recover() error {
	fmt.Printf("recover: Starting recovery process\n")
	if idx.txLog == nil {
		fmt.Printf("recover: No transaction log present, skipping recovery\n")
		return nil
	}

	fmt.Printf("recover: Attempting to recover entries from transaction log\n")
	entries, err := idx.txLog.Recover()
	if err != nil {
		return fmt.Errorf("failed to recover from transaction log: %v", err)
	}

	// Reset index state
	fmt.Printf("recover: Resetting index state\n")
	idx.terms = make(map[string]*PostingList)
	idx.docIDMap = make(map[int]*document.Document)
	idx.docCount = 0
	idx.nextDocID = 0

	fmt.Printf("recover: Processing %d entries in chronological order\n", len(entries))
	// Process entries in chronological order
	for _, entry := range entries {
		if !entry.Committed {
			continue
		}

		fmt.Printf("recover: Processing entry [Operation: %s, DocID: %d, Committed: %v]\n", 
			entry.Operation, entry.DocumentID, entry.Committed)
		switch entry.Operation {
		case txlog.OpAdd:
			if entry.Document != nil {
				// Create a new document and copy all fields
				newDoc := document.NewDocument()
				for _, field := range entry.Document.GetFields() {
					newDoc.AddField(field.Name, field.Value)
				}
				
				// Use the original document ID from the log entry
				idx.docIDMap[entry.DocumentID] = newDoc
				idx.docCount++
				
				// Index the document terms
				docTermFreqs := make(map[string]int)
				for _, field := range newDoc.GetFields() {
					fieldValue, ok := field.Value.(string)
					if !ok {
						continue
					}
					
					tokens := idx.analyzer.Analyze(fieldValue)
					for _, token := range tokens {
						docTermFreqs[token.Text]++
					}
				}
				
				// Update posting lists
				for term, freq := range docTermFreqs {
					postingList, exists := idx.terms[term]
					if !exists {
						postingList = &PostingList{
							Postings: make(map[int]*PostingEntry),
						}
						idx.terms[term] = postingList
					}
					
					postingEntry := &PostingEntry{
						DocID:    entry.DocumentID,
						TermFreq: freq,
					}
					postingList.Postings[entry.DocumentID] = postingEntry
					postingList.DocFreq++
				}
			}
		case txlog.OpUpdate:
			if entry.Document != nil {
				// Create a new document and copy all fields
				newDoc := document.NewDocument()
				for _, field := range entry.Document.GetFields() {
					if err := newDoc.AddField(field.Name, field.Value); err != nil {
						return fmt.Errorf("failed to restore field %s: %v", field.Name, err)
					}
				}
			
				// Store document directly in map since we're recovering
				idx.docIDMap[entry.DocumentID] = newDoc
				
				// Index the document terms
				docTermFreqs := make(map[string]int)
				for _, field := range newDoc.GetFields() {
					fieldValue, ok := field.Value.(string)
					if !ok {
						continue
					}
					
					tokens := idx.analyzer.Analyze(fieldValue)
					for _, token := range tokens {
						docTermFreqs[token.Text]++
					}
				}
				
				// Update posting lists
				for term, freq := range docTermFreqs {
					postingList, exists := idx.terms[term]
					if !exists {
						postingList = &PostingList{
							Postings: make(map[int]*PostingEntry),
						}
						idx.terms[term] = postingList
					}
					
					postingEntry := &PostingEntry{
						DocID:    entry.DocumentID,
						TermFreq: freq,
					}
					postingList.Postings[entry.DocumentID] = postingEntry
					postingList.DocFreq++
				}
			}
		case txlog.OpDelete:
			// Only attempt delete if document exists
			if _, exists := idx.docIDMap[entry.DocumentID]; exists {
				if err := idx.deleteDocumentInternal(entry.DocumentID); err != nil {
					return fmt.Errorf("failed to replay delete operation: %v", err)
				}
			}
		}
	}

	// Update nextDocID to be after the highest used ID
	maxID := -1
	for docID := range idx.docIDMap {
		if docID > maxID {
			maxID = docID
		}
	}
	idx.nextDocID = maxID + 1
	fmt.Printf("recover: Set nextDocID to %d after scanning existing documents\n", idx.nextDocID)

	fmt.Printf("recover: Recovery completed successfully, truncating log\n")
	// Clear the log after successful recovery
	return idx.txLog.Truncate()
}

// addDocumentInternal adds a document without transaction logging
func (idx *Index) addDocumentInternal(doc *document.Document) (int, error) {
	if doc == nil {
		return 0, fmt.Errorf("cannot index nil document")
	}

	// Note: Caller must hold write lock
	docID := idx.nextDocID
	idx.nextDocID++
	idx.docCount++

	// Store document in map
	idx.docIDMap[docID] = doc

	// Track total term frequencies across all fields
	type termInfo struct {
		freq   int
		fields []string
	}
	docTermInfo := make(map[string]*termInfo)

	// First pass: collect term frequencies across all fields
	for _, field := range doc.GetFields() {
		fieldValue, ok := field.Value.(string)
		if !ok {
			continue
		}

		tokens := idx.analyzer.Analyze(fieldValue)
		for _, token := range tokens {
			info, exists := docTermInfo[token.Text]
			if !exists {
				info = &termInfo{fields: make([]string, 0)}
				docTermInfo[token.Text] = info
			}
			info.freq++
			// Only add field name once
			fieldFound := false
			for _, f := range info.fields {
				if f == field.Name {
					fieldFound = true
					break
				}
			}
			if !fieldFound {
				info.fields = append(info.fields, field.Name)
			}
		}
	}

	// Second pass: update posting lists
	for term, info := range docTermInfo {
		postingList, exists := idx.terms[term]
		if !exists {
			postingList = &PostingList{
				Postings: make(map[int]*PostingEntry),
			}
			idx.terms[term] = postingList
		}

		entry := &PostingEntry{
			DocID:    docID,
			TermFreq: info.freq,
			Fields:   info.fields,
		}
		postingList.Postings[docID] = entry
		postingList.DocFreq++
	}

	return docID, nil
}

// AddDocument adds a document to the index with transaction logging
func (idx *Index) AddDocument(doc *document.Document) (int, error) {
	fmt.Printf("AddDocument: Starting...\n")
	if doc == nil {
		return 0, fmt.Errorf("cannot index nil document")
	}

	fmt.Printf("AddDocument: Attempting to acquire write lock\n")
	idx.mu.Lock()
	fmt.Printf("AddDocument: Write lock acquired\n")
	defer func() {
		idx.mu.Unlock()
		fmt.Printf("AddDocument: Released write lock\n")
	}()

	// Get the next document ID under the lock
	docID := idx.nextDocID

	// Handle transaction logging if enabled
	if idx.txLog != nil {
		fmt.Printf("AddDocument: Using transaction log\n")
		if err := idx.txLog.LogOperation(txlog.OpAdd, docID, doc); err != nil {
			return 0, fmt.Errorf("failed to log add operation: %v", err)
		}

		// Add the document with transaction logging
		id, err := idx.addDocumentInternal(doc)
		if err != nil {
			idx.txLog.Rollback(docID)
			return 0, err
		}

		// Commit the operation
		if err := idx.txLog.Commit(docID); err != nil {
			return 0, fmt.Errorf("failed to commit add operation: %v", err)
		}

		return id, nil
	}

	// If no transaction log, add document directly
	return idx.addDocumentInternal(doc)
}

// updateDocumentInternal updates a document without transaction logging
func (idx *Index) updateDocumentInternal(docID int, doc *document.Document) error {
	if doc == nil {
		return fmt.Errorf("cannot update with nil document")
	}

	// Note: Caller must hold write lock
	oldDoc, exists := idx.docIDMap[docID]
	if !exists {
		return fmt.Errorf("document with ID %d does not exist", docID)
	}

	// Remove old document's terms
	for _, field := range oldDoc.GetFields() {
		fieldValue, ok := field.Value.(string)
		if !ok {
			continue
		}

		tokens := idx.analyzer.Analyze(fieldValue)
		for _, token := range tokens {
			if postingList, exists := idx.terms[token.Text]; exists {
				if _, exists := postingList.Postings[docID]; exists {
					delete(postingList.Postings, docID)
					postingList.DocFreq--
					if postingList.DocFreq == 0 {
						delete(idx.terms, token.Text)
					}
				}
			}
		}
	}

	// Add new document's terms
	docTermFreqs := make(map[string]int)
	for _, field := range doc.GetFields() {
		fieldValue, ok := field.Value.(string)
		if !ok {
			continue
		}

		tokens := idx.analyzer.Analyze(fieldValue)
		for _, token := range tokens {
			docTermFreqs[token.Text]++
		}
	}

	for term, freq := range docTermFreqs {
		postingList, exists := idx.terms[term]
		if !exists {
			postingList = &PostingList{
				Postings: make(map[int]*PostingEntry),
			}
			idx.terms[term] = postingList
		}

		entry := &PostingEntry{
			DocID:    docID,
			TermFreq: freq,
		}
		postingList.Postings[docID] = entry
		if _, exists := postingList.Postings[docID]; !exists {
			postingList.DocFreq++
		}
	}

	idx.docIDMap[docID] = doc
	return nil
}

// UpdateDocument updates a document with transaction logging
func (idx *Index) UpdateDocument(docID int, doc *document.Document) error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	// Log the operation first
	if idx.txLog != nil {
		if err := idx.txLog.LogOperation(txlog.OpUpdate, docID, doc); err != nil {
			return fmt.Errorf("failed to log update operation: %v", err)
		}

		// Update the document
		if err := idx.updateDocumentInternal(docID, doc); err != nil {
			idx.txLog.Rollback(docID)
			return err
		}

		// Commit the operation
		if err := idx.txLog.Commit(docID); err != nil {
			return fmt.Errorf("failed to commit update operation: %v", err)
		}

		return nil
	}

	// If no transaction log, just update the document
	return idx.updateDocumentInternal(docID, doc)
}

// deleteDocumentInternal deletes a document without transaction logging
func (idx *Index) deleteDocumentInternal(docID int) error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	doc, exists := idx.docIDMap[docID]
	if !exists {
		return fmt.Errorf("document with ID %d does not exist", docID)
	}

	// Remove document's terms from posting lists
	for _, field := range doc.GetFields() {
		fieldValue, ok := field.Value.(string)
		if !ok {
			continue
		}

		tokens := idx.analyzer.Analyze(fieldValue)
		for _, token := range tokens {
			if postingList, exists := idx.terms[token.Text]; exists {
				if _, exists := postingList.Postings[docID]; exists {
					delete(postingList.Postings, docID)
					postingList.DocFreq--
					if postingList.DocFreq == 0 {
						delete(idx.terms, token.Text)
					}
				}
			}
		}
	}

	delete(idx.docIDMap, docID)
	idx.docCount--
	return nil
}

// DeleteDocument deletes a document with transaction logging
func (idx *Index) DeleteDocument(docID int) error {
	// Log the operation first if transaction logging is enabled
	if idx.txLog != nil {
		if err := idx.txLog.LogOperation(txlog.OpDelete, docID, nil); err != nil {
			return fmt.Errorf("failed to log delete operation: %v", err)
		}

		// Delete the document
		if err := idx.deleteDocumentInternal(docID); err != nil {
			idx.txLog.Rollback(docID)
			return err
		}

		// Commit the operation
		if err := idx.txLog.Commit(docID); err != nil {
			return fmt.Errorf("failed to commit delete operation: %v", err)
		}

		return nil
	}

	// If no transaction log, just delete the document
	return idx.deleteDocumentInternal(docID)
}

// Close closes the index and its transaction log
func (idx *Index) Close() error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	if idx.txLog != nil {
		if err := idx.txLog.Close(); err != nil {
			return fmt.Errorf("failed to close transaction log: %v", err)
		}
	}
	return nil
}

// GetDocument retrieves a document by its ID
func (idx *Index) GetDocument(docID int) (*document.Document, error) {
	fmt.Printf("GetDocument: Attempting to acquire read lock for docID %d\n", docID)
	idx.mu.RLock()
	defer func() {
		idx.mu.RUnlock()
		fmt.Printf("GetDocument: Released read lock for docID %d\n", docID)
	}()

	doc, exists := idx.docIDMap[docID]
	if !exists {
		return nil, fmt.Errorf("document with ID %d not found", docID)
	}
	return doc, nil
}

// GetPostingList retrieves the posting list for a term
func (idx *Index) GetPostingList(term string) (*PostingList, error) {
	if term == "" {
		return nil, fmt.Errorf("empty term")
	}

	idx.mu.RLock()
	defer idx.mu.RUnlock()

	// Analyze the term using the same analyzer
	tokens := idx.analyzer.Analyze(term)
	if len(tokens) == 0 {
		return nil, nil
	}

	// Use the first token as the term
	analyzedTerm := tokens[0].Text
	postingList, exists := idx.terms[analyzedTerm]
	if !exists {
		return nil, nil
	}

	return postingList, nil
}

// GetTermFrequency returns the frequency of a term in a document
func (idx *Index) GetTermFrequency(term string, docID int) (int, error) {
	fmt.Printf("GetTermFrequency: Attempting to acquire read lock for term '%s' docID %d\n", term, docID)
	idx.mu.RLock()
	defer func() {
		idx.mu.RUnlock()
		fmt.Printf("GetTermFrequency: Released read lock for term '%s' docID %d\n", term, docID)
	}()

	if postingList, exists := idx.terms[term]; exists {
		if entry, exists := postingList.Postings[docID]; exists {
			return entry.TermFreq, nil
		}
	}
	return 0, nil
}

// GetDocumentFrequency returns the number of documents containing a term
func (idx *Index) GetDocumentFrequency(term string) (int, error) {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	if postingList, exists := idx.terms[term]; exists {
		return postingList.DocFreq, nil
	}
	return 0, nil
}

// GetPostings returns the posting list entries for a term
func (idx *Index) GetPostings(term string) map[int]*PostingEntry {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	if postingList, exists := idx.terms[term]; exists {
		// Create a copy to avoid concurrent access issues
		entries := make(map[int]*PostingEntry, len(postingList.Postings))
		for docID, entry := range postingList.Postings {
			entries[docID] = entry
		}
		return entries
	}
	return make(map[int]*PostingEntry)
}

// GetDocumentCount returns the total number of documents in the index
func (idx *Index) GetDocumentCount() int {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	return idx.docCount
}

// GetTerms returns a copy of the terms map for serialization
func (idx *Index) GetTerms() map[string]*PostingList {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	terms := make(map[string]*PostingList, len(idx.terms))
	for term, postingList := range idx.terms {
		terms[term] = postingList
	}
	return terms
}

// GetNextDocID returns the next document ID
func (idx *Index) GetNextDocID() int {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	return idx.nextDocID
}

// Analyzer returns the analyzer used by this index
func (idx *Index) Analyzer() analysis.Analyzer {
	idx.mu.RLock()
	defer idx.mu.RUnlock()
	return idx.analyzer
}

// RestoreFromData restores the index state from serialized data
func (idx *Index) RestoreFromData(terms map[string]*PostingList, docCount, nextDocID int) error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	idx.terms = terms
	idx.docCount = docCount
	idx.nextDocID = nextDocID
	return nil
}

// Optimize performs index optimization by removing gaps in document IDs
// and cleaning up unused terms
func (idx *Index) Optimize() error {
	idx.mu.Lock()
	defer idx.mu.Unlock()

	// Create new document ID mapping
	newDocIDMap := make(map[int]*document.Document)
	oldToNewID := make(map[int]int)
	newID := 0

	// Reassign document IDs sequentially
	for oldID, doc := range idx.docIDMap {
		newDocIDMap[newID] = doc
		oldToNewID[oldID] = newID
		newID++
	}

	// Update posting lists with new document IDs
	newTerms := make(map[string]*PostingList)
	for term, postingList := range idx.terms {
		newPostings := make(map[int]*PostingEntry)
		for oldID, entry := range postingList.Postings {
			if newID, exists := oldToNewID[oldID]; exists {
				entry.DocID = newID
				newPostings[newID] = entry
			}
		}
		if len(newPostings) > 0 {
			postingList.Postings = newPostings
			newTerms[term] = postingList
		}
	}

	// Update index state
	idx.docIDMap = newDocIDMap
	idx.terms = newTerms
	idx.nextDocID = len(newDocIDMap)

	return nil
}

// IndexDocument indexes an ElasticSearch-compatible document
func (idx *Index) IndexDocument(indexName string, docID string, doc map[string]interface{}) error {
    // Create new document
    internalDoc := document.NewDocument()

    // Copy all fields from the ElasticSearch document
    for field, value := range doc {
        // Skip metadata fields that start with _
        if len(field) > 0 && field[0] == '_' {
            continue
        }
        if err := internalDoc.AddField(field, value); err != nil {
            return fmt.Errorf("failed to add field %s: %v", field, err)
        }
    }

    // If docID is provided, try to update existing document
    if docID != "" {
        // Convert string docID to int
        var intDocID int
        _, err := fmt.Sscanf(docID, "%d", &intDocID)
        if err != nil {
            return fmt.Errorf("invalid document ID format: %v", err)
        }

        // Check if document exists
        existingDoc, err := idx.GetDocument(intDocID)
        if err == nil && existingDoc != nil {
            // Update existing document
            return idx.UpdateDocument(intDocID, internalDoc)
        }
    }

    // Add as new document
    _, err := idx.AddDocument(internalDoc)
    return err
}

// GetAllDocuments returns all documents in the index
func (idx *Index) GetAllDocuments() ([]*document.Document, error) {
	idx.mu.RLock()
	defer idx.mu.RUnlock()

	docs := make([]*document.Document, 0, len(idx.docIDMap))
	for _, doc := range idx.docIDMap {
		docs = append(docs, doc)
	}
	return docs, nil
}

================
File: logger/logger_test.go
================
package logger

import (
	"net/http"
	"net/http/httptest"
	"os"
	"strings"
	"testing"
	"time"
)

func TestInitialize(t *testing.T) {
	// Clean up any existing log files
	os.RemoveAll("logs")
	defer os.RemoveAll("logs")

	// Test initialization
	err := Initialize()
	if err != nil {
		t.Fatalf("Failed to initialize loggers: %v", err)
	}
	defer Close()

	// Check if log files were created
	files := []string{"info.log", "error.log", "request.log"}
	for _, file := range files {
		if _, err := os.Stat("logs/" + file); os.IsNotExist(err) {
			t.Errorf("Log file %s was not created", file)
		}
	}
}

func TestLogging(t *testing.T) {
	// Clean up any existing log files
	os.RemoveAll("logs")
	defer os.RemoveAll("logs")

	// Initialize loggers
	err := Initialize()
	if err != nil {
		t.Fatalf("Failed to initialize loggers: %v", err)
	}
	defer Close()

	// Test info logging
	Info("Test info message")
	content, err := os.ReadFile("logs/info.log")
	if err != nil {
		t.Errorf("Failed to read info log: %v", err)
	}
	if !strings.Contains(string(content), "Test info message") {
		t.Error("Info message was not logged correctly")
	}

	// Test error logging
	Error("Test error message")
	content, err = os.ReadFile("logs/error.log")
	if err != nil {
		t.Errorf("Failed to read error log: %v", err)
	}
	if !strings.Contains(string(content), "Test error message") {
		t.Error("Error message was not logged correctly")
	}
}

func TestLoggingMiddleware(t *testing.T) {
	// Clean up any existing log files
	os.RemoveAll("logs")
	defer os.RemoveAll("logs")

	// Initialize loggers
	err := Initialize()
	if err != nil {
		t.Fatalf("Failed to initialize loggers: %v", err)
	}
	defer Close()

	// Create a test handler
	handler := http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		w.WriteHeader(http.StatusOK)
		w.Write([]byte("OK"))
	})

	// Create a test server with the logging middleware
	server := httptest.NewServer(LoggingMiddleware(handler))
	defer server.Close()

	// Make a test request
	resp, err := http.Get(server.URL)
	if err != nil {
		t.Fatalf("Failed to make request: %v", err)
	}
	defer resp.Body.Close()

	// Wait a bit for the log to be written
	time.Sleep(100 * time.Millisecond)

	// Verify the request was logged
	content, err := os.ReadFile("logs/request.log")
	if err != nil {
		t.Errorf("Failed to read request log: %v", err)
	}
	logContent := string(content)
	if !strings.Contains(logContent, "Method=GET") {
		t.Error("Request method was not logged")
	}
	if !strings.Contains(logContent, "StatusCode=200") {
		t.Error("Status code was not logged")
	}
}

func TestResponseWriter(t *testing.T) {
	// Create a test response writer
	w := httptest.NewRecorder()
	rw := newResponseWriter(w)

	// Test default status code
	if rw.statusCode != http.StatusOK {
		t.Errorf("Default status code should be 200, got %d", rw.statusCode)
	}

	// Test writing a different status code
	rw.WriteHeader(http.StatusNotFound)
	if rw.statusCode != http.StatusNotFound {
		t.Errorf("Status code should be 404, got %d", rw.statusCode)
	}
}

================
File: logger/logger.go
================
package logger

import (
	"fmt"
	"log"
	"net/http"
	"os"
	"time"
)

var (
	infoLogger    *log.Logger
	errorLogger   *log.Logger
	requestLogger *log.Logger
	
	// File descriptors that need to be closed
	infoFile    *os.File
	errorFile   *os.File
	requestFile *os.File
)

// Initialize sets up the loggers
func Initialize() error {
	// Create logs directory if it doesn't exist
	if err := os.MkdirAll("logs", 0755); err != nil {
		return fmt.Errorf("failed to create logs directory: %v", err)
	}

	var err error
	infoFile, err = os.OpenFile("logs/info.log", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		Close() // Close any previously opened files
		return fmt.Errorf("failed to open info log file: %v", err)
	}

	errorFile, err = os.OpenFile("logs/error.log", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		Close() // Close any previously opened files
		return fmt.Errorf("failed to open error log file: %v", err)
	}

	requestFile, err = os.OpenFile("logs/request.log", os.O_APPEND|os.O_CREATE|os.O_WRONLY, 0644)
	if err != nil {
		Close() // Close any previously opened files
		return fmt.Errorf("failed to open request log file: %v", err)
	}

	infoLogger = log.New(infoFile, "INFO: ", log.Ldate|log.Ltime|log.Lshortfile)
	errorLogger = log.New(errorFile, "ERROR: ", log.Ldate|log.Ltime|log.Lshortfile)
	requestLogger = log.New(requestFile, "REQUEST: ", log.Ldate|log.Ltime)

	return nil
}

// Close properly closes all log file descriptors
func Close() {
	if infoFile != nil {
		infoFile.Close()
		infoFile = nil
	}
	if errorFile != nil {
		errorFile.Close()
		errorFile = nil
	}
	if requestFile != nil {
		requestFile.Close()
		requestFile = nil
	}
}

// Info logs an informational message
func Info(format string, v ...interface{}) {
	if infoLogger != nil {
		infoLogger.Printf(format, v...)
	} else {
		log.Printf("INFO: "+format, v...)
	}
}

// Error logs an error message
func Error(format string, v ...interface{}) {
	if errorLogger != nil {
		errorLogger.Printf(format, v...)
	} else {
		log.Printf("ERROR: "+format, v...)
	}
}

// LogRequest logs HTTP request details
func LogRequest(r *http.Request, statusCode int, duration time.Duration) {
	if requestLogger != nil {
		requestLogger.Printf(
			"Method=%s Path=%s StatusCode=%d Duration=%v RemoteAddr=%s UserAgent=%s",
			r.Method,
			r.URL.Path,
			statusCode,
			duration,
			r.RemoteAddr,
			r.UserAgent(),
		)
	}
}

// responseWriter wraps http.ResponseWriter to capture the status code
type responseWriter struct {
	http.ResponseWriter
	statusCode int
}

func newResponseWriter(w http.ResponseWriter) *responseWriter {
	return &responseWriter{w, http.StatusOK}
}

func (rw *responseWriter) WriteHeader(code int) {
	rw.statusCode = code
	rw.ResponseWriter.WriteHeader(code)
}

// LoggingMiddleware is a middleware that logs request details
func LoggingMiddleware(next http.Handler) http.Handler {
	return http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
		start := time.Now()
		rw := newResponseWriter(w)
		
		// Call the next handler
		next.ServeHTTP(rw, r)
		
		// Log the request after it's handled
		duration := time.Since(start)
		LogRequest(r, rw.statusCode, duration)
	})
}

================
File: query/parser_test.go
================
package query

import (
	"testing"
	"reflect"
)

func TestQueryParser(t *testing.T) {
	parser := NewParser("content")

	tests := []struct {
		name    string
		input   string
		want    *ParsedQuery
		wantErr bool
	}{
		{
			name:  "Simple term query",
			input: "test",
			want: &ParsedQuery{
				Type:  TermQuery,
				Field: "content",
				Terms: []string{"test"},
			},
			wantErr: false,
		},
		{
			name:  "Field query",
			input: "title:test",
			want: &ParsedQuery{
				Type:  FieldQuery,
				Field: "title",
				Terms: []string{"test"},
			},
			wantErr: false,
		},
		{
			name:  "Phrase query",
			input: "\"quick brown fox\"",
			want: &ParsedQuery{
				Type:     PhraseQuery,
				Field:    "content",
				Terms:    []string{"quick", "brown", "fox"},
				IsPhrase: true,
			},
			wantErr: false,
		},
		{
			name:  "Field phrase query",
			input: "title:\"quick brown fox\"",
			want: &ParsedQuery{
				Type:     PhraseQuery,
				Field:    "title",
				Terms:    []string{"quick", "brown", "fox"},
				IsPhrase: true,
			},
			wantErr: false,
		},
		{
			name:  "AND query",
			input: "quick AND fox",
			want: &ParsedQuery{
				Type: TermQuery,
				SubQueries: []ParsedQuery{
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"quick"},
					},
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"fox"},
					},
				},
				Operator: "AND",
			},
			wantErr: false,
		},
		{
			name:  "OR query",
			input: "quick OR fox",
			want: &ParsedQuery{
				Type: TermQuery,
				SubQueries: []ParsedQuery{
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"quick"},
					},
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"fox"},
					},
				},
				Operator: "OR",
			},
			wantErr: false,
		},
		{
			name:    "Empty query",
			input:   "",
			want:    nil,
			wantErr: true,
		},
		{
			name:    "Invalid field query",
			input:   "title:",
			want:    nil,
			wantErr: true,
		},
		{
			name:    "Invalid phrase query",
			input:   "\"\"",
			want:    nil,
			wantErr: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			got, err := parser.Parse(tt.input)
			if (err != nil) != tt.wantErr {
				t.Errorf("Parse() error = %v, wantErr %v", err, tt.wantErr)
				return
			}
			if !reflect.DeepEqual(got, tt.want) {
				t.Errorf("Parse() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestQueryValidation(t *testing.T) {
	parser := NewParser("content")

	tests := []struct {
		name    string
		query   *ParsedQuery
		wantErr bool
	}{
		{
			name: "Valid term query",
			query: &ParsedQuery{
				Type:  TermQuery,
				Field: "content",
				Terms: []string{"test"},
			},
			wantErr: false,
		},
		{
			name:    "Nil query",
			query:   nil,
			wantErr: true,
		},
		{
			name: "Empty query",
			query: &ParsedQuery{
				Type:  TermQuery,
				Field: "content",
			},
			wantErr: true,
		},
		{
			name: "Invalid phrase query",
			query: &ParsedQuery{
				Type:     PhraseQuery,
				Field:    "content",
				Terms:    []string{"test"},
				IsPhrase: true,
			},
			wantErr: true,
		},
		{
			name: "Valid AND query",
			query: &ParsedQuery{
				Type: TermQuery,
				SubQueries: []ParsedQuery{
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"quick"},
					},
					{
						Type:  TermQuery,
						Field: "content",
						Terms: []string{"fox"},
					},
				},
				Operator: "AND",
			},
			wantErr: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			err := parser.Validate(tt.query)
			if (err != nil) != tt.wantErr {
				t.Errorf("Validate() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

================
File: query/parser.go
================
package query

import (
	"fmt"
	"strings"
)

// Use the QueryType from query.go

// ParsedQuery represents a parsed search query
type ParsedQuery struct {
	Type       QueryType
	Field      string
	Terms      []string
	IsPhrase   bool
	SubQueries []ParsedQuery
	Operator   string // "AND" or "OR"
}

// Parser handles query parsing
type Parser struct {
	defaultField string
}

// NewParser creates a new query parser
func NewParser(defaultField string) *Parser {
	return &Parser{
		defaultField: defaultField,
	}
}

// Parse parses a query string into a ParsedQuery object
func (p *Parser) Parse(queryStr string) (*ParsedQuery, error) {
	queryStr = strings.TrimSpace(queryStr)
	if queryStr == "" {
		return nil, fmt.Errorf("empty query")
	}

	// Handle field-specific queries (field:value)
	if strings.Contains(queryStr, ":") {
		parts := strings.SplitN(queryStr, ":", 2)
		if len(parts) != 2 {
			return nil, fmt.Errorf("invalid field query syntax")
		}
		field := strings.TrimSpace(parts[0])
		value := strings.TrimSpace(parts[1])
		
		if value == "" {
			return nil, fmt.Errorf("empty field value")
		}
		
		// Handle phrase queries
		if strings.HasPrefix(value, "\"") && strings.HasSuffix(value, "\"") {
			value = strings.Trim(value, "\"")
			terms := strings.Fields(value)
			if len(terms) < 2 {
				return nil, fmt.Errorf("phrase query must contain at least two terms")
			}
			return &ParsedQuery{
				Type:     PhraseQuery,
				Field:    field,
				Terms:    terms,
				IsPhrase: true,
			}, nil
		}
		
		terms := strings.Fields(value)
		if len(terms) == 0 {
			return nil, fmt.Errorf("empty field value")
		}
		
		return &ParsedQuery{
			Type:  FieldQuery,
			Field: field,
			Terms: terms,
		}, nil
	}

	// Handle phrase queries
	if strings.HasPrefix(queryStr, "\"") && strings.HasSuffix(queryStr, "\"") {
		queryStr = strings.Trim(queryStr, "\"")
		terms := strings.Fields(queryStr)
		if len(terms) < 2 {
			return nil, fmt.Errorf("phrase query must contain at least two terms")
		}
		return &ParsedQuery{
			Type:     PhraseQuery,
			Field:    p.defaultField,
			Terms:    terms,
			IsPhrase: true,
		}, nil
	}

	// Handle AND/OR queries
	if strings.Contains(queryStr, " AND ") {
		parts := strings.Split(queryStr, " AND ")
		subQueries := make([]ParsedQuery, 0, len(parts))
		for _, part := range parts {
			subQuery, err := p.Parse(part)
			if err != nil {
				return nil, err
			}
			subQueries = append(subQueries, *subQuery)
		}
		return &ParsedQuery{
			Type:       TermQuery,
			SubQueries: subQueries,
			Operator:   "AND",
		}, nil
	}

	if strings.Contains(queryStr, " OR ") {
		parts := strings.Split(queryStr, " OR ")
		subQueries := make([]ParsedQuery, 0, len(parts))
		for _, part := range parts {
			subQuery, err := p.Parse(part)
			if err != nil {
				return nil, err
			}
			subQueries = append(subQueries, *subQuery)
		}
		return &ParsedQuery{
			Type:       TermQuery,
			SubQueries: subQueries,
			Operator:   "OR",
		}, nil
	}

	// Simple term query
	terms := strings.Fields(queryStr)
	if len(terms) == 0 {
		return nil, fmt.Errorf("empty query")
	}

	return &ParsedQuery{
		Type:  TermQuery,
		Field: p.defaultField,
		Terms: terms,
	}, nil
}

// Validate checks if a query is valid
func (p *Parser) Validate(query *ParsedQuery) error {
	if query == nil {
		return fmt.Errorf("nil query")
	}

	if len(query.Terms) == 0 && len(query.SubQueries) == 0 {
		return fmt.Errorf("query must contain at least one term or subquery")
	}

	if query.IsPhrase && len(query.Terms) < 2 {
		return fmt.Errorf("phrase query must contain at least two terms")
	}

	// Validate subqueries recursively
	for _, subQuery := range query.SubQueries {
		if err := p.Validate(&subQuery); err != nil {
			return err
		}
	}

	return nil
}

================
File: query/query_test.go
================
package query

import (
	"my-indexer/document"
	"testing"
	"time"
)

func TestTermQuery(t *testing.T) {
	query := NewTermQuery("title", "test")

	tests := []struct {
		name  string
		value interface{}
		want  bool
	}{
		{"Exact match", "test", true},
		{"No match", "other", false},
		{"Non-string value", 123, false},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			if got := query.Match(tt.value); got != tt.want {
				t.Errorf("TermQuery.Match() = %v, want %v", got, tt.want)
			}
		})
	}
}

func TestRangeQuery(t *testing.T) {
	tests := []struct {
		name     string
		query    *RangeQueryImpl
		doc      *document.Document
		expected bool
	}{
		{
			name: "Numeric range",
			query: &RangeQueryImpl{
				field: "age",
				gt:    10.0,
				lt:    20.0,
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("age", 15.0)
				return doc
			}(),
			expected: true,
		},
		{
			name: "Inclusive numeric range",
			query: &RangeQueryImpl{
				field: "age",
				gte:   10.0,
				lte:   20.0,
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("age", 20.0) // Test inclusive upper bound
				return doc
			}(),
			expected: true,
		},
		{
			name: "Mixed inclusive/exclusive range",
			query: &RangeQueryImpl{
				field: "age",
				gt:    10.0,
				lte:   20.0,
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("age", 20.0) // Should match due to lte
				return doc
			}(),
			expected: true,
		},
		{
			name: "Outside range",
			query: &RangeQueryImpl{
				field: "age",
				gte:   10.0,
				lte:   20.0,
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("age", 25.0)
				return doc
			}(),
			expected: false,
		},
		{
			name: "Time range",
			query: &RangeQueryImpl{
				field: "timestamp",
				gt:    time.Date(2020, 1, 1, 0, 0, 0, 0, time.UTC),
				lt:    time.Date(2021, 1, 1, 0, 0, 0, 0, time.UTC),
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("timestamp", time.Date(2020, 6, 1, 0, 0, 0, 0, time.UTC))
				return doc
			}(),
			expected: true,
		},
		{
			name: "Inclusive time range",
			query: &RangeQueryImpl{
				field: "timestamp",
				gte:   time.Date(2020, 1, 1, 0, 0, 0, 0, time.UTC),
				lte:   time.Date(2021, 1, 1, 0, 0, 0, 0, time.UTC),
			},
			doc: func() *document.Document {
				doc := document.NewDocument()
				doc.AddField("timestamp", time.Date(2021, 1, 1, 0, 0, 0, 0, time.UTC)) // Test inclusive upper bound
				return doc
			}(),
			expected: true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			result := tt.query.Match(tt.doc)
			if result != tt.expected {
				t.Errorf("%s: RangeQuery.Match() = %v, want %v", tt.name, result, tt.expected)
			}
		})
	}
}

func TestBooleanQuery(t *testing.T) {
	t.Run("Must queries", func(t *testing.T) {
		query := NewBooleanQuery()
		query.AddMust(NewTermQuery("title", "test"))
		query.AddMust(NewTermQuery("category", "book"))

		tests := []struct {
			name  string
			value map[string]string
			want  bool
		}{
			{
				"All conditions met",
				map[string]string{"title": "test", "category": "book"},
				true,
			},
			{
				"One condition not met",
				map[string]string{"title": "test", "category": "article"},
				false,
			},
			{
				"No conditions met",
				map[string]string{"title": "other", "category": "article"},
				false,
			},
		}

		for _, tt := range tests {
			t.Run(tt.name, func(t *testing.T) {
				if got := query.Match(tt.value); got != tt.want {
					t.Errorf("BooleanQuery.Match() = %v, want %v", got, tt.want)
				}
			})
		}
	})

	t.Run("Should queries", func(t *testing.T) {
		query := NewBooleanQuery()
		query.AddShould(NewTermQuery("title", "test"))
		query.AddShould(NewTermQuery("title", "example"))

		tests := []struct {
			name  string
			value string
			want  bool
		}{
			{"First condition met", "test", true},
			{"Second condition met", "example", true},
			{"No conditions met", "other", false},
		}

		for _, tt := range tests {
			t.Run(tt.name, func(t *testing.T) {
				if got := query.Match(tt.value); got != tt.want {
					t.Errorf("BooleanQuery.Match() = %v, want %v", got, tt.want)
				}
			})
		}
	})

	t.Run("Must not queries", func(t *testing.T) {
		query := NewBooleanQuery()
		query.AddMustNot(NewTermQuery("status", "deleted"))

		tests := []struct {
			name  string
			value string
			want  bool
		}{
			{"Condition not met (good)", "active", true},
			{"Condition met (bad)", "deleted", false},
		}

		for _, tt := range tests {
			t.Run(tt.name, func(t *testing.T) {
				if got := query.Match(tt.value); got != tt.want {
					t.Errorf("BooleanQuery.Match() = %v, want %v", got, tt.want)
				}
			})
		}
	})
}

func TestQueryMapper(t *testing.T) {
	mapper := NewQueryMapper()

	t.Run("Term query mapping", func(t *testing.T) {
		dslQuery := map[string]interface{}{
			"term": map[string]interface{}{
				"title": "test",
			},
		}

		query, err := mapper.MapQuery(dslQuery)
		if err != nil {
			t.Fatalf("MapQuery() error = %v", err)
		}

		if query.Type() != TermQuery {
			t.Errorf("Expected TermQuery, got %v", query.Type())
		}

		if !query.Match("test") {
			t.Error("Query should match 'test'")
		}
	})

	t.Run("Range query mapping", func(t *testing.T) {
		dslQuery := map[string]interface{}{
			"range": map[string]interface{}{
				"price": map[string]interface{}{
					"gt": 10.0,
					"lt": 20.0,
				},
			},
		}

		query, err := mapper.MapQuery(dslQuery)
		if err != nil {
			t.Fatalf("MapQuery() error = %v", err)
		}

		if query.Type() != RangeQuery {
			t.Errorf("Expected RangeQuery, got %v", query.Type())
		}

		if !query.Match(15.0) {
			t.Error("Query should match 15.0")
		}
	})

	t.Run("Bool query mapping", func(t *testing.T) {
		dslQuery := map[string]interface{}{
			"bool": map[string]interface{}{
				"must": []interface{}{
					map[string]interface{}{
						"term": map[string]interface{}{
							"status": "active",
						},
					},
				},
				"must_not": []interface{}{
					map[string]interface{}{
						"term": map[string]interface{}{
							"deleted": "true",
						},
					},
				},
			},
		}

		query, err := mapper.MapQuery(dslQuery)
		if err != nil {
			t.Fatalf("MapQuery() error = %v", err)
		}

		if query.Type() != BooleanQuery {
			t.Errorf("Expected BooleanQuery, got %v", query.Type())
		}
	})

	t.Run("Invalid query", func(t *testing.T) {
		dslQuery := map[string]interface{}{
			"invalid": map[string]interface{}{},
		}

		if _, err := mapper.MapQuery(dslQuery); err == nil {
			t.Error("Expected error for invalid query type")
		}
	})
}

================
File: query/query.go
================
package query

import (
	"fmt"
	"my-indexer/document"
	"strings"
	"time"
)

// QueryType represents the type of internal query
type QueryType int

const (
	// TermQuery for exact term matches
	TermQuery QueryType = iota
	// PhraseQuery for phrase matches
	PhraseQuery
	// FieldQuery for field-specific queries
	FieldQuery
	// PrefixQuery for prefix matches
	PrefixQuery
	// RangeQuery for range comparisons
	RangeQuery
	// BooleanQuery for combining multiple queries
	BooleanQuery
	// MatchQuery for analyzed text queries
	MatchQuery
	// MatchPhraseQuery for exact phrase matches
	MatchPhraseQuery
	// MatchAllQuery for matching all documents
	MatchAllQuery
)

// Query represents the internal query interface
type Query interface {
	Type() QueryType
	Field() string
	Match(value interface{}) bool
}

// TermQueryImpl represents an exact term match query
type TermQueryImpl struct {
	field string
	term  string
}

func NewTermQuery(field, term string) *TermQueryImpl {
	return &TermQueryImpl{field: field, term: term}
}

func (q *TermQueryImpl) Type() QueryType { return TermQuery }
func (q *TermQueryImpl) Field() string   { return q.field }
func (q *TermQueryImpl) Term() string    { return q.term }
func (q *TermQueryImpl) Match(value interface{}) bool {
	if str, ok := value.(string); ok {
		return str == q.term
	}
	return false
}

// RangeQueryImpl implements a range query
type RangeQueryImpl struct {
	field string
	gt    interface{} // Exclusive greater than
	gte   interface{} // Inclusive greater than or equal
	lt    interface{} // Exclusive less than
	lte   interface{} // Inclusive less than or equal
}

func NewRangeQuery(field string) *RangeQueryImpl {
	return &RangeQueryImpl{field: field}
}

func (q *RangeQueryImpl) Type() QueryType { return RangeQuery }
func (q *RangeQueryImpl) Field() string   { return q.field }
func (q *RangeQueryImpl) Gt() interface{} { return q.gt }
func (q *RangeQueryImpl) Lt() interface{} { return q.lt }
func (q *RangeQueryImpl) Gte() interface{} { return q.gte }
func (q *RangeQueryImpl) Lte() interface{} { return q.lte }

// GreaterThan sets the exclusive greater than value for the range query
func (q *RangeQueryImpl) GreaterThan(val interface{}) {
	q.gt = val
	q.gte = nil // Clear inclusive operator
}

// GreaterThanOrEqual sets the inclusive greater than or equal value for the range query
func (q *RangeQueryImpl) GreaterThanOrEqual(val interface{}) {
	q.gte = val
	q.gt = nil // Clear exclusive operator
}

// LessThan sets the exclusive less than value for the range query
func (q *RangeQueryImpl) LessThan(val interface{}) {
	q.lt = val
	q.lte = nil // Clear inclusive operator
}

// LessThanOrEqual sets the inclusive less than or equal value for the range query
func (q *RangeQueryImpl) LessThanOrEqual(val interface{}) {
	q.lte = val
	q.lt = nil // Clear exclusive operator
}

func (q *RangeQueryImpl) matchNumeric(val float64) bool {
	if q.gt != nil {
		if gt, ok := q.gt.(float64); ok && val <= gt {
			return false
		}
	}
	if q.gte != nil {
		if gte, ok := q.gte.(float64); ok && val < gte {
			return false
		}
	}
	if q.lt != nil {
		if lt, ok := q.lt.(float64); ok && val >= lt {
			return false
		}
	}
	if q.lte != nil {
		if lte, ok := q.lte.(float64); ok && val > lte {
			return false
		}
	}
	return true
}

func (q *RangeQueryImpl) matchTime(val time.Time) bool {
	if q.gt != nil {
		if gt, ok := q.gt.(time.Time); ok && val.Before(gt) || val.Equal(gt) {
			return false
		}
	}
	if q.gte != nil {
		if gte, ok := q.gte.(time.Time); ok && val.Before(gte) {
			return false
		}
	}
	if q.lt != nil {
		if lt, ok := q.lt.(time.Time); ok && val.After(lt) || val.Equal(lt) {
			return false
		}
	}
	if q.lte != nil {
		if lte, ok := q.lte.(time.Time); ok && val.After(lte) {
			return false
		}
	}
	return true
}

func (q *RangeQueryImpl) Match(value interface{}) bool {
	// Handle direct value comparison first
	switch v := value.(type) {
	case float64:
		return q.matchNumeric(v)
	case time.Time:
		return q.matchTime(v)
	}

	// Handle document case
	doc, ok := value.(*document.Document)
	if !ok {
		return false
	}

	field, err := doc.GetField(q.field)
	if err != nil {
		return false
	}

	// Handle field value comparison
	switch v := field.Value.(type) {
	case float64:
		return q.matchNumeric(v)
	case time.Time:
		return q.matchTime(v)
	default:
		return false
	}
}

// BooleanQueryImpl represents a boolean combination of queries
type BooleanQueryImpl struct {
	field    string
	must     []Query
	should   []Query
	mustNot  []Query
	minMatch int
}

func NewBooleanQuery() *BooleanQueryImpl {
	return &BooleanQueryImpl{minMatch: 1}
}

func (q *BooleanQueryImpl) Type() QueryType { return BooleanQuery }
func (q *BooleanQueryImpl) Field() string   { return q.field }

func (q *BooleanQueryImpl) Must() []Query   { return q.must }
func (q *BooleanQueryImpl) Should() []Query { return q.should }

func (q *BooleanQueryImpl) AddMust(query Query)    { q.must = append(q.must, query) }
func (q *BooleanQueryImpl) AddShould(query Query)  { q.should = append(q.should, query) }
func (q *BooleanQueryImpl) AddMustNot(query Query) { q.mustNot = append(q.mustNot, query) }

func (q *BooleanQueryImpl) Match(value interface{}) bool {
	// Handle map values for field-specific queries
	valueMap, ok := value.(map[string]string)
	if !ok {
		// If not a map, treat as a single value
		// Must match all MUST queries
		for _, must := range q.must {
			if !must.Match(value) {
				return false
			}
		}

		// Must not match any MUST NOT queries
		for _, mustNot := range q.mustNot {
			if mustNot.Match(value) {
				return false
			}
		}

		// Must match at least minMatch of SHOULD queries if any exist
		if len(q.should) > 0 {
			matches := 0
			for _, should := range q.should {
				if should.Match(value) {
					matches++
					if matches >= q.minMatch {
						return true
					}
				}
			}
			return false
		}

		return true
	}

	// Handle map values for field-specific queries
	// Must match all MUST queries
	for _, must := range q.must {
		fieldValue, exists := valueMap[must.Field()]
		if !exists || !must.Match(fieldValue) {
			return false
		}
	}

	// Must not match any MUST NOT queries
	for _, mustNot := range q.mustNot {
		fieldValue, exists := valueMap[mustNot.Field()]
		if exists && mustNot.Match(fieldValue) {
			return false
		}
	}

	// Must match at least minMatch of SHOULD queries if any exist
	if len(q.should) > 0 {
		matches := 0
		for _, should := range q.should {
			fieldValue, exists := valueMap[should.Field()]
			if exists && should.Match(fieldValue) {
				matches++
				if matches >= q.minMatch {
					return true
				}
			}
		}
		return false
	}

	return true
}

// MatchQueryImpl represents a match query that matches analyzed text
type MatchQueryImpl struct {
	field string
	text  string
}

func NewMatchQuery(field, text string) *MatchQueryImpl {
	return &MatchQueryImpl{field: field, text: text}
}

func (q *MatchQueryImpl) Type() QueryType { return MatchQuery }
func (q *MatchQueryImpl) Field() string   { return q.field }
func (q *MatchQueryImpl) Text() string    { return q.text }
func (q *MatchQueryImpl) Match(value interface{}) bool {
	if str, ok := value.(string); ok {
		// For now, we'll do a simple case-insensitive contains check
		// In a real implementation, this would use the analyzer
		return strings.Contains(strings.ToLower(str), strings.ToLower(q.text))
	}
	return false
}

// MatchPhraseQueryImpl represents a match_phrase query that matches exact phrases
type MatchPhraseQueryImpl struct {
	field  string
	phrase string
}

func NewMatchPhraseQuery(field, phrase string) *MatchPhraseQueryImpl {
	return &MatchPhraseQueryImpl{field: field, phrase: phrase}
}

func (q *MatchPhraseQueryImpl) Type() QueryType { return MatchPhraseQuery }
func (q *MatchPhraseQueryImpl) Field() string   { return q.field }
func (q *MatchPhraseQueryImpl) Match(value interface{}) bool {
	if str, ok := value.(string); ok {
		// For now, we'll do a simple case-insensitive exact match
		// In a real implementation, this would use the analyzer
		return strings.EqualFold(str, q.phrase)
	}
	return false
}

// MatchAllQueryImpl represents a match_all query that matches all documents
type MatchAllQueryImpl struct{}

func NewMatchAllQuery() *MatchAllQueryImpl {
	return &MatchAllQueryImpl{}
}

func (q *MatchAllQueryImpl) Type() QueryType { return MatchAllQuery }
func (q *MatchAllQueryImpl) Field() string   { return "" }
func (q *MatchAllQueryImpl) Match(value interface{}) bool {
	return true
}

// QueryMapper maps ElasticSearch DSL queries to internal query representations
type QueryMapper struct{}

func NewQueryMapper() *QueryMapper {
	return &QueryMapper{}
}

// MapQuery maps an ElasticSearch DSL query to our internal query representation
func (m *QueryMapper) MapQuery(dslQuery map[string]interface{}) (Query, error) {
	if len(dslQuery) != 1 {
		return nil, fmt.Errorf("invalid query structure: expected exactly one root query type")
	}

	for queryType, queryBody := range dslQuery {
		switch queryType {
		case "term":
			return m.mapTermQuery(queryBody)
		case "match":
			return m.mapMatchQuery(queryBody)
		case "match_phrase":
			return m.mapMatchPhraseQuery(queryBody)
		case "match_all":
			return NewMatchAllQuery(), nil
		case "range":
			return m.mapRangeQuery(queryBody)
		case "bool":
			return m.mapBoolQuery(queryBody)
		default:
			return nil, fmt.Errorf("unsupported query type: %s", queryType)
		}
	}

	return nil, fmt.Errorf("invalid query structure")
}

func (m *QueryMapper) mapTermQuery(body interface{}) (Query, error) {
	termBody, ok := body.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid term query structure")
	}

	if len(termBody) != 1 {
		return nil, fmt.Errorf("term query must specify exactly one field")
	}

	for field, value := range termBody {
		switch v := value.(type) {
		case string:
			return NewTermQuery(field, v), nil
		case map[string]interface{}:
			if termValue, ok := v["value"].(string); ok {
				return NewTermQuery(field, termValue), nil
			}
			if termValue, ok := v["term"].(string); ok {
				return NewTermQuery(field, termValue), nil
			}
		}
		return nil, fmt.Errorf("term query value must be a string or {value: string}")
	}

	return nil, fmt.Errorf("invalid term query structure")
}

func (m *QueryMapper) mapRangeQuery(body interface{}) (Query, error) {
	rangeBody, ok := body.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid range query structure")
	}

	if len(rangeBody) != 1 {
		return nil, fmt.Errorf("range query must specify exactly one field")
	}

	for field, conditions := range rangeBody {
		condMap, ok := conditions.(map[string]interface{})
		if !ok {
			return nil, fmt.Errorf("invalid range conditions structure")
		}

		query := NewRangeQuery(field)
		for op, val := range condMap {
			switch op {
			case "gt":
				query.GreaterThan(val)
			case "gte":
				query.GreaterThanOrEqual(val)
			case "lt":
				query.LessThan(val)
			case "lte":
				query.LessThanOrEqual(val)
			default:
				return nil, fmt.Errorf("unsupported range operator: %s", op)
			}
		}
		return query, nil
	}

	return nil, fmt.Errorf("invalid range query structure")
}

func (m *QueryMapper) mapBoolQuery(body interface{}) (Query, error) {
	boolBody, ok := body.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid bool query structure")
	}

	query := NewBooleanQuery()

	for clause, queries := range boolBody {
		queryList, ok := queries.([]interface{})
		if !ok {
			return nil, fmt.Errorf("invalid bool clause structure for %s", clause)
		}

		for _, q := range queryList {
			queryMap, ok := q.(map[string]interface{})
			if !ok {
				return nil, fmt.Errorf("invalid query in bool clause")
			}

			subQuery, err := m.MapQuery(queryMap)
			if err != nil {
				return nil, fmt.Errorf("failed to parse query in bool clause: %v", err)
			}

			switch clause {
			case "must":
				query.AddMust(subQuery)
			case "should":
				query.AddShould(subQuery)
			case "must_not":
				query.AddMustNot(subQuery)
			default:
				return nil, fmt.Errorf("unsupported bool clause: %s", clause)
			}
		}
	}

	return query, nil
}

func (m *QueryMapper) mapMatchQuery(body interface{}) (Query, error) {
	matchBody, ok := body.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid match query structure")
	}

	if len(matchBody) != 1 {
		return nil, fmt.Errorf("match query must specify exactly one field")
	}

	for field, value := range matchBody {
		switch v := value.(type) {
		case string:
			return NewMatchQuery(field, v), nil
		case map[string]interface{}:
			if query, ok := v["query"].(string); ok {
				return NewMatchQuery(field, query), nil
			}
			if query, ok := v["value"].(string); ok {
				return NewMatchQuery(field, query), nil
			}
		}
		return nil, fmt.Errorf("match query value must be a string or {query: string}")
	}

	return nil, fmt.Errorf("invalid match query structure")
}

func (m *QueryMapper) mapMatchPhraseQuery(body interface{}) (Query, error) {
	phraseBody, ok := body.(map[string]interface{})
	if !ok {
		return nil, fmt.Errorf("invalid match_phrase query structure")
	}

	if len(phraseBody) != 1 {
		return nil, fmt.Errorf("match_phrase query must specify exactly one field")
	}

	for field, value := range phraseBody {
		switch v := value.(type) {
		case string:
			return NewMatchPhraseQuery(field, v), nil
		case map[string]interface{}:
			if query, ok := v["query"].(string); ok {
				return NewMatchPhraseQuery(field, query), nil
			}
		}
		return nil, fmt.Errorf("invalid match_phrase query value")
	}

	return nil, fmt.Errorf("invalid match_phrase query structure")
}

================
File: router/bulk.go
================
package router

import (
	"bufio"
	"encoding/json"
	"fmt"
	"net/http"
	"strings"

	"my-indexer/document"
)

// handleBulk handles bulk operations
func (r *Router) handleBulk(w http.ResponseWriter, req *http.Request) {
	// Only allow POST method
	if req.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	// Validate content type
	if req.Header.Get("Content-Type") != "application/x-ndjson" {
		http.Error(w, "Content-Type must be application/x-ndjson", http.StatusBadRequest)
		return
	}

	// Parse path to get index name
	parts := strings.Split(req.URL.Path, "/")
	if len(parts) < 2 || parts[1] == "" {
		http.Error(w, "Invalid index name", http.StatusBadRequest)
		return
	}
	indexName := parts[1]

	// Process bulk request
	scanner := bufio.NewScanner(req.Body)
	defer req.Body.Close()

	var responses []map[string]interface{}
	var currentAction map[string]interface{}
	lineNum := 0

	for scanner.Scan() {
		line := scanner.Text()
		if line == "" {
			continue // Skip empty lines
		}

		lineNum++
		if lineNum%2 == 1 {
			// Action line
			if err := json.Unmarshal([]byte(line), &currentAction); err != nil {
				http.Error(w, fmt.Sprintf("Invalid JSON at line %d: %v", lineNum, err), http.StatusBadRequest)
				return
			}

			// Validate action
			if len(currentAction) != 1 {
				http.Error(w, fmt.Sprintf("Invalid action at line %d: exactly one action type expected", lineNum), http.StatusBadRequest)
				return
			}

			// Check for valid action types
			validAction := false
			for _, actionType := range []string{"index", "create", "update", "delete"} {
				if _, ok := currentAction[actionType]; ok {
					validAction = true
					break
				}
			}
			if !validAction {
				http.Error(w, fmt.Sprintf("Invalid action type at line %d: must be one of index, create, update, or delete", lineNum), http.StatusBadRequest)
				return
			}
		} else {
			// Document line (for index/create/update operations)
			var doc map[string]interface{}
			if err := json.Unmarshal([]byte(line), &doc); err != nil {
				http.Error(w, fmt.Sprintf("Invalid JSON at line %d: %v", lineNum, err), http.StatusBadRequest)
				return
			}

			// Process the action
			response := make(map[string]interface{})
			switch {
			case currentAction["index"] != nil:
				// Create a new document
				newDoc := document.NewDocument()
				for field, value := range doc {
					newDoc.AddField(field, value)
				}

				// Add the document to the index
				docID, err := r.index.AddDocument(newDoc)
				if err != nil {
					response["index"] = map[string]interface{}{
						"_index":  indexName,
						"_id":     fmt.Sprintf("%d", docID),
						"status":  "error",
						"message": err.Error(),
					}
				} else {
					response["index"] = map[string]interface{}{
						"_index": indexName,
						"_id":    fmt.Sprintf("%d", docID),
						"status": "success",
					}
				}
			// Add other action types (create, update, delete) here
			default:
				http.Error(w, "Unsupported action type", http.StatusBadRequest)
				return
			}
			responses = append(responses, response)
		}
	}

	if err := scanner.Err(); err != nil {
		http.Error(w, fmt.Sprintf("Error reading request body: %v", err), http.StatusBadRequest)
		return
	}

	// Send response
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(map[string]interface{}{
		"took":      0, // TODO: Add timing
		"errors":    false,
		"responses": responses,
	})
}

================
File: router/router_test.go
================
package router

import (
	"bytes"
	"net/http"
	"net/http/httptest"
	"strings"
	"testing"
)

func TestValidateDocumentRequest(t *testing.T) {
	tests := []struct {
		name        string
		method      string
		path        string
		body        string
		wantErr     error
	}{
		{
			name:    "Valid PUT request",
			method:  http.MethodPut,
			path:    "/test-index/_doc/123",
			body:    `{"field": "value"}`,
			wantErr: nil,
		},
		{
			name:    "Valid GET request",
			method:  http.MethodGet,
			path:    "/test-index/_doc/123",
			body:    "",
			wantErr: nil,
		},
		{
			name:    "Valid DELETE request",
			method:  http.MethodDelete,
			path:    "/test-index/_doc/123",
			body:    "",
			wantErr: nil,
		},
		{
			name:    "Missing index name",
			method:  http.MethodPut,
			path:    "/_doc/123",
			body:    `{"field": "value"}`,
			wantErr: ErrInvalidIndex,
		},
		{
			name:    "Missing document ID",
			method:  http.MethodPut,
			path:    "/test-index/_doc/",
			body:    `{"field": "value"}`,
			wantErr: ErrInvalidDocID,
		},
		{
			name:    "Invalid path format",
			method:  http.MethodPut,
			path:    "/test-index/123",
			body:    `{"field": "value"}`,
			wantErr: ErrInvalidIndex,
		},
		{
			name:    "PUT request without body",
			method:  http.MethodPut,
			path:    "/test-index/_doc/123",
			body:    "",
			wantErr: ErrMissingBody,
		},
		{
			name:    "PUT request with invalid JSON",
			method:  http.MethodPut,
			path:    "/test-index/_doc/123",
			body:    `{"field": invalid}`,
			wantErr: ErrInvalidJSON,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			req := httptest.NewRequest(tt.method, tt.path, strings.NewReader(tt.body))
			if tt.method == http.MethodPut && tt.body != "" {
				req.Header.Set("Content-Type", "application/json")
			}
			
			err := validateDocumentRequest(req)
			
			if err != tt.wantErr {
				t.Errorf("validateDocumentRequest() error = %v, wantErr %v", err, tt.wantErr)
			}
		})
	}
}

func TestDocumentEndpoint(t *testing.T) {
	router := NewRouter()

	tests := []struct {
		name           string
		method         string
		path           string
		body           string
		expectedStatus int
	}{
		{
			name:           "Valid PUT request",
			method:         http.MethodPut,
			path:          "/test-index/_doc/1",
			body:          `{"field": "value"}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Valid GET request",
			method:         http.MethodGet,
			path:          "/test-index/_doc/1",
			body:          "",
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Valid DELETE request",
			method:         http.MethodDelete,
			path:          "/test-index/_doc/1",
			body:          "",
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Invalid method",
			method:         http.MethodPost,
			path:          "/test-index/_doc/1",
			body:          "",
			expectedStatus: http.StatusMethodNotAllowed,
		},
		{
			name:           "Invalid JSON in PUT request",
			method:         http.MethodPut,
			path:          "/test-index/_doc/1",
			body:          `{"field": invalid}`,
			expectedStatus: http.StatusBadRequest,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			req := httptest.NewRequest(tt.method, tt.path, strings.NewReader(tt.body))
			w := httptest.NewRecorder()
			router.ServeHTTP(w, req)

			if w.Code != tt.expectedStatus {
				t.Errorf("expected status %d but got %d", tt.expectedStatus, w.Code)
			}
		})
	}
}

func TestBulkEndpoint(t *testing.T) {
	router := NewRouter()

	tests := []struct {
		name           string
		method         string
		body           string
		expectedStatus int
	}{
		{
			name:           "Valid bulk request",
			method:         http.MethodPost,
			body:          `{"index": {"_index": "test", "_id": "1"}}
{"field1": "value1", "field2": "value2"}
{"index": {"_index": "test", "_id": "2"}}
{"field1": "value3", "field2": "value4"}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Invalid method",
			method:         http.MethodPut,
			body:          "",
			expectedStatus: http.StatusMethodNotAllowed,
		},
		{
			name:           "Invalid JSON",
			method:         http.MethodPost,
			body:          `{"invalid`,
			expectedStatus: http.StatusBadRequest,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			req := httptest.NewRequest(tt.method, "/test/_bulk", bytes.NewBufferString(tt.body))
			if tt.method == http.MethodPost {
				req.Header.Set("Content-Type", "application/x-ndjson")
			}
			w := httptest.NewRecorder()
			router.ServeHTTP(w, req)

			if w.Code != tt.expectedStatus {
				t.Errorf("expected status %d but got %d", tt.expectedStatus, w.Code)
			}
		})
	}
}

func TestSearchEndpoint(t *testing.T) {
	router := NewRouter()

	// Add test data
	req := httptest.NewRequest(http.MethodPut, "/test-index/_doc/1", strings.NewReader(`{"field": "value"}`))
	w := httptest.NewRecorder()
	router.ServeHTTP(w, req)
	if w.Code != http.StatusOK {
		t.Fatalf("failed to set up test data: %d", w.Code)
	}

	tests := []struct {
		name           string
		method         string
		body           string
		expectedStatus int
	}{
		{
			name:           "Valid GET request",
			method:         http.MethodGet,
			body:          "",
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Valid POST request",
			method:         http.MethodPost,
			body:          `{"query": {"match_all": {}}}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Invalid method",
			method:         http.MethodPut,
			body:          "",
			expectedStatus: http.StatusMethodNotAllowed,
		},
		{
			name:           "Invalid JSON in POST request",
			method:         http.MethodPost,
			body:          `{"query": invalid}`,
			expectedStatus: http.StatusBadRequest,
		},
		{
			name:           "Unsupported query type",
			method:         http.MethodPost,
			body:          `{"query": {"unknown_type": {}}}`,
			expectedStatus: http.StatusBadRequest,
		},
		{
			name:           "Empty query object",
			method:         http.MethodPost,
			body:          `{"query": {}}`,
			expectedStatus: http.StatusBadRequest,
		},
		{
			name:           "Invalid query structure",
			method:         http.MethodPost,
			body:          `{"query": "not an object"}`,
			expectedStatus: http.StatusBadRequest,
		},
		{
			name:           "Match query",
			method:         http.MethodPost,
			body:          `{"query": {"match": {"field": "value"}}}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Term query",
			method:         http.MethodPost,
			body:          `{"query": {"term": {"field": "value"}}}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Range query",
			method:         http.MethodPost,
			body:          `{"query": {"range": {"field": {"gt": 5}}}}`,
			expectedStatus: http.StatusOK,
		},
		{
			name:           "Bool query",
			method:         http.MethodPost,
			body:          `{"query": {"bool": {"must": [{"match": {"field": "value"}}]}}}`,
			expectedStatus: http.StatusOK,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			req := httptest.NewRequest(tt.method, "/test-index/_search", strings.NewReader(tt.body))
			if tt.method == http.MethodPost {
				req.Header.Set("Content-Type", "application/json")
			}
			w := httptest.NewRecorder()
			router.ServeHTTP(w, req)

			if w.Code != tt.expectedStatus {
				t.Errorf("expected status %d but got %d for test %s", tt.expectedStatus, w.Code, tt.name)
			}
		})
	}
}

================
File: router/router.go
================
package router

import (
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"
	"time"

	"my-indexer/logger"
	"my-indexer/index"
	"my-indexer/analysis"
	"my-indexer/document"
	"my-indexer/search"
	"my-indexer/query"
)

// IndexDocumentStore adapts Index to implement search.DocumentStore
type IndexDocumentStore struct {
	idx *index.Index
}

// LoadDocument implements search.DocumentStore
func (s *IndexDocumentStore) LoadDocument(docID int) (*document.Document, error) {
	return s.idx.GetDocument(docID)
}

// LoadAllDocuments implements search.DocumentStore
func (s *IndexDocumentStore) LoadAllDocuments() ([]*document.Document, error) {
	return s.idx.GetAllDocuments()
}

// Router handles HTTP requests for the indexer
type Router struct {
	mux    *http.ServeMux
	index  *index.Index
	search *search.Search
}

// NewRouter creates a new Router instance
func NewRouter() *Router {
	analyzer := analysis.NewStandardAnalyzer()
	idx := index.NewIndex(analyzer)
	store := &IndexDocumentStore{idx: idx}
	
	router := &Router{
		mux:    http.NewServeMux(),
		index:  idx,
		search: search.NewSearch(idx, store),
	}

	// Initialize the logger
	logger.Initialize()

	// Register handlers
	router.RegisterElasticSearchHandlers()

	return router
}

// Close performs cleanup of router resources
func (r *Router) Close() {
	logger.Close()
}

// ServeHTTP implements the http.Handler interface
func (r *Router) ServeHTTP(w http.ResponseWriter, req *http.Request) {
	// Log the request
	logger.Info("Received request: %s %s", req.Method, req.URL.Path)

	// Handle the request based on the path
	if strings.Contains(req.URL.Path, "/_doc/") {
		r.handleDocument(w, req)
		return
	}

	if strings.HasSuffix(req.URL.Path, "/_bulk") {
		r.handleBulk(w, req)
		return
	}

	if strings.Contains(req.URL.Path, "/_search") {
		r.handleSearch(w, req)
		return
	}

	if strings.HasSuffix(req.URL.Path, "/_msearch") {
		r.handleMultiSearch(w, req)
		return
	}

	if strings.HasSuffix(req.URL.Path, "/_cat/indices") {
		r.handleListIndices(w, req)
		return
	}

	if strings.HasSuffix(req.URL.Path, "/_scroll") {
		r.handleScroll(w, req)
		return
	}

	if strings.HasSuffix(req.URL.Path, "/_index") {
		r.handleIndex(w, req)
		return
	}

	// Not found
	http.NotFound(w, req)
}

// RegisterElasticSearchHandlers registers all ElasticSearch-compatible endpoints
func (r *Router) RegisterElasticSearchHandlers() {
	// Document API endpoints
	r.mux.HandleFunc("/", r.handleDocument)                // Single document operations (matches /index/_doc/id)
	r.mux.HandleFunc("/_index", r.handleIndex)            // Index API endpoint
	r.mux.HandleFunc("/_bulk", r.handleBulk)              // Bulk operations
	r.mux.HandleFunc("/_search", r.handleSearch)          // Search
	r.mux.HandleFunc("/_msearch", r.handleMultiSearch)    // Multi-search
	r.mux.HandleFunc("/_cat/indices", r.handleListIndices) // List indices
	r.mux.HandleFunc("/_scroll", r.handleScroll)          // Scroll API
}

// ElasticSearchResponse represents a standard ES response format
type ElasticSearchResponse struct {
	Took     int  `json:"took"`
	TimedOut bool `json:"timed_out"`
	Shards   struct {
		Total      int `json:"total"`
		Successful int `json:"successful"`
		Failed     int `json:"failed"`
	} `json:"_shards"`
	Result string `json:"result,omitempty"`
	Status int    `json:"status,omitempty"`
}

// errorResponse sends an error response in JSON format
func (r *Router) errorResponse(w http.ResponseWriter, code int, message string) {
	logger.Error("Error response: %s (code: %d)", message, code)
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(code)
	json.NewEncoder(w).Encode(map[string]string{"error": message})
}

// Handler functions for ElasticSearch-compatible endpoints
func (r *Router) handleDocument(w http.ResponseWriter, req *http.Request) {
	logger.Info("Handling document request: %s %s", req.Method, req.URL.Path)

	// Check method first
	if req.Method != http.MethodPut && req.Method != http.MethodGet && req.Method != http.MethodDelete {
		r.errorResponse(w, http.StatusMethodNotAllowed, "method not allowed")
		return
	}

	// Validate the request
	if err := validateDocumentRequest(req); err != nil {
		r.errorResponse(w, http.StatusBadRequest, err.Error())
		return
	}

	// Extract index name and document ID from path
	parts := strings.Split(req.URL.Path, "/")
	indexName := parts[1]
	docID := parts[3]

	switch req.Method {
	case http.MethodPut:
		// TODO: Implement document creation/update
		logger.Info("Creating/updating document: index=%s, id=%s", indexName, docID)
		w.WriteHeader(http.StatusOK)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"_index": indexName,
			"_id":    docID,
			"result": "created",
			"status": http.StatusOK,
		})

	case http.MethodGet:
		// TODO: Implement document retrieval
		logger.Info("Retrieving document: index=%s, id=%s", indexName, docID)
		w.WriteHeader(http.StatusOK)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"_index": indexName,
			"_id":    docID,
			"found":  true,
			"status": http.StatusOK,
		})

	case http.MethodDelete:
		// TODO: Implement document deletion
		logger.Info("Deleting document: index=%s, id=%s", indexName, docID)
		w.WriteHeader(http.StatusOK)
		json.NewEncoder(w).Encode(map[string]interface{}{
			"_index": indexName,
			"_id":    docID,
			"result": "deleted",
			"status": http.StatusOK,
		})
	}
}

func (r *Router) handleSearch(w http.ResponseWriter, req *http.Request) {
	// Only allow GET and POST methods
	if req.Method != http.MethodGet && req.Method != http.MethodPost {
		http.Error(w, "Method not allowed", http.StatusMethodNotAllowed)
		return
	}

	var queryMapObj map[string]interface{}
	var err error

	if req.Method == http.MethodGet {
		// For GET requests without a query parameter, use match_all query
		queryStr := req.URL.Query().Get("q")
		if queryStr == "" {
			queryMapObj = map[string]interface{}{
				"match_all": map[string]interface{}{},
			}
		} else {
			// Create a match query for the q parameter
			queryMapObj = map[string]interface{}{
				"match": map[string]interface{}{
					"_all": queryStr,
				},
			}
		}
	} else {
		// Parse query from request body for POST
		body, err := io.ReadAll(req.Body)
		if err != nil {
			http.Error(w, "Failed to read request body", http.StatusBadRequest)
			return
		}
		defer req.Body.Close()

		var searchRequest struct {
			Query map[string]interface{} `json:"query"`
		}

		if err := json.Unmarshal(body, &searchRequest); err != nil {
			http.Error(w, "Invalid JSON in request body", http.StatusBadRequest)
			return
		}

		if searchRequest.Query == nil {
			http.Error(w, "Query object is required", http.StatusBadRequest)
			return
		}

		queryMapObj = searchRequest.Query
	}

	// Initialize query mapper
	queryMapper := query.NewQueryMapper()

	// Prepare query wrapper
	var queryWrapper map[string]interface{}

	// If the query is a direct match/term/range/bool query
	if queryType, ok := getQueryType(queryMapObj); ok {
		switch queryType {
		case "match", "term", "match_phrase", "match_all", "range", "bool":
			// For match queries, ensure proper structure
			if queryType == "match" {
				if fieldMap, ok := queryMapObj[queryType].(map[string]interface{}); ok {
					for field, fieldValue := range fieldMap {
						// If the field value is a string, wrap it in a query object
						if _, ok := fieldValue.(string); ok {
							fieldMap[field] = map[string]interface{}{
								"query": fieldValue,
							}
						}
					}
				}
			}
			queryWrapper = queryMapObj
			goto processQuery
		}
	}

	// If no valid query type found, treat the entire query object as a match query
	queryWrapper = map[string]interface{}{"match": queryMapObj}

processQuery:
	// Pass the query object to the mapper
	queryObj, err := queryMapper.MapQuery(queryWrapper)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to map query: %v", err), http.StatusBadRequest)
		return
	}

	// Execute the query
	results, err := r.search.SearchWithQuery(queryObj)
	if err != nil {
		http.Error(w, fmt.Sprintf("Failed to execute search: %v", err), http.StatusInternalServerError)
		return
	}

	// Return results
	w.Header().Set("Content-Type", "application/json")
	json.NewEncoder(w).Encode(results)
}

func getQueryType(query map[string]interface{}) (string, bool) {
	for queryType := range query {
		return queryType, true
	}
	return "", false
}

func (r *Router) handleMultiSearch(w http.ResponseWriter, req *http.Request) {
	// TODO: Implement multi-search
	http.Error(w, "Not implemented", http.StatusNotImplemented)
}

func (r *Router) handleListIndices(w http.ResponseWriter, req *http.Request) {
	if req.Method != http.MethodGet {
		r.errorResponse(w, http.StatusMethodNotAllowed, "only GET method is allowed")
		return
	}
	// TODO: Implement list indices
	http.Error(w, "Not implemented", http.StatusNotImplemented)
}

func (r *Router) handleScroll(w http.ResponseWriter, req *http.Request) {
	// TODO: Implement scroll API
	http.Error(w, "Not implemented", http.StatusNotImplemented)
}

func (r *Router) handleIndex(w http.ResponseWriter, req *http.Request) {
	if req.Method != http.MethodPost && req.Method != http.MethodPut {
		r.errorResponse(w, http.StatusMethodNotAllowed, "method not allowed")
		return
	}

	// Parse the request body
	var doc map[string]interface{}
	if err := json.NewDecoder(req.Body).Decode(&doc); err != nil {
		r.errorResponse(w, http.StatusBadRequest, "invalid request body")
		return
	}

	// Extract index name and document ID from URL path
	parts := strings.Split(strings.Trim(req.URL.Path, "/"), "/")
	if len(parts) < 1 {
		r.errorResponse(w, http.StatusBadRequest, "invalid index path")
		return
	}

	indexName := parts[0]
	docID := ""
	if len(parts) > 1 {
		docID = parts[1]
	}

	// Index the document
	startTime := time.Now()
	err := r.index.IndexDocument(indexName, docID, doc)
	if err != nil {
		r.errorResponse(w, http.StatusInternalServerError, err.Error())
		return
	}

	// Prepare ElasticSearch-compatible response
	resp := ElasticSearchResponse{
		Took:     int(time.Since(startTime).Milliseconds()),
		TimedOut: false,
		Shards: struct {
			Total      int `json:"total"`
			Successful int `json:"successful"`
			Failed     int `json:"failed"`
		}{
			Total:      1,
			Successful: 1,
			Failed:     0,
		},
		Result: "created",
		Status: http.StatusCreated,
	}

	// Send response
	w.Header().Set("Content-Type", "application/json")
	w.WriteHeader(http.StatusCreated)
	json.NewEncoder(w).Encode(resp)
}

================
File: router/validation.go
================
package router

import (
	"bufio"
	"encoding/json"
	"errors"
	"fmt"
	"io"
	"net/http"
	"strings"
)

const (
	// MaxRequestBodySize is 10MB
	MaxRequestBodySize = 10 * 1024 * 1024
)

var (
	ErrInvalidJSON     = errors.New("invalid JSON in request body")
	ErrBodyTooLarge    = errors.New("request body too large")
	ErrEmptyBody       = errors.New("request body is empty")
	ErrMissingBody     = errors.New("request body is required")
	ErrInvalidIndex    = errors.New("invalid index name")
	ErrInvalidDocID    = errors.New("invalid document ID")
	ErrInvalidBulkData = errors.New("invalid bulk request data")
)

// validateRequestBody checks if the request body is present and not too large
func validateRequestBody(r *http.Request) ([]byte, error) {
	if r.Body == nil {
		return nil, ErrMissingBody
	}
	defer r.Body.Close()

	// Set size limit on request body
	r.Body = http.MaxBytesReader(nil, r.Body, MaxRequestBodySize)

	body, err := io.ReadAll(r.Body)
	if err != nil {
		if strings.Contains(err.Error(), "http: request body too large") {
			return nil, ErrBodyTooLarge
		}
		return nil, fmt.Errorf("failed to read request body: %v", err)
	}

	if len(body) == 0 {
		return nil, ErrEmptyBody
	}

	return body, nil
}

// validateJSONBody validates that the request body contains valid JSON
func validateJSONBody(body []byte) error {
	var js json.RawMessage
	if err := json.Unmarshal(body, &js); err != nil {
		return ErrInvalidJSON
	}
	return nil
}

// validateDocumentRequest validates a document API request
func validateDocumentRequest(r *http.Request) error {
	// Extract and validate index name and document ID from path
	parts := strings.Split(strings.Trim(r.URL.Path, "/"), "/")
	
	// Check basic path structure
	if len(parts) < 3 {
		if len(parts) >= 2 && parts[1] == "_doc" {
			return ErrInvalidDocID
		}
		return ErrInvalidIndex
	}

	// Validate index name first
	indexName := parts[0]
	if indexName == "" || indexName == "_doc" {
		return ErrInvalidIndex
	}

	// Validate _doc part
	if parts[1] != "_doc" {
		return ErrInvalidIndex
	}

	// Validate document ID
	if parts[2] == "" {
		return ErrInvalidDocID
	}

	// For PUT requests, validate the request body
	if r.Method == http.MethodPut {
		if r.Body == nil {
			return ErrMissingBody
		}
		body, err := io.ReadAll(r.Body)
		if err != nil {
			return err
		}
		if len(body) == 0 {
			return ErrMissingBody
		}

		if !json.Valid(body) {
			return ErrInvalidJSON
		}
	}

	return nil
}

// validateBulkRequest validates a bulk API request
func validateBulkRequest(r *http.Request) error {
	// Validate Content-Type for NDJSON format
	if r.Header.Get("Content-Type") != "application/x-ndjson" {
		return fmt.Errorf("invalid Content-Type, expected application/x-ndjson")
	}

	// Limit request body size to 10MB
	r.Body = http.MaxBytesReader(nil, r.Body, 10<<20)
	defer r.Body.Close()

	// Read and validate each line as a separate JSON object
	scanner := bufio.NewScanner(r.Body)
	lineCount := 0
	for scanner.Scan() {
		line := scanner.Text()
		if line == "" {
			continue // Skip empty lines
		}

		var action map[string]interface{}
		if err := json.Unmarshal([]byte(line), &action); err != nil {
			return fmt.Errorf("invalid JSON at line %d: %v", lineCount+1, err)
		}

		// Validate action type
		if len(action) != 1 {
			return fmt.Errorf("invalid action at line %d: exactly one action type expected", lineCount+1)
		}

		// Check for valid action types
		validAction := false
		for _, actionType := range []string{"index", "create", "update", "delete"} {
			if _, ok := action[actionType]; ok {
				validAction = true
				break
			}
		}
		if !validAction {
			return fmt.Errorf("invalid action type at line %d: must be one of index, create, update, or delete", lineCount+1)
		}

		lineCount++
	}

	if err := scanner.Err(); err != nil {
		return fmt.Errorf("error reading request body: %v", err)
	}

	if lineCount == 0 {
		return fmt.Errorf("empty bulk request")
	}

	return nil
}

// validateSearchRequest validates a search API request
func validateSearchRequest(r *http.Request) error {
	// Extract and validate index name from path
	parts := strings.Split(r.URL.Path, "/")
	if len(parts) < 3 {
		return ErrInvalidIndex
	}

	indexName := parts[1]
	if indexName == "" {
		return ErrInvalidIndex
	}

	// Validate method
	if r.Method != http.MethodGet && r.Method != http.MethodPost {
		return &validationError{
			status:  http.StatusMethodNotAllowed,
			message: "only GET and POST methods are allowed",
		}
	}

	// For POST requests, validate the Content-Type and request body
	if r.Method == http.MethodPost {
		// Validate Content-Type header
		contentType := r.Header.Get("Content-Type")
		if contentType != "" {
			mediaType := strings.ToLower(strings.Split(contentType, ";")[0])
			if mediaType != "application/json" {
				return &validationError{
					status:  http.StatusBadRequest,
					message: fmt.Sprintf("invalid Content-Type: expected 'application/json', got '%s'", contentType),
				}
			}
		}

		// Limit request body size to 10MB to prevent memory exhaustion
		r.Body = http.MaxBytesReader(nil, r.Body, 10<<20)
		
		// Read and validate JSON structure
		var body map[string]interface{}
		decoder := json.NewDecoder(r.Body)
		
		if err := decoder.Decode(&body); err != nil {
			switch {
			case err.Error() == "http: request body too large":
				return fmt.Errorf("request body exceeds 10MB limit")
			case strings.Contains(err.Error(), "cannot unmarshal"):
				return fmt.Errorf("malformed JSON: %v", err)
			default:
				return fmt.Errorf("invalid JSON: %v", err)
			}
		}

		// Validate query structure if present
		if query, exists := body["query"]; exists {
			queryMap, ok := query.(map[string]interface{})
			if !ok {
				return &validationError{
					status:  http.StatusBadRequest,
					message: "'query' must be an object",
				}
			}

			// Validate supported query types
			if len(queryMap) == 0 {
				return &validationError{
					status:  http.StatusBadRequest,
					message: "empty query object",
				}
			}

			// Check for valid query types
			validQueryTypes := map[string]bool{
				"match_all": true,
				"match":     true,
				"term":      true,
				"range":     true,
				"bool":      true,
			}

			// Helper function to validate field values
			validateFieldValue := func(value interface{}) error {
				switch v := value.(type) {
				case map[string]interface{}, string, float64, int, bool, []interface{}:
					return nil // Allow arrays, objects and primitive types
				default:
					return fmt.Errorf("invalid field value type: %T", v)
				}
			}

			// Helper function to validate query clauses recursively
			var validateQueryClause func(clause map[string]interface{}) error
			validateQueryClause = func(clause map[string]interface{}) error {
				for queryType, value := range clause {
					if !validQueryTypes[queryType] {
						return &validationError{
							status:  http.StatusBadRequest,
							message: fmt.Sprintf("unsupported query type: %s", queryType),
						}
					}

					switch queryType {
					case "match_all":
						// match_all can be empty object or null
						if value != nil {
							_, ok := value.(map[string]interface{})
							if !ok {
								return &validationError{
									status:  http.StatusBadRequest,
									message: "match_all value must be an object or null",
								}
							}
						}
					case "match", "term":
						valueMap, ok := value.(map[string]interface{})
						if !ok {
							return &validationError{
								status:  http.StatusBadRequest,
								message: fmt.Sprintf("%s query must be an object", queryType),
							}
						}
						for field, fieldValue := range valueMap {
							if err := validateFieldValue(fieldValue); err != nil {
								return &validationError{
									status:  http.StatusBadRequest,
									message: fmt.Sprintf("invalid value for field %s: %v", field, err),
								}
							}
						}
					case "range":
						rangeMap, ok := value.(map[string]interface{})
						if !ok {
							return &validationError{
								status:  http.StatusBadRequest,
								message: "range query must be an object",
							}
						}
						for field, conditions := range rangeMap {
							condMap, ok := conditions.(map[string]interface{})
							if !ok {
								return &validationError{
									status:  http.StatusBadRequest,
									message: fmt.Sprintf("range conditions for field %s must be an object", field),
								}
							}
							for op, val := range condMap {
								switch op {
								case "gt", "gte", "lt", "lte", "eq":
									if err := validateFieldValue(val); err != nil {
										return &validationError{
											status:  http.StatusBadRequest,
											message: fmt.Sprintf("invalid range value for %s: %v", op, err),
										}
									}
								default:
									return &validationError{
										status:  http.StatusBadRequest,
										message: fmt.Sprintf("unsupported range operator: %s", op),
									}
								}
							}
						}
					case "bool":
						boolMap, ok := value.(map[string]interface{})
						if !ok {
							return &validationError{
								status:  http.StatusBadRequest,
								message: "bool query must be an object",
							}
						}
						for clause, clauseValue := range boolMap {
							switch clause {
							case "must", "should", "must_not", "filter":
								switch clauses := clauseValue.(type) {
								case []interface{}:
									for _, subQuery := range clauses {
										subQueryMap, ok := subQuery.(map[string]interface{})
										if !ok {
											return &validationError{
												status:  http.StatusBadRequest,
												message: fmt.Sprintf("bool %s array elements must be objects", clause),
											}
										}
										if err := validateQueryClause(subQueryMap); err != nil {
											return err
										}
									}
								case map[string]interface{}:
									if err := validateQueryClause(clauses); err != nil {
										return err
									}
								default:
									return &validationError{
										status:  http.StatusBadRequest,
										message: fmt.Sprintf("bool %s clauses must be an object or array of objects", clause),
									}
								}
							case "minimum_should_match":
								switch v := clauseValue.(type) {
								case float64, int:
									// These are valid types
								default:
									return &validationError{
										status:  http.StatusBadRequest,
										message: fmt.Sprintf("minimum_should_match must be a number, got %T", v),
									}
								}
							default:
								return &validationError{
									status:  http.StatusBadRequest,
									message: fmt.Sprintf("unsupported bool operation: %s", clause),
								}
							}
						}
					}
				}
				return nil
			}

			// Validate the query structure
			if err := validateQueryClause(queryMap); err != nil {
				return err
			}
		}
	}

	return nil
}

type validationError struct {
	status  int
	message string
}

func (e *validationError) Error() string {
	return e.message
}

================
File: search/es_formatter.go
================
package search

import (
	"time"
)

// ESResponse represents an ElasticSearch-compatible response
type ESResponse struct {
	Took     int        `json:"took"`
	TimedOut bool       `json:"timed_out"`
	Shards   ESShards   `json:"_shards"`
	Hits     ESHits     `json:"hits"`
}

// ESShards represents shard information in an ES response
type ESShards struct {
	Total      int `json:"total"`
	Successful int `json:"successful"`
	Skipped    int `json:"skipped"`
	Failed     int `json:"failed"`
}

// ESHits represents the hits section of an ES response
type ESHits struct {
	Total    ESTotal   `json:"total"`
	MaxScore float64   `json:"max_score"`
	Hits     []ESHit   `json:"hits"`
}

// ESTotal represents the total hits information
type ESTotal struct {
	Value    int    `json:"value"`
	Relation string `json:"relation"`
}

// ESHit represents a single hit in an ES response
type ESHit struct {
	Index  string                 `json:"_index"`
	ID     string                 `json:"_id"`
	Score  float64               `json:"_score"`
	Source map[string]interface{} `json:"_source"`
}

// FormatESResponse formats search results into an ElasticSearch-compatible response
func FormatESResponse(results *Results, took time.Duration, index string) *ESResponse {
	hits := make([]ESHit, 0, len(results.hits))
	var maxScore float64

	for _, hit := range results.hits {
		if hit.Score > maxScore {
			maxScore = hit.Score
		}

		// Convert document fields to map
		source := make(map[string]interface{})
		for name, field := range hit.Source.GetFields() {
			source[name] = field.Value
		}

		hits = append(hits, ESHit{
			Index:  index,
			ID:     hit.ID,
			Score:  hit.Score,
			Source: source,
		})
	}

	return &ESResponse{
		Took:     int(took.Milliseconds()),
		TimedOut: false,
		Shards: ESShards{
			Total:      1,
			Successful: 1,
			Skipped:    0,
			Failed:     0,
		},
		Hits: ESHits{
			Total: ESTotal{
				Value:    len(hits),
				Relation: "eq",
			},
			MaxScore: maxScore,
			Hits:     hits,
		},
	}
}

================
File: search/query_executor_test.go
================
package search

import (
	"strings"
	"testing"

	"my-indexer/analysis"
	"my-indexer/document"
	"my-indexer/index"
	"my-indexer/query"
)

// mockAnalyzer implements a simple analyzer for testing
type mockAnalyzer struct{}

func (a *mockAnalyzer) Analyze(text string) []analysis.Token {
	// Split text into tokens by whitespace
	words := strings.Fields(strings.ToLower(text))
	tokens := make([]analysis.Token, len(words))
	for i, word := range words {
		tokens[i] = analysis.Token{
			Text:      word,
			Position:  i,
			StartByte: 0, // Simplified for testing
			EndByte:   0, // Simplified for testing
		}
	}
	return tokens
}

// MockDocumentStore implements DocumentStore for testing
type MockDocumentStore struct {
	docs map[int]*document.Document
}

func newMockDocumentStore() *MockDocumentStore {
	return &MockDocumentStore{
		docs: make(map[int]*document.Document),
	}
}

func (s *MockDocumentStore) LoadDocument(docID int) (*document.Document, error) {
	return s.docs[docID], nil
}

func (s *MockDocumentStore) LoadAllDocuments() ([]*document.Document, error) {
	docs := make([]*document.Document, 0, len(s.docs))
	for _, doc := range s.docs {
		docs = append(docs, doc)
	}
	return docs, nil
}

func TestQueryExecutor(t *testing.T) {
	// Setup test environment
	analyzer := &mockAnalyzer{}
	idx := index.NewIndex(analyzer)
	store := newMockDocumentStore()
	search := NewSearch(idx, store)
	executor := NewQueryExecutor(search)

	// Add test documents
	doc1 := document.NewDocument()
	doc1.AddField("title", "The quick brown fox")
	doc1.AddField("content", "jumps over the lazy dog")
	doc1.AddField("age", 5)
	store.docs[0] = doc1

	doc2 := document.NewDocument()
	doc2.AddField("title", "The lazy brown dog")
	doc2.AddField("content", "sleeps in the sun")
	doc2.AddField("age", 3)
	store.docs[1] = doc2

	doc3 := document.NewDocument()
	doc3.AddField("title", "A brown fox")
	doc3.AddField("content", "jump over fences")
	doc3.AddField("age", 7)
	store.docs[2] = doc3

	// Add documents to index
	idx.AddDocument(doc1)
	idx.AddDocument(doc2)
	idx.AddDocument(doc3)

	// Test cases
	t.Run("Term Query", func(t *testing.T) {
		q := query.NewTermQuery("title", "quick")
		results, err := executor.Execute(q)
		if err != nil {
			t.Errorf("Failed to execute term query: %v", err)
		}
		if len(results.hits) != 1 {
			t.Errorf("Expected 1 result, got %d", len(results.hits))
		}
		if len(results.hits) > 0 && results.hits[0].DocID != 0 {
			t.Errorf("Expected document 0 to match, got document %d", results.hits[0].DocID)
		}
	})

	t.Run("Range Query", func(t *testing.T) {
		q := query.NewRangeQuery("age")
		q.GreaterThan(2.0)
		q.LessThan(4.0)
		results, err := executor.Execute(q)
		if err != nil {
			t.Errorf("Failed to execute range query: %v", err)
		}
		if len(results.hits) != 1 {
			t.Errorf("Expected 1 result, got %d", len(results.hits))
		}
	})

	t.Run("Boolean Query", func(t *testing.T) {
		bq := query.NewBooleanQuery()
		bq.AddMust(query.NewTermQuery("title", "quick"))
		bq.AddMust(query.NewTermQuery("content", "jumps"))
		
		results, err := executor.Execute(bq)
		if err != nil {
			t.Errorf("Failed to execute boolean query: %v", err)
		}
		if len(results.hits) != 1 {
			t.Errorf("Expected 1 result, got %d", len(results.hits))
		}
	})
}

================
File: search/query_executor.go
================
package search

import (
	"fmt"
	"math"
	"my-indexer/query"
	"sort"
)

// QueryExecutor executes internal queries and returns search results
type QueryExecutor struct {
	search *Search
}

// NewQueryExecutor creates a new query executor
func NewQueryExecutor(search *Search) *QueryExecutor {
	return &QueryExecutor{
		search: search,
	}
}

// Execute executes an internal query and returns search results
func (e *QueryExecutor) Execute(q query.Query) (*Results, error) {
	e.search.mu.RLock()
	defer e.search.mu.RUnlock()

	// Handle different query types
	switch q.Type() {
	case query.TermQuery:
		return e.executeTermQuery(q)
	case query.PhraseQuery:
		return e.executePhraseQuery(q)
	case query.RangeQuery:
		return e.executeRangeQuery(q)
	case query.BooleanQuery:
		return e.executeBooleanQuery(q)
	case query.MatchQuery:
		return e.executeMatchQuery(q)
	default:
		return nil, fmt.Errorf("unsupported query type: %v", q.Type())
	}
}

// executeTermQuery executes a term query
func (e *QueryExecutor) executeTermQuery(q query.Query) (*Results, error) {
	tq, ok := q.(*query.TermQueryImpl)
	if !ok {
		return nil, fmt.Errorf("invalid term query type")
	}

	// Get the analyzer from the search instance
	tokens := e.search.idx.Analyzer().Analyze(tq.Term())
	if len(tokens) == 0 {
		return &Results{hits: make([]*Result, 0)}, nil
	}
	
	// Use the first token's text as our search term
	term := tokens[0].Text
	
	// Get posting list for the term
	postings := e.search.idx.GetPostings(term)
	
	// Create results
	results := &Results{
		hits: make([]*Result, 0, len(postings)),
	}

	// Process each document
	for docID, posting := range postings {
		// Check if the term appears in the specified field
		fieldFound := false
		for _, field := range posting.Fields {
			if field == tq.Field() {
				fieldFound = true
				break
			}
		}
		if !fieldFound {
			continue
		}

		// Load document
		doc, err := e.search.store.LoadDocument(docID)
		if err != nil {
			return nil, fmt.Errorf("failed to load document %d: %w", docID, err)
		}

		// Calculate score using TF-IDF
		score := e.calculateScore(docID, []string{term})

		results.hits = append(results.hits, &Result{
			ID:     fmt.Sprintf("%d", docID),
			Score:  score,
			Source: doc,
		})
	}

	// Sort results by score
	sort.Sort(results)

	return results, nil
}

// executePhraseQuery executes a phrase query
func (e *QueryExecutor) executePhraseQuery(q query.Query) (*Results, error) {
	// For now, treat phrase queries as term queries
	// TODO: Implement proper phrase query execution using term positions
	return e.executeTermQuery(q)
}

// executeRangeQuery executes a range query
func (e *QueryExecutor) executeRangeQuery(q query.Query) (*Results, error) {
	// Get all documents and filter by range
	results := &Results{
		hits: make([]*Result, 0),
	}

	// Scan all documents (inefficient, but works for now)
	// TODO: Implement field indexing for efficient range queries
	for docID := 0; docID < e.search.idx.GetDocumentCount(); docID++ {
		doc, err := e.search.store.LoadDocument(docID)
		if err != nil {
			continue
		}

		// Check if document matches range criteria
		field, err := doc.GetField(q.Field())
		if err != nil {
			continue
		}

		// Convert field value to float64 for comparison
		var fieldValue float64
		switch v := field.Value.(type) {
		case int:
			fieldValue = float64(v)
		case int32:
			fieldValue = float64(v)
		case int64:
			if v > 9223372036854775807 || v < -9223372036854775808 {
				return nil, fmt.Errorf("int64 value %d exceeds float64 range", v)
			}
			fieldValue = float64(v)
		case float32:
			fieldValue = float64(v)
		case float64:
			fieldValue = v
		case uint:
			fieldValue = float64(v)
		case uint32:
			fieldValue = float64(v)
		case uint64:
			if v > 18446744073709551615 {
				return nil, fmt.Errorf("uint64 value %d exceeds float64 range", v)
			}
			fieldValue = float64(v)
		default:
			// Skip non-numeric fields
			continue
		}

		rq := q.(*query.RangeQueryImpl)
		if rq.Gt() != nil {
			if gt, ok := rq.Gt().(float64); ok {
				if fieldValue <= gt {
					continue
				}
			} else {
				return nil, fmt.Errorf("gt value is not a float64")
			}
		}
		if rq.Lt() != nil {
			if lt, ok := rq.Lt().(float64); ok {
				if fieldValue >= lt {
					continue
				}
			} else {
				return nil, fmt.Errorf("lt value is not a float64")
			}
		}

		results.hits = append(results.hits, &Result{
			ID:     fmt.Sprintf("%d", docID),
			Score:  1.0, // Default score for range queries
			Source: doc,
		})
	}

	return results, nil
}

// executeBooleanQuery executes a boolean query
func (e *QueryExecutor) executeBooleanQuery(q query.Query) (*Results, error) {
	bq, ok := q.(*query.BooleanQueryImpl)
	if !ok {
		return nil, fmt.Errorf("invalid boolean query type")
	}

	// Execute must queries
	var mustResults *Results
	if len(bq.Must()) > 0 {
		var err error
		mustResults, err = e.executeMustClauses(bq.Must())
		if err != nil {
			return nil, err
		}
	}

	// Execute should queries
	var shouldResults *Results
	if len(bq.Should()) > 0 {
		var err error
		shouldResults, err = e.executeShouldClauses(bq.Should())
		if err != nil {
			return nil, err
		}
	}

	// If both must and should clauses are empty, return empty results
	if mustResults == nil && shouldResults == nil {
		return &Results{hits: make([]*Result, 0)}, nil
	}

	// Combine results
	return e.combineResults(mustResults, shouldResults), nil
}

// executeMatchQuery executes a match query
func (e *QueryExecutor) executeMatchQuery(q query.Query) (*Results, error) {
	mq, ok := q.(*query.MatchQueryImpl)
	if !ok {
		return nil, fmt.Errorf("invalid match query type")
	}

	// Get the analyzer from the search instance
	tokens := e.search.idx.Analyzer().Analyze(mq.Text())
	if len(tokens) == 0 {
		return &Results{hits: make([]*Result, 0)}, nil
	}

	// Process each token
	results := &Results{
		hits: make([]*Result, 0),
	}

	// Track seen documents to avoid duplicates
	seenDocs := make(map[int]bool)

	for _, token := range tokens {
		// Get posting list for the term
		postings := e.search.idx.GetPostings(token.Text)

		// Process each document
		for docID, posting := range postings {
			// Skip if we've already processed this document
			if seenDocs[docID] {
				continue
			}

			// Check if the term appears in the specified field
			fieldFound := false
			for _, field := range posting.Fields {
				if field == mq.Field() {
					fieldFound = true
					break
				}
			}
			if !fieldFound {
				continue
			}

			// Load document
			doc, err := e.search.store.LoadDocument(docID)
			if err != nil {
				return nil, fmt.Errorf("failed to load document %d: %w", docID, err)
			}

			// Calculate score using TF-IDF
			score := e.calculateScore(docID, []string{token.Text})

			results.hits = append(results.hits, &Result{
				ID:     fmt.Sprintf("%d", docID),
				Score:  score,
				Source: doc,
			})

			// Mark document as seen
			seenDocs[docID] = true
		}
	}

	// Sort results by score
	sort.Sort(results)

	return results, nil
}

// executeMustClauses executes must clauses of a boolean query
func (e *QueryExecutor) executeMustClauses(queries []query.Query) (*Results, error) {
	if len(queries) == 0 {
		return nil, nil
	}

	// Execute first query
	results, err := e.Execute(queries[0])
	if err != nil {
		return nil, err
	}

	// Filter results through remaining queries
	for _, q := range queries[1:] {
		nextResults, err := e.Execute(q)
		if err != nil {
			return nil, err
		}

		// Keep only documents that appear in both result sets
		filteredHits := make([]*Result, 0)
		docMap := make(map[string]*Result)
		for _, hit := range nextResults.hits {
			docMap[hit.ID] = hit
		}

		for _, hit := range results.hits {
			if _, exists := docMap[hit.ID]; exists {
				filteredHits = append(filteredHits, hit)
			}
		}

		results.hits = filteredHits
	}

	return results, nil
}

// executeShouldClauses executes should clauses of a boolean query
func (e *QueryExecutor) executeShouldClauses(queries []query.Query) (*Results, error) {
	if len(queries) == 0 {
		return nil, nil
	}

	// Create a map to track unique documents and their highest scores
	docMap := make(map[string]*Result)

	// Execute each query and merge results
	for _, q := range queries {
		results, err := e.Execute(q)
		if err != nil {
			return nil, err
		}

		for _, hit := range results.hits {
			if existing, exists := docMap[hit.ID]; exists {
				// Keep the higher score
				if hit.Score > existing.Score {
					docMap[hit.ID] = hit
				}
			} else {
				docMap[hit.ID] = hit
			}
		}
	}

	// Convert map to results
	results := &Results{
		hits: make([]*Result, 0, len(docMap)),
	}
	for _, hit := range docMap {
		results.hits = append(results.hits, hit)
	}

	// Sort by score
	sort.Sort(results)

	return results, nil
}

// combineResults combines must and should results
func (e *QueryExecutor) combineResults(must, should *Results) *Results {
	if must == nil && should == nil {
		return &Results{hits: make([]*Result, 0)}
	}

	if must == nil {
		return should
	}

	if should == nil {
		return must
	}

	// Combine scores from must and should clauses
	docMap := make(map[string]*Result)
	
	// Add all must results
	for _, hit := range must.hits {
		docMap[hit.ID] = hit
	}

	// Add scores from should results
	for _, hit := range should.hits {
		if existing, exists := docMap[hit.ID]; exists {
			// Combine scores
			existing.Score += hit.Score
		}
	}

	// Convert map back to results
	results := &Results{
		hits: make([]*Result, 0, len(docMap)),
	}
	for _, hit := range docMap {
		results.hits = append(results.hits, hit)
	}

	// Sort by combined score
	sort.Sort(results)

	return results
}

// calculateScore calculates TF-IDF score for a document
func (e *QueryExecutor) calculateScore(docID int, terms []string) float64 {
	var score float64

	// Calculate TF-IDF score for each term
	for _, term := range terms {
		postings := e.search.idx.GetPostings(term)
		if entry, exists := postings[docID]; exists {
			tf := float64(entry.TermFreq)  // Using TermFreq field from PostingEntry
			df := float64(len(postings))
			if df > 0 {
				// TF-IDF scoring: tf * idf
				// idf = log(1 + N/df) where N is total number of documents
				// Adding 1 ensures IDF is always positive
				N := float64(e.search.idx.GetDocumentCount())
				idf := math.Log1p(N / df)
				score += tf * idf
			}
		}
	}

	return score
}

================
File: search/search_test.go
================
package search

import (
	"sort"
	"testing"

	"my-indexer/analysis"
	"my-indexer/document"
	"my-indexer/index"
	"fmt"
)

// mockDocumentStore implements DocumentStore for testing
type mockDocumentStore struct {
	docs map[int]*document.Document
}

func (m *mockDocumentStore) LoadDocument(docID int) (*document.Document, error) {
	if doc, ok := m.docs[docID]; ok {
		return doc, nil
	}
	return nil, fmt.Errorf("document not found: %d", docID)
}

func (m *mockDocumentStore) LoadAllDocuments() ([]*document.Document, error) {
	docs := make([]*document.Document, 0, len(m.docs))
	for _, doc := range m.docs {
		docs = append(docs, doc)
	}
	return docs, nil
}

func newMockStore() *mockDocumentStore {
	return &mockDocumentStore{
		docs: make(map[int]*document.Document),
	}
}

func TestSearch(t *testing.T) {
	// Create analyzer
	analyzer := analysis.NewStandardAnalyzer()

	// Create index
	idx := index.NewIndex(analyzer)

	// Create document store
	store := newMockStore()

	// Create search
	search := NewSearch(idx, store)

	// Create test documents
	docs := []*document.Document{
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("title", "The quick brown fox")
			doc.AddField("content", "The quick brown fox jumps over the lazy dog")
			return doc
		}(),
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("title", "Lazy dog sleeping")
			doc.AddField("content", "The lazy dog is sleeping in the sun")
			return doc
		}(),
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("title", "Quick rabbit running")
			doc.AddField("content", "A quick rabbit is running in the field")
			return doc
		}(),
	}

	// Add documents to index and store
	for i, doc := range docs {
		docID, err := idx.AddDocument(doc)
		if err != nil {
			t.Fatalf("Failed to add document %d: %v", i, err)
		}
		store.docs[docID] = doc
	}

	tests := []struct {
		name          string
		terms         []string
		op            Operator
		expectedDocs  int
		expectedTerms []string
	}{
		{
			name:          "Single term search",
			terms:         []string{"quick"},
			op:            OR,
			expectedDocs:  2,
			expectedTerms: []string{"quick"},
		},
		{
			name:          "AND search",
			terms:         []string{"quick", "fox"},
			op:            AND,
			expectedDocs:  1,
			expectedTerms: []string{"quick", "fox"},
		},
		{
			name:          "OR search",
			terms:         []string{"quick", "lazy"},
			op:            OR,
			expectedDocs:  3,
			expectedTerms: []string{"quick", "lazy"},
		},
		{
			name:          "Empty search",
			terms:         []string{},
			op:            OR,
			expectedDocs:  0,
			expectedTerms: []string{},
		},
		{
			name:          "No results",
			terms:         []string{"nonexistent"},
			op:            OR,
			expectedDocs:  0,
			expectedTerms: []string{"nonexistent"},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			results, err := search.Search(tt.terms, tt.op)
			if err != nil {
				t.Fatalf("Search failed: %v", err)
			}

			if len(results.GetHits()) != tt.expectedDocs {
				t.Errorf("Expected %d docs, got %d", tt.expectedDocs, len(results.GetHits()))
			}

			// Verify results are sorted by score
			hits := results.GetHits()
			if len(hits) > 1 {
				if !sort.SliceIsSorted(hits, func(i, j int) bool {
					return hits[i].Score > hits[j].Score
				}) {
					t.Error("Results are not sorted by score")
				}
			}

			// Verify operator behavior
			if tt.op == AND && len(hits) > 0 {
				// For AND, all terms should be present in each result
				for _, hit := range hits {
					doc := hit.Doc
					for _, term := range tt.terms {
						found := false
						for _, field := range doc.GetFields() {
							tokens := analyzer.Analyze(field.Value.(string))
							for _, token := range tokens {
								if token.Text == term {
									found = true
									break
								}
							}
							if found {
								break
							}
						}
						if !found {
							t.Errorf("Term %q not found in document for AND search", term)
						}
					}
				}
			}
		})
	}
}

func TestSearchScoring(t *testing.T) {
	// Create analyzer
	analyzer := analysis.NewStandardAnalyzer()

	// Create index
	idx := index.NewIndex(analyzer)

	// Create document store
	store := newMockStore()

	// Create search
	search := NewSearch(idx, store)

	// Create test documents with varying term frequencies
	docs := []*document.Document{
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("title", "test document")
			doc.AddField("content", "test test test") // 4 occurrences of "test"
			return doc
		}(),
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("title", "test")
			doc.AddField("content", "test document") // 2 occurrences of "test"
			return doc
		}(),
		func() *document.Document {
			doc := document.NewDocument()
			doc.AddField("content", "test") // 1 occurrence of "test"
			return doc
		}(),
	}

	// Add documents to index and store
	for i, doc := range docs {
		docID, err := idx.AddDocument(doc)
		if err != nil {
			t.Fatalf("Failed to add document %d: %v", i, err)
		}
		store.docs[docID] = doc
	}

	// Search for "test"
	results, err := search.Search([]string{"test"}, OR)
	if err != nil {
		t.Fatalf("Search failed: %v", err)
	}

	hits := results.GetHits()
	if len(hits) != 3 {
		t.Fatalf("Expected 3 results, got %d", len(hits))
	}

	// Verify scoring order (document with more "test" occurrences should score higher)
	frequencies := make([]float64, len(hits))
	for i, hit := range hits {
		frequencies[i] = hit.Score
	}

	// Check that scores are in descending order
	for i := 1; i < len(frequencies); i++ {
		if frequencies[i-1] <= frequencies[i] {
			t.Errorf("Results not properly scored by term frequency: %v", frequencies)
			break
		}
	}
}

func TestConcurrentSearch(t *testing.T) {
	// Create analyzer
	analyzer := analysis.NewStandardAnalyzer()

	// Create index
	idx := index.NewIndex(analyzer)

	// Create document store
	store := newMockStore()

	// Create search
	search := NewSearch(idx, store)

	// Add test documents
	doc := document.NewDocument()
	doc.AddField("content", "test document")
	docID, err := idx.AddDocument(doc)
	if err != nil {
		t.Fatalf("Failed to add document: %v", err)
	}
	store.docs[docID] = doc

	// Run concurrent searches
	done := make(chan bool)
	for i := 0; i < 10; i++ {
		go func() {
			_, err := search.Search([]string{"test"}, OR)
			if err != nil {
				t.Errorf("Concurrent search failed: %v", err)
			}
			done <- true
		}()
	}

	// Wait for all searches to complete
	for i := 0; i < 10; i++ {
		<-done
	}
}

================
File: search/search.go
================
package search

import (
	"fmt"
	"math"
	"sort"
	"sync"

	"my-indexer/document"
	"my-indexer/index"
	"my-indexer/query"
)

// Operator represents a boolean operator
type Operator int

const (
	// AND requires all terms to be present
	AND Operator = iota
	// OR requires at least one term to be present
	OR
)

// Result represents a search result with its score
type Result struct {
	Index  string             `json:"_index"`
	Type   string             `json:"_type"`
	ID     string             `json:"_id"`
	DocID  int               `json:"doc_id"`
	Score  float64            `json:"_score"`
	Source *document.Document `json:"_source"`
	Doc    *document.Document `json:"doc"` // Alias for Source for backward compatibility
}

// Results represents a sorted list of search results
type Results struct {
	hits   []*Result
	maxDoc int
}

// Len returns the number of results
func (r *Results) Len() int { return len(r.hits) }

// Less compares results by score
func (r *Results) Less(i, j int) bool {
	// Sort by score in descending order
	return r.hits[i].Score > r.hits[j].Score
}

// Swap swaps two results
func (r *Results) Swap(i, j int) { r.hits[i], r.hits[j] = r.hits[j], r.hits[i] }

// GetHits returns the sorted results
func (r *Results) GetHits() []*Result {
	return r.hits
}

// Search performs a search operation on the index
type Search struct {
	idx    *index.Index
	mu     sync.RWMutex
	store  DocumentStore
	maxDoc int
}

// DocumentStore is an interface for loading documents
type DocumentStore interface {
	LoadDocument(docID int) (*document.Document, error)
	LoadAllDocuments() ([]*document.Document, error)
}

// NewSearch creates a new search instance
func NewSearch(idx *index.Index, store DocumentStore) *Search {
	return &Search{
		idx:   idx,
		store: store,
	}
}

// calculateScore calculates the score for a document based on term frequencies
func (s *Search) calculateScore(docID int, terms []string) float64 {
	var score float64

	// Calculate TF-IDF score for each term
	for _, term := range terms {
		tf, err := s.idx.GetTermFrequency(term, docID)
		if err != nil {
			continue
		}
		df, err := s.idx.GetDocumentFrequency(term)
		if err != nil {
			continue
		}
		if df > 0 {
			// TF-IDF scoring: tf * idf
			// idf = log(1 + N/df) where N is total number of documents
			// Adding 1 ensures IDF is always positive
			N := float64(s.idx.GetDocumentCount())
			idf := math.Log1p(N / float64(df))
			score += float64(tf) * idf
		}
	}

	return score
}

// Search performs a search with the given terms and operator
func (s *Search) Search(terms []string, op Operator) (*Results, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	if len(terms) == 0 {
		return &Results{}, nil
	}

	// Get document IDs based on operator
	docIDs := make(map[int]bool)
	
	// Handle first term
	postings := s.idx.GetPostings(terms[0])
	for docID := range postings {
		docIDs[docID] = true
	}

	// Process remaining terms based on operator
	for _, term := range terms[1:] {
		postings := s.idx.GetPostings(term)
		
		switch op {
		case AND:
			// Remove documents that don't contain the term
			for docID := range docIDs {
				if _, exists := postings[docID]; !exists {
					delete(docIDs, docID)
				}
			}
		case OR:
			// Add documents that contain the term
			for docID := range postings {
				docIDs[docID] = true
			}
		}
	}

	// Calculate scores and create results
	results := &Results{
		hits: make([]*Result, 0, len(docIDs)),
	}

	for docID := range docIDs {
		score := s.calculateScore(docID, terms)
		doc, err := s.store.LoadDocument(docID)
		if err != nil {
			return nil, fmt.Errorf("failed to load document %d: %w", docID, err)
		}

		results.hits = append(results.hits, &Result{
			Index:  "",
			Type:   "",
			ID:     fmt.Sprintf("%d", docID),
			DocID:  docID,
			Score:  score,
			Source: doc,
			Doc:    doc,
		})
	}

	// Sort results by score
	sort.Sort(results)

	return results, nil
}

// SearchWithQuery performs a search using a Query object
func (s *Search) SearchWithQuery(query query.Query) (*Results, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	// Get matching document IDs based on query type
	docIDs := make(map[int]bool)
	
	qType := query.Type()
	switch qType {
	case 0: // TermQuery
		// For term queries, use the inverted index directly
		// Get the term value from the query match against an empty string
		if query.Match("") {
			// If it matches empty string, skip
			return &Results{}, nil
		}
		// Find first non-empty string that doesn't match
		for i := 'a'; i <= 'z'; i++ {
			testStr := string(i)
			if !query.Match(testStr) {
				postings := s.idx.GetPostings(testStr)
				for docID, posting := range postings {
					if query.Field() == "" || posting.FieldName == query.Field() {
						docIDs[docID] = true
					}
				}
				break
			}
		}
	case 6: // MatchQuery
		// For match queries, analyze the text and search for each term
		analyzer := s.idx.Analyzer()
		// Get sample text that would match
		sampleText := "test"
		if query.Match(sampleText) {
			tokens := analyzer.Analyze(sampleText)
			for _, token := range tokens {
				postings := s.idx.GetPostings(token.Text)
				for docID, posting := range postings {
					if query.Field() == "" || posting.FieldName == query.Field() {
						docIDs[docID] = true
					}
				}
			}
		}
	case 8: // MatchAllQuery
		// For match_all queries, get all documents
		docs, err := s.store.LoadAllDocuments()
		if err != nil {
			return nil, fmt.Errorf("failed to get all documents: %w", err)
		}
		for _, doc := range docs {
			docIDs[doc.ID] = true
		}
	default:
		// For other query types, fall back to loading and filtering documents
		docs, err := s.store.LoadAllDocuments()
		if err != nil {
			return nil, fmt.Errorf("failed to get documents: %w", err)
		}
		for _, doc := range docs {
			for field, value := range doc.GetFields() {
				if query.Field() == "" || query.Field() == field {
					if query.Match(value) {
						docIDs[doc.ID] = true
						break
					}
				}
			}
		}
	}

	// Create results from matching documents
	results := &Results{
		hits: make([]*Result, 0, len(docIDs)),
	}

	// Get the search terms for scoring
	var terms []string
	switch qType {
	case 0: // TermQuery
		// Use same technique as above to extract term
		for i := 'a'; i <= 'z'; i++ {
			testStr := string(i)
			if !query.Match(testStr) {
				terms = []string{testStr}
				break
			}
		}
	case 6: // MatchQuery
		analyzer := s.idx.Analyzer()
		sampleText := "test"
		if query.Match(sampleText) {
			tokens := analyzer.Analyze(sampleText)
			terms = make([]string, len(tokens))
			for i, token := range tokens {
				terms[i] = token.Text
			}
		}
	default:
		terms = []string{}
	}

	for docID := range docIDs {
		doc, err := s.store.LoadDocument(docID)
		if err != nil {
			return nil, fmt.Errorf("failed to load document %d: %w", docID, err)
		}

		score := 1.0
		if len(terms) > 0 {
			score = s.calculateScore(docID, terms)
		}

		result := &Result{
			Index:  "",
			Type:   "",
			ID:     fmt.Sprintf("%d", docID),
			DocID:  docID,
			Score:  score,
			Source: doc,
			Doc:    doc,
		}
		results.hits = append(results.hits, result)
	}

	// Sort results by score
	sort.Sort(results)

	return results, nil
}

================
File: storage/storage_test.go
================
package storage

import (
	"os"
	"path/filepath"
	"testing"

	"my-indexer/document"
	"my-indexer/index"
)

func TestIndexStorage(t *testing.T) {
	// Create temporary directory for testing
	tempDir, err := os.MkdirTemp("", "indexer-test-*")
	if err != nil {
		t.Fatalf("Failed to create temp directory: %v", err)
	}
	defer os.RemoveAll(tempDir)

	// Create storage
	storage, err := NewIndexStorage(tempDir)
	if err != nil {
		t.Fatalf("Failed to create storage: %v", err)
	}

	// Create and populate test index
	idx := index.NewIndex(nil)

	// Add test documents
	doc1 := document.NewDocument()
	err = doc1.AddField("title", "Test Document 1")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}
	err = doc1.AddField("content", "This is a test document")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}

	doc2 := document.NewDocument()
	err = doc2.AddField("title", "Test Document 2")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}
	err = doc2.AddField("content", "Another test document")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}

	// Add documents to index
	docID1, err := idx.AddDocument(doc1)
	if err != nil {
		t.Fatalf("Failed to add document 1: %v", err)
	}

	docID2, err := idx.AddDocument(doc2)
	if err != nil {
		t.Fatalf("Failed to add document 2: %v", err)
	}

	// Test saving documents
	err = storage.SaveDocument(docID1, doc1)
	if err != nil {
		t.Errorf("Failed to save document 1: %v", err)
	}

	err = storage.SaveDocument(docID2, doc2)
	if err != nil {
		t.Errorf("Failed to save document 2: %v", err)
	}

	// Test saving index
	err = storage.SaveIndex(idx)
	if err != nil {
		t.Errorf("Failed to save index: %v", err)
	}

	// Test loading index
	loadedIdx, err := storage.LoadIndex()
	if err != nil {
		t.Errorf("Failed to load index: %v", err)
	}

	// Verify loaded index
	if loadedIdx.GetDocumentCount() != idx.GetDocumentCount() {
		t.Errorf("Loaded index document count = %d, want %d",
			loadedIdx.GetDocumentCount(), idx.GetDocumentCount())
	}

	// Test loading documents
	loadedDoc1, err := storage.LoadDocument(docID1)
	if err != nil {
		t.Errorf("Failed to load document 1: %v", err)
	}

	loadedDoc2, err := storage.LoadDocument(docID2)
	if err != nil {
		t.Errorf("Failed to load document 2: %v", err)
	}

	// Verify loaded documents
	doc1Fields := doc1.GetFields()
	loadedDoc1Fields := loadedDoc1.GetFields()
	if len(doc1Fields) != len(loadedDoc1Fields) {
		t.Errorf("Loaded document 1 field count = %d, want %d",
			len(loadedDoc1Fields), len(doc1Fields))
	}

	doc2Fields := doc2.GetFields()
	loadedDoc2Fields := loadedDoc2.GetFields()
	if len(doc2Fields) != len(loadedDoc2Fields) {
		t.Errorf("Loaded document 2 field count = %d, want %d",
			len(loadedDoc2Fields), len(doc2Fields))
	}

	// Test document removal
	err = storage.RemoveDocument(docID1)
	if err != nil {
		t.Errorf("Failed to remove document 1: %v", err)
	}

	_, err = storage.LoadDocument(docID1)
	if err == nil {
		t.Error("Expected error loading removed document")
	}

	// Test clearing storage
	err = storage.Clear()
	if err != nil {
		t.Errorf("Failed to clear storage: %v", err)
	}

	// Verify storage is cleared
	if _, err := os.Stat(filepath.Join(tempDir, "index.gob")); !os.IsNotExist(err) {
		t.Error("Index file still exists after clear")
	}

	entries, err := os.ReadDir(filepath.Join(tempDir, "documents"))
	if err != nil {
		t.Errorf("Failed to read documents directory: %v", err)
	}
	if len(entries) > 0 {
		t.Error("Documents directory not empty after clear")
	}
}

func TestConcurrentAccess(t *testing.T) {
	// Create temporary directory for testing
	tempDir, err := os.MkdirTemp("", "indexer-test-*")
	if err != nil {
		t.Fatalf("Failed to create temp directory: %v", err)
	}
	defer os.RemoveAll(tempDir)

	// Create storage
	storage, err := NewIndexStorage(tempDir)
	if err != nil {
		t.Fatalf("Failed to create storage: %v", err)
	}

	// Create test index and document
	idx := index.NewIndex(nil)
	doc := document.NewDocument()
	err = doc.AddField("content", "test document")
	if err != nil {
		t.Fatalf("Failed to add field to document: %v", err)
	}

	docID, err := idx.AddDocument(doc)
	if err != nil {
		t.Fatalf("Failed to add document: %v", err)
	}

	// Save initial state
	err = storage.SaveIndex(idx)
	if err != nil {
		t.Fatalf("Failed to save index: %v", err)
	}
	err = storage.SaveDocument(docID, doc)
	if err != nil {
		t.Fatalf("Failed to save document: %v", err)
	}

	// Test concurrent access
	done := make(chan bool)
	for i := 0; i < 10; i++ {
		go func() {
			// Load index
			_, err := storage.LoadIndex()
			if err != nil {
				t.Errorf("Concurrent LoadIndex failed: %v", err)
			}

			// Load document
			_, err = storage.LoadDocument(docID)
			if err != nil {
				t.Errorf("Concurrent LoadDocument failed: %v", err)
			}

			done <- true
		}()
	}

	// Wait for all goroutines
	for i := 0; i < 10; i++ {
		<-done
	}
}

================
File: storage/storage.go
================
package storage

import (
	"encoding/gob"
	"fmt"
	"os"
	"path/filepath"
	"sync"

	"my-indexer/document"
	"my-indexer/index"
)

// IndexStorage handles persistence of the index
type IndexStorage struct {
	mu           sync.RWMutex
	indexPath    string
	documentsDir string
}

// IndexData represents the serializable form of the index
type IndexData struct {
	Terms    map[string]*index.PostingList
	DocCount int
	NextID   int
}

// DocumentData represents the serializable form of a document
type DocumentData struct {
	Fields map[string]document.Field
}

// NewIndexStorage creates a new index storage
func NewIndexStorage(baseDir string) (*IndexStorage, error) {
	// Create base directory if it doesn't exist
	if err := os.MkdirAll(baseDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create base directory: %w", err)
	}

	// Create documents directory
	documentsDir := filepath.Join(baseDir, "documents")
	if err := os.MkdirAll(documentsDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create documents directory: %w", err)
	}

	return &IndexStorage{
		indexPath:    filepath.Join(baseDir, "index.gob"),
		documentsDir: documentsDir,
	}, nil
}

// SaveIndex persists the index to disk
func (s *IndexStorage) SaveIndex(idx *index.Index) error {
	s.mu.Lock()
	defer s.mu.Unlock()

	// Create a temporary file for atomic write
	tempPath := s.indexPath + ".tmp"
	file, err := os.Create(tempPath)
	if err != nil {
		return fmt.Errorf("failed to create temporary index file: %w", err)
	}
	defer file.Close()

	// Prepare index data for serialization
	data := &IndexData{
		Terms:    idx.GetTerms(),
		DocCount: idx.GetDocumentCount(),
		NextID:   idx.GetNextDocID(),
	}

	// Serialize index data
	encoder := gob.NewEncoder(file)
	if err := encoder.Encode(data); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to encode index: %w", err)
	}

	// Ensure all data is written to disk
	if err := file.Sync(); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to sync index file: %w", err)
	}

	// Atomic rename
	if err := os.Rename(tempPath, s.indexPath); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to save index file: %w", err)
	}

	return nil
}

// LoadIndex loads the index from disk
func (s *IndexStorage) LoadIndex() (*index.Index, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	file, err := os.Open(s.indexPath)
	if err != nil {
		if os.IsNotExist(err) {
			return index.NewIndex(nil), nil
		}
		return nil, fmt.Errorf("failed to open index file: %w", err)
	}
	defer file.Close()

	var data IndexData
	decoder := gob.NewDecoder(file)
	if err := decoder.Decode(&data); err != nil {
		return nil, fmt.Errorf("failed to decode index: %w", err)
	}

	// Create new index and restore its state
	idx := index.NewIndex(nil)
	if err := idx.RestoreFromData(data.Terms, data.DocCount, data.NextID); err != nil {
		return nil, fmt.Errorf("failed to restore index: %w", err)
	}

	return idx, nil
}

// SaveDocument persists a document to disk
func (s *IndexStorage) SaveDocument(docID int, doc *document.Document) error {
	s.mu.Lock()
	defer s.mu.Unlock()

	docPath := filepath.Join(s.documentsDir, fmt.Sprintf("doc_%d.gob", docID))
	
	// Create a temporary file for atomic write
	tempPath := docPath + ".tmp"
	file, err := os.Create(tempPath)
	if err != nil {
		return fmt.Errorf("failed to create temporary document file: %w", err)
	}
	defer file.Close()

	// Prepare document data for serialization
	data := &DocumentData{
		Fields: doc.GetFields(),
	}

	// Serialize document data
	encoder := gob.NewEncoder(file)
	if err := encoder.Encode(data); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to encode document: %w", err)
	}

	// Ensure all data is written to disk
	if err := file.Sync(); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to sync document file: %w", err)
	}

	// Atomic rename
	if err := os.Rename(tempPath, docPath); err != nil {
		os.Remove(tempPath)
		return fmt.Errorf("failed to save document file: %w", err)
	}

	return nil
}

// LoadDocument loads a document from disk
func (s *IndexStorage) LoadDocument(docID int) (*document.Document, error) {
	s.mu.RLock()
	defer s.mu.RUnlock()

	docPath := filepath.Join(s.documentsDir, fmt.Sprintf("doc_%d.gob", docID))
	file, err := os.Open(docPath)
	if err != nil {
		return nil, fmt.Errorf("failed to open document file: %w", err)
	}
	defer file.Close()

	var data DocumentData
	decoder := gob.NewDecoder(file)
	if err := decoder.Decode(&data); err != nil {
		return nil, fmt.Errorf("failed to decode document: %w", err)
	}

	// Create new document and restore its fields
	doc := document.NewDocument()
	for name, field := range data.Fields {
		if err := doc.AddField(name, field.Value); err != nil {
			return nil, fmt.Errorf("failed to restore document field: %w", err)
		}
	}

	return doc, nil
}

// RemoveDocument removes a document from disk
func (s *IndexStorage) RemoveDocument(docID int) error {
	s.mu.Lock()
	defer s.mu.Unlock()

	docPath := filepath.Join(s.documentsDir, fmt.Sprintf("doc_%d.gob", docID))
	if err := os.Remove(docPath); err != nil && !os.IsNotExist(err) {
		return fmt.Errorf("failed to remove document file: %w", err)
	}

	return nil
}

// Clear removes all index and document files
func (s *IndexStorage) Clear() error {
	s.mu.Lock()
	defer s.mu.Unlock()

	// Remove index file
	if err := os.Remove(s.indexPath); err != nil && !os.IsNotExist(err) {
		return fmt.Errorf("failed to remove index file: %w", err)
	}

	// Remove all document files
	entries, err := os.ReadDir(s.documentsDir)
	if err != nil {
		return fmt.Errorf("failed to read documents directory: %w", err)
	}

	for _, entry := range entries {
		if err := os.Remove(filepath.Join(s.documentsDir, entry.Name())); err != nil {
			return fmt.Errorf("failed to remove document file: %w", err)
		}
	}

	return nil
}

================
File: txlog/txlog_test.go
================
package txlog

import (
	"os"
	"testing"

	"my-indexer/document"
)

func TestTransactionLogging(t *testing.T) {
	// Create temporary directory for test logs
	tmpDir, err := os.MkdirTemp("", "txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)

	// Create transaction log
	txLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create transaction log: %v", err)
	}
	defer txLog.Close()

	// Test logging an add operation
	doc := document.NewDocument()
	doc.AddField("title", "test document")
	err = txLog.LogOperation(OpAdd, 1, doc)
	if err != nil {
		t.Errorf("Failed to log add operation: %v", err)
	}

	// Verify uncommitted operations
	uncommitted := txLog.GetUncommittedOperations()
	if len(uncommitted) != 1 {
		t.Errorf("Expected 1 uncommitted operation, got %d", len(uncommitted))
	}

	// Test commit
	err = txLog.Commit(1)
	if err != nil {
		t.Errorf("Failed to commit operation: %v", err)
	}

	uncommitted = txLog.GetUncommittedOperations()
	if len(uncommitted) != 0 {
		t.Errorf("Expected 0 uncommitted operations after commit, got %d", len(uncommitted))
	}
}

func TestRecovery(t *testing.T) {
	tmpDir, err := os.MkdirTemp("", "txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)

	// Create and populate transaction log
	txLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create transaction log: %v", err)
	}

	// Add some operations
	doc1 := document.NewDocument()
	doc1.AddField("title", "doc1")
	doc2 := document.NewDocument()
	doc2.AddField("title", "doc2")

	txLog.LogOperation(OpAdd, 1, doc1)
	txLog.LogOperation(OpAdd, 2, doc2)
	txLog.Commit(1)

	// Close the log
	txLog.Close()

	// Create new transaction log instance and recover
	recoveredLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create new transaction log: %v", err)
	}
	defer recoveredLog.Close()

	entries, err := recoveredLog.Recover()
	if err != nil {
		t.Fatalf("Failed to recover log: %v", err)
	}

	// Verify recovered entries
	if len(entries) != 3 { // 2 adds + 1 commit
		t.Errorf("Expected 3 recovered entries, got %d", len(entries))
	}

	var committed, uncommitted int
	for _, entry := range entries {
		if entry.Committed {
			committed++
		} else {
			uncommitted++
		}
	}

	if committed != 1 {
		t.Errorf("Expected 1 committed entry, got %d", committed)
	}
	if uncommitted != 2 {
		t.Errorf("Expected 2 uncommitted entries, got %d", uncommitted)
	}
}

func TestRollback(t *testing.T) {
	tmpDir, err := os.MkdirTemp("", "txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)

	txLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create transaction log: %v", err)
	}
	defer txLog.Close()

	// Log an operation
	doc := document.NewDocument()
	doc.AddField("title", "test rollback")
	err = txLog.LogOperation(OpAdd, 1, doc)
	if err != nil {
		t.Errorf("Failed to log operation: %v", err)
	}

	// Verify operation is uncommitted
	uncommitted := txLog.GetUncommittedOperations()
	if len(uncommitted) != 1 {
		t.Errorf("Expected 1 uncommitted operation, got %d", len(uncommitted))
	}

	// Test rollback
	err = txLog.Rollback(1)
	if err != nil {
		t.Errorf("Failed to rollback operation: %v", err)
	}

	// Verify operation was rolled back
	uncommitted = txLog.GetUncommittedOperations()
	if len(uncommitted) != 0 {
		t.Errorf("Expected 0 uncommitted operations after rollback, got %d", len(uncommitted))
	}
}

func TestCrashRecovery(t *testing.T) {
	tmpDir, err := os.MkdirTemp("", "txlog_test")
	if err != nil {
		t.Fatalf("Failed to create temp dir: %v", err)
	}
	defer os.RemoveAll(tmpDir)

	// Create and populate transaction log
	txLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create transaction log: %v", err)
	}

	// Add some operations
	doc1 := document.NewDocument()
	doc1.AddField("title", "doc1")
	doc2 := document.NewDocument()
	doc2.AddField("title", "doc2")

	txLog.LogOperation(OpAdd, 1, doc1)
	txLog.Commit(1)
	txLog.LogOperation(OpAdd, 2, doc2)

	// Simulate crash by not closing properly
	txLog.file.Close()

	// Recover after crash
	recoveredLog, err := NewTransactionLog(tmpDir)
	if err != nil {
		t.Fatalf("Failed to create new transaction log: %v", err)
	}
	defer recoveredLog.Close()

	entries, err := recoveredLog.Recover()
	if err != nil {
		t.Fatalf("Failed to recover log: %v", err)
	}

	// Verify recovered entries
	if len(entries) != 3 { // 2 adds + 1 commit
		t.Errorf("Expected 3 recovered entries, got %d", len(entries))
	}

	// Check that doc1 is committed and doc2 is not
	var foundCommitted, foundUncommitted bool
	for _, entry := range entries {
		if entry.DocumentID == 1 && entry.Committed {
			foundCommitted = true
		}
		if entry.DocumentID == 2 && !entry.Committed {
			foundUncommitted = true
		}
	}

	if !foundCommitted {
		t.Error("Failed to find committed document 1")
	}
	if !foundUncommitted {
		t.Error("Failed to find uncommitted document 2")
	}
}

================
File: txlog/txlog.go
================
package txlog

import (
	"encoding/json"
	"fmt"
	"io"
	"os"
	"path/filepath"
	"sync"
	"time"

	"my-indexer/document"
)

// Operation types for transaction log entries
const (
	OpAdd    = "add"
	OpUpdate = "update"
	OpDelete = "delete"
)

// LogEntry represents a single operation in the transaction log
type LogEntry struct {
	Operation   string              `json:"operation"`
	Timestamp   time.Time           `json:"timestamp"`
	DocumentID  int                 `json:"document_id"`
	Document    *document.Document  `json:"document,omitempty"`
	Committed   bool               `json:"committed"`
}

// TransactionLog manages write-ahead logging and recovery
type TransactionLog struct {
	mu           sync.RWMutex
	file         *os.File
	logPath      string
	encoder      *json.Encoder
	decoder      *json.Decoder
	uncommitted  map[int]*LogEntry
}

// NewTransactionLog creates a new transaction log
func NewTransactionLog(logDir string) (*TransactionLog, error) {
	if err := os.MkdirAll(logDir, 0755); err != nil {
		return nil, fmt.Errorf("failed to create log directory: %v", err)
	}

	logPath := filepath.Join(logDir, "transaction.log")
	file, err := os.OpenFile(logPath, os.O_RDWR|os.O_CREATE|os.O_APPEND, 0644)
	if err != nil {
		return nil, fmt.Errorf("failed to open log file: %v", err)
	}

	txLog := &TransactionLog{
		file:        file,
		logPath:     logPath,
		encoder:     json.NewEncoder(file),
		decoder:     json.NewDecoder(file),
		uncommitted: make(map[int]*LogEntry),
	}

	return txLog, nil
}

// LogOperation logs an operation to the transaction log
func (t *TransactionLog) LogOperation(op string, docID int, doc *document.Document) error {
	t.mu.Lock()
	defer t.mu.Unlock()

	entry := &LogEntry{
		Operation:  op,
		Timestamp:  time.Now(),
		DocumentID: docID,
		Document:   doc,
		Committed:  false,
	}

	if err := t.encoder.Encode(entry); err != nil {
		return fmt.Errorf("failed to encode log entry: %v", err)
	}

	t.uncommitted[docID] = entry
	return nil
}

// Commit marks an operation as committed
func (t *TransactionLog) Commit(docID int) error {
	t.mu.Lock()
	defer t.mu.Unlock()

	entry, exists := t.uncommitted[docID]
	if !exists {
		return fmt.Errorf("no uncommitted operation found for document ID %d", docID)
	}

	entry.Committed = true
	if err := t.encoder.Encode(entry); err != nil {
		return fmt.Errorf("failed to encode commit entry: %v", err)
	}

	delete(t.uncommitted, docID)
	return nil
}

// Rollback removes an uncommitted operation
func (t *TransactionLog) Rollback(docID int) error {
	t.mu.Lock()
	defer t.mu.Unlock()

	if _, exists := t.uncommitted[docID]; !exists {
		return fmt.Errorf("no uncommitted operation found for document ID %d", docID)
	}

	delete(t.uncommitted, docID)
	return nil
}

// GetUncommittedOperations returns all uncommitted operations
func (t *TransactionLog) GetUncommittedOperations() []*LogEntry {
	t.mu.RLock()
	defer t.mu.RUnlock()

	entries := make([]*LogEntry, 0, len(t.uncommitted))
	for _, entry := range t.uncommitted {
		entries = append(entries, entry)
	}
	return entries
}

// Recover processes the transaction log and returns operations that need to be replayed
func (t *TransactionLog) Recover() ([]*LogEntry, error) {
	t.mu.Lock()
	defer t.mu.Unlock()

	if _, err := t.file.Seek(0, 0); err != nil {
		return nil, fmt.Errorf("failed to seek to start of log: %v", err)
	}

	var entries []*LogEntry
	t.uncommitted = make(map[int]*LogEntry) // Reset uncommitted map

	for {
		var entry LogEntry
		if err := t.decoder.Decode(&entry); err != nil {
			if err == io.EOF {
				break
			}
			return nil, fmt.Errorf("failed to decode log entry: %v", err)
		}

		entries = append(entries, &entry)
		
		// Track uncommitted entries in memory
		if !entry.Committed {
			t.uncommitted[entry.DocumentID] = &entry
		} else {
			// If we see a commit entry, remove the corresponding uncommitted entry
			delete(t.uncommitted, entry.DocumentID)
		}
	}

	return entries, nil
}

// Close closes the transaction log file
func (t *TransactionLog) Close() error {
	t.mu.Lock()
	defer t.mu.Unlock()

	if err := t.file.Close(); err != nil {
		return fmt.Errorf("failed to close log file: %v", err)
	}
	return nil
}

// Truncate removes all entries from the log file
func (t *TransactionLog) Truncate() error {
	t.mu.Lock()
	defer t.mu.Unlock()

	if err := t.file.Truncate(0); err != nil {
		return fmt.Errorf("failed to truncate log file: %v", err)
	}
	if _, err := t.file.Seek(0, 0); err != nil {
		return fmt.Errorf("failed to seek to start of log: %v", err)
	}
	t.uncommitted = make(map[int]*LogEntry)
	return nil
}

================
File: .dockerignore
================
# Git
.git
.gitignore

# IDE
.vscode
.idea

# Build artifacts
*.exe
*.exe~
*.dll
*.so
*.dylib
*.test
*.out

# Logs
*.log

# OS specific
.DS_Store
Thumbs.db

================
File: .gitignore
================
.aider*
.env

================
File: .repomixignore
================
.venv
.aider*

================
File: Dockerfile
================
FROM golang:1.21-alpine

WORKDIR /app

# Copy go mod and sum files
COPY go.mod ./

# Download all dependencies
RUN go mod download

# Copy the source code
COPY . .

# Build the application
RUN go build -o main .

# Run tests by default
CMD ["go", "test", "-v", "./..."]

================
File: elasticsearch_api_ref.md
================
# REST API Reference - Open Distro Documentation
**The Open Distro project is archived.** Open Distro development has moved to OpenSearch. The Open Distro plugins will continue to work with legacy versions of Elasticsearch OSS, but we recommend upgrading to OpenSearch to take advantage of the latest features and improvements.

Elasticsearch REST API reference
--------------------------------

This reference originates from the Elasticsearch REST API specification. Were extremely grateful to the Elasticsearch community for their numerous contributions to open source software, including this documentation.

* * *

bulk
----

Perform multiple index, update, and/or delete operations in a single request.

```
POST {index}/_bulk
PUT {index}/_bulk

```


```
POST {index}/{type}/_bulk
PUT {index}/{type}/_bulk

```


#### HTTP request body

The operation definition and data (action-data pairs), separated by newlines.

**Required**: True

#### URL parameters



* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the bulk operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies. Otherwise, set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1).
* Parameter: refresh
  * Type: enum
  * Description: If true, refresh the affected shards to make this operation visible to search. If wait_for, wait for a refresh to make this operation visible to search. If false (the default), do nothing with refreshes.
* Parameter: routing
  * Type: string
  * Description: Specific routing value.
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout.
* Parameter: type
  * Type: string
  * Description: Default document type for items that dont provide one.
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or the default list of fields to return, can be overridden on each sub-request.
* Parameter: _source_excludes
  * Type: list
  * Description: Default list of fields to exclude from the returned _source field, can be overridden on each sub-request.
* Parameter: _source_includes
  * Type: list
  * Description: Default list of fields to extract and return from the _source field, can be overridden on each sub-request.
* Parameter: pipeline
  * Type: string
  * Description: The pipeline ID to preprocess incoming documents with.
* Parameter: require_alias
  * Type: boolean
  * Description: Sets require_alias for all incoming documents, defaults to false (unset).


cat.aliases
-----------

Shows information about currently configured aliases to indices including filter and routing infos.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


cat.allocation
--------------

Provides a snapshot of how many shards are allocated to each data node and how much disk space they are using.

```
GET _cat/allocation/{node_id}

```


#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: bytes
  * Type: enum
  * Description: The unit in which to display byte values
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.count
---------

Provides quick access to the document count of the entire cluster, or individual indices.

#### URL parameters


|Parameter|Type   |Description                                                      |
|---------|-------|-----------------------------------------------------------------|
|format   |string |a short version of the Accept header, e.g. json, yaml            |
|h        |list   |Comma-separated list of column names to display                  |
|help     |boolean|Return help information                                          |
|s        |list   |Comma-separated list of column names or column aliases to sort by|
|v        |boolean|Verbose mode. Display column headers                             |


cat.fielddata
-------------

Shows how much heap memory is currently being used by fielddata on every data node in the cluster.

```
GET _cat/fielddata/{fields}

```


#### URL parameters


|Parameter|Type   |Description                                                      |
|---------|-------|-----------------------------------------------------------------|
|format   |string |a short version of the Accept header, e.g. json, yaml            |
|bytes    |enum   |The unit in which to display byte values                         |
|h        |list   |Comma-separated list of column names to display                  |
|help     |boolean|Return help information                                          |
|s        |list   |Comma-separated list of column names or column aliases to sort by|
|v        |boolean|Verbose mode. Display column headers                             |
|fields   |list   |A comma-separated list of fields to return in the output         |


cat.health
----------

Returns a concise representation of the cluster health.

#### URL parameters


|Parameter|Type   |Description                                                      |
|---------|-------|-----------------------------------------------------------------|
|format   |string |a short version of the Accept header, e.g. json, yaml            |
|h        |list   |Comma-separated list of column names to display                  |
|help     |boolean|Return help information                                          |
|s        |list   |Comma-separated list of column names or column aliases to sort by|
|time     |enum   |The unit in which to display time values                         |
|ts       |boolean|Set to false to disable timestamping                             |
|v        |boolean|Verbose mode. Display column headers                             |


cat.help
--------

Returns help for the Cat APIs.

#### URL parameters


|Parameter|Type   |Description                                                      |
|---------|-------|-----------------------------------------------------------------|
|help     |boolean|Return help information                                          |
|s        |list   |Comma-separated list of column names or column aliases to sort by|


cat.indices
-----------

Returns information about indices: number of primaries and replicas, document counts, disk size, 

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: bytes
  * Type: enum
  * Description: The unit in which to display byte values
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: health
  * Type: enum
  * Description: A health status (green, yellow, or red to filter only indices matching the specified health status
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: pri
  * Type: boolean
  * Description: Set to true to return stats only for primary shards
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers
* Parameter: include_unloaded_segments
  * Type: boolean
  * Description: If set to true segment stats will include stats for segments that are not currently loaded into memory
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


cat.master
----------

Returns information about the master node.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.nodeattrs
-------------

Returns information about custom node attributes.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.nodes
---------

Returns basic statistics about performance of cluster nodes.

#### URL parameters



* Parameter: bytes
  * Type: enum
  * Description: The unit in which to display byte values
* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: full_id
  * Type: boolean
  * Description: Return the full node ID instead of the shortened version (default: false)
* Parameter: local
  * Type: boolean
  * Description: Calculate the selected nodes using the local cluster state rather than the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.pending\_tasks
------------------

Returns a concise representation of the cluster pending tasks.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.plugins
-----------

Returns information about installed plugins across nodes node.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.recovery
------------

Returns information about index shard recoveries, both on-going completed.

```
GET _cat/recovery/{index}

```


#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: active_only
  * Type: boolean
  * Description: If true, the response only includes ongoing shard recoveries
* Parameter: bytes
  * Type: enum
  * Description: The unit in which to display byte values
* Parameter: detailed
  * Type: boolean
  * Description: If true, the response includes detailed information about shard recoveries
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: index
  * Type: list
  * Description: Comma-separated list or wildcard expression of index names to limit the returned information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.repositories
----------------

Returns information about snapshot repositories registered in the cluster.

#### URL parameters


|Parameter     |Type   |Description                                                         |
|--------------|-------|--------------------------------------------------------------------|
|format        |string |a short version of the Accept header, e.g. json, yaml               |
|local         |boolean|Return local information, do not retrieve the state from master node|
|master_timeout|time   |Explicit operation timeout for connection to master node            |
|h             |list   |Comma-separated list of column names to display                     |
|help          |boolean|Return help information                                             |
|s             |list   |Comma-separated list of column names or column aliases to sort by   |
|v             |boolean|Verbose mode. Display column headers                                |


cat.segments
------------

Provides low-level information about the segments in the shards of an index.

```
GET _cat/segments/{index}

```


#### URL parameters


|Parameter|Type   |Description                                                      |
|---------|-------|-----------------------------------------------------------------|
|format   |string |a short version of the Accept header, e.g. json, yaml            |
|bytes    |enum   |The unit in which to display byte values                         |
|h        |list   |Comma-separated list of column names to display                  |
|help     |boolean|Return help information                                          |
|s        |list   |Comma-separated list of column names or column aliases to sort by|
|v        |boolean|Verbose mode. Display column headers                             |


cat.shards
----------

Provides a detailed view of shard allocation on nodes.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: bytes
  * Type: enum
  * Description: The unit in which to display byte values
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.snapshots
-------------

Returns all snapshots in a specific repository.

```
GET _cat/snapshots/{repository}

```


#### URL parameters


|Parameter         |Type   |Description                                                      |
|------------------|-------|-----------------------------------------------------------------|
|format            |string |a short version of the Accept header, e.g. json, yaml            |
|ignore_unavailable|boolean|Set to true to ignore unavailable snapshots                      |
|master_timeout    |time   |Explicit operation timeout for connection to master node         |
|h                 |list   |Comma-separated list of column names to display                  |
|help              |boolean|Return help information                                          |
|s                 |list   |Comma-separated list of column names or column aliases to sort by|
|time              |enum   |The unit in which to display time values                         |
|v                 |boolean|Verbose mode. Display column headers                             |


cat.tasks
---------

Returns information about the tasks currently executing on one or more nodes in the cluster.

#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: nodes
  * Type: list
  * Description: A comma-separated list of node IDs or names to limit the returned information; use _local to return information from the node youre connecting to, leave empty to get information from all nodes
* Parameter: actions
  * Type: list
  * Description: A comma-separated list of actions that should be returned. Leave empty to return all.
* Parameter: detailed
  * Type: boolean
  * Description: Return detailed task information (default: false)
* Parameter: parent_task_id
  * Type: string
  * Description: Return tasks with specified parent task id (node_id:task_number). Set to -1 to return all.
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: time
  * Type: enum
  * Description: The unit in which to display time values
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.templates
-------------

Returns information about existing templates.

```
GET _cat/templates/{name}

```


#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


cat.thread\_pool
----------------

Returns cluster-wide thread pool statistics per node. By default the active, queue and rejected statistics are returned for all thread pools.

```
GET _cat/thread_pool/{thread_pool_patterns}

```


#### URL parameters



* Parameter: format
  * Type: string
  * Description: a short version of the Accept header, e.g. json, yaml
* Parameter: size
  * Type: enum
  * Description: The multiplier in which to display values
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: h
  * Type: list
  * Description: Comma-separated list of column names to display
* Parameter: help
  * Type: boolean
  * Description: Return help information
* Parameter: s
  * Type: list
  * Description: Comma-separated list of column names or column aliases to sort by
* Parameter: v
  * Type: boolean
  * Description: Verbose mode. Display column headers


Explicitly clears the search context for a scroll.

```
DELETE _search/scroll/{scroll_id}

```


#### HTTP request body

A comma-separated list of scroll IDs to clear if none was specified via the scroll\_id parameter

cluster.allocation\_explain
---------------------------

Provides explanations for shard allocations in the cluster.

```
GET _cluster/allocation/explain
POST _cluster/allocation/explain

```


#### HTTP request body

The index, shard, and primary flag to explain. Empty means explain the first unassigned shard

#### URL parameters



* Parameter: include_yes_decisions
  * Type: boolean
  * Description: Return YES decisions in explanation (default: false)
* Parameter: include_disk_info
  * Type: boolean
  * Description: Return information about disk usage and shard sizes (default: false)


cluster.delete\_component\_template
-----------------------------------

Deletes a component template

```
DELETE _component_template/{name}

```


#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit operation timeout              |
|master_timeout|time|Specify timeout for connection to master|


cluster.delete\_voting\_config\_exclusions
------------------------------------------

Clears cluster voting config exclusions.

```
DELETE _cluster/voting_config_exclusions

```


#### URL parameters



* Parameter: wait_for_removal
  * Type: boolean
  * Description: Specifies whether to wait for all excluded nodes to be removed from the cluster before clearing the voting configuration exclusions list.


cluster.exists\_component\_template
-----------------------------------

Returns information about whether a particular component template exist

```
HEAD _component_template/{name}

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


cluster.get\_component\_template
--------------------------------

Returns one or more component templates

```
GET _component_template/{name}

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


cluster.get\_settings
---------------------

Returns cluster settings.

#### URL parameters


|Parameter       |Type   |Description                                             |
|----------------|-------|--------------------------------------------------------|
|flat_settings   |boolean|Return settings in flat format (default: false)         |
|master_timeout  |time   |Explicit operation timeout for connection to master node|
|timeout         |time   |Explicit operation timeout                              |
|include_defaults|boolean|Whether to return all default clusters setting.         |


cluster.health
--------------

Returns basic information about the health of the cluster.

```
GET _cluster/health/{index}

```


#### URL parameters



* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: level
  * Type: enum
  * Description: Specify the level of detail for returned information
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Wait until the specified number of shards is active
* Parameter: wait_for_nodes
  * Type: string
  * Description: Wait until the specified number of nodes is available
* Parameter: wait_for_events
  * Type: enum
  * Description: Wait until all currently queued events with the given priority are processed
* Parameter: wait_for_no_relocating_shards
  * Type: boolean
  * Description: Whether to wait until there are no relocating shards in the cluster
* Parameter: wait_for_no_initializing_shards
  * Type: boolean
  * Description: Whether to wait until there are no initializing shards in the cluster
* Parameter: wait_for_status
  * Type: enum
  * Description: Wait until cluster is in a specific state


cluster.pending\_tasks
----------------------

Returns a list of any cluster-level changes (e.g. create index, update mapping, allocate or fail shard) which have not yet been executed.

```
GET _cluster/pending_tasks

```


#### URL parameters



* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


cluster.post\_voting\_config\_exclusions
----------------------------------------

Updates the cluster voting config exclusions by node ids or node names.

```
POST _cluster/voting_config_exclusions

```


#### URL parameters



* Parameter: node_ids
  * Type: string
  * Description: A comma-separated list of the persistent ids of the nodes to exclude from the voting configuration. If specified, you may not also specify ?node_names.
* Parameter: node_names
  * Type: string
  * Description: A comma-separated list of the names of the nodes to exclude from the voting configuration. If specified, you may not also specify ?node_ids.
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout


cluster.put\_component\_template
--------------------------------

Creates or updates a component template

```
PUT _component_template/{name}
POST _component_template/{name}

```


#### HTTP request body

The template definition

**Required**: True

#### URL parameters



* Parameter: create
  * Type: boolean
  * Description: Whether the index template should only be added if new or can also replace an existing one
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


cluster.put\_settings
---------------------

Updates the cluster settings.

#### HTTP request body

The settings to be updated. Can be either `transient` or `persistent` (survives cluster restart).

**Required**: True

#### URL parameters


|Parameter     |Type   |Description                                             |
|--------------|-------|--------------------------------------------------------|
|flat_settings |boolean|Return settings in flat format (default: false)         |
|master_timeout|time   |Explicit operation timeout for connection to master node|
|timeout       |time   |Explicit operation timeout                              |


cluster.remote\_info
--------------------

Returns the information about configured remote clusters.

cluster.reroute
---------------

Allows to manually change the allocation of individual shards in the cluster.

#### HTTP request body

The definition of `commands` to perform (`move`, `cancel`, `allocate`)

#### URL parameters



* Parameter: dry_run
  * Type: boolean
  * Description: Simulate the operation only and return the resulting state
* Parameter: explain
  * Type: boolean
  * Description: Return an explanation of why the commands can or cannot be executed
* Parameter: retry_failed
  * Type: boolean
  * Description: Retries allocation of shards that are blocked due to too many subsequent allocation failures
* Parameter: metric
  * Type: list
  * Description: Limit the information returned to the specified metrics. Defaults to all but metadata
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout


cluster.state
-------------

Returns a comprehensive information about the state of the cluster.

```
GET _cluster/state/{metric}

```


```
GET _cluster/state/{metric}/{index}

```


#### URL parameters



* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: wait_for_metadata_version
  * Type: number
  * Description: Wait for the metadata version to be equal or greater than the specified metadata version
* Parameter: wait_for_timeout
  * Type: time
  * Description: The maximum time to wait for wait_for_metadata_version before timing out
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


cluster.stats
-------------

Returns high-level overview of cluster statistics.

```
GET _cluster/stats/nodes/{node_id}

```


#### URL parameters


|Parameter    |Type   |Description                                    |
|-------------|-------|-----------------------------------------------|
|flat_settings|boolean|Return settings in flat format (default: false)|
|timeout      |time   |Explicit operation timeout                     |


count
-----

Returns number of documents matching a query.

```
POST {index}/_count
GET {index}/_count

```


```
POST {index}/{type}/_count
GET {index}/{type}/_count

```


#### HTTP request body

A query to restrict the results specified with the Query DSL (optional)

#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: ignore_throttled
  * Type: boolean
  * Description: Whether specified concrete, expanded or aliased indices should be ignored when throttled
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: min_score
  * Type: number
  * Description: Include only documents with a specific _score value in the result
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: routing
  * Type: list
  * Description: A comma-separated list of specific routing values
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
* Parameter: analyzer
  * Type: string
  * Description: The analyzer to use for the query string
* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcard and prefix queries should be analyzed (default: false)
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
* Parameter: df
  * Type: string
  * Description: The field to use as default where no field prefix is given in the query string
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
* Parameter: terminate_after
  * Type: number
  * Description: The maximum count for each shard, upon reaching which the query execution will terminate early


create
------

Creates a new document in the index.

Returns a 409 response when a document with a same ID already exists in the index.

```
PUT {index}/_create/{id}
POST {index}/_create/{id}

```


```
PUT {index}/{type}/{id}/_create
POST {index}/{type}/{id}/_create

```


#### HTTP request body

The document

**Required**: True

#### URL parameters



* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the index operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
* Parameter: refresh
  * Type: enum
  * Description: If true then refresh the affected shards to make this operation visible to search, if wait_for then wait for a refresh to make this operation visible to search, if false (the default) then do nothing with refreshes.
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type
* Parameter: pipeline
  * Type: string
  * Description: The pipeline id to preprocess incoming documents with


dangling\_indices.delete\_dangling\_index
-----------------------------------------

Deletes the specified dangling index

```
DELETE _dangling/{index_uuid}

```


#### URL parameters


|Parameter       |Type   |Description                                              |
|----------------|-------|---------------------------------------------------------|
|accept_data_loss|boolean|Must be set to true in order to delete the dangling index|
|timeout         |time   |Explicit operation timeout                               |
|master_timeout  |time   |Specify timeout for connection to master                 |


dangling\_indices.import\_dangling\_index
-----------------------------------------

Imports the specified dangling index

```
POST _dangling/{index_uuid}

```


#### URL parameters


|Parameter       |Type   |Description                                              |
|----------------|-------|---------------------------------------------------------|
|accept_data_loss|boolean|Must be set to true in order to import the dangling index|
|timeout         |time   |Explicit operation timeout                               |
|master_timeout  |time   |Specify timeout for connection to master                 |


dangling\_indices.list\_dangling\_indices
-----------------------------------------

Returns all dangling indices.

delete
------

Removes a document from the index.

```
DELETE {index}/{type}/{id}

```


#### URL parameters



* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the delete operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
* Parameter: refresh
  * Type: enum
  * Description: If true then refresh the affected shards to make this operation visible to search, if wait_for then wait for a refresh to make this operation visible to search, if false (the default) then do nothing with refreshes.
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: if_seq_no
  * Type: number
  * Description: only perform the delete operation if the last operation that has changed the document has the specified sequence number
* Parameter: if_primary_term
  * Type: number
  * Description: only perform the delete operation if the last operation that has changed the document has the specified primary term
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


delete\_by\_query
-----------------

Deletes documents matching the provided query.

```
POST {index}/_delete_by_query

```


```
POST {index}/{type}/_delete_by_query

```


#### HTTP request body

The search definition using the Query DSL

**Required**: True

#### URL parameters



* Parameter: analyzer
  * Type: string
  * Description: The analyzer to use for the query string
  * : 
* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcard and prefix queries should be analyzed (default: false)
  * : 
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
  * : 
* Parameter: df
  * Type: string
  * Description: The field to use as default where no field prefix is given in the query string
  * : 
* Parameter: from
  * Type: number
  * Description: Starting offset (default: 0)
  * : 
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
  * : 
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
  * : 
* Parameter: conflicts
  * Type: enum
  * Description: What to do when the delete by query hits version conflicts?
  * : 
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
  * : 
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
  * : 
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
  * : 
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
  * : 
* Parameter: routing
  * Type: list
  * Description: A comma-separated list of specific routing values
  * : 
* Parameter: scroll
  * Type: time
  * Description: Specify how long a consistent view of the index should be maintained for scrolled search
  * : 
* Parameter: search_type
  * Type: enum
  * Description: Search operation type
  * : 
* Parameter: search_timeout
  * Type: time
  * Description: Explicit timeout for each search request. Defaults to no timeout.
  * : 
* Parameter: size
  * Type: number
  * Description: Deprecated, please use max_docs instead
  * : 
* Parameter: max_docs
  * Type: number
  * Description: Maximum number of documents to process (default: all documents)
  * : 
* Parameter: sort
  * Type: list
  * Description: A comma-separated list of : pairs
  * : 
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
  * : 
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
  * : 
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
  * : 
* Parameter: terminate_after
  * Type: number
  * Description: The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate early.
  * : 
* Parameter: stats
  * Type: list
  * Description: Specific tag of the request for logging and statistical purposes
  * : 
* Parameter: version
  * Type: boolean
  * Description: Specify whether to return document version as part of a hit
  * : 
* Parameter: request_cache
  * Type: boolean
  * Description: Specify if request cache should be used for this request or not, defaults to index level setting
  * : 
* Parameter: refresh
  * Type: boolean
  * Description: Should the effected indexes be refreshed?
  * : 
* Parameter: timeout
  * Type: time
  * Description: Time each individual bulk request should wait for shards that are unavailable.
  * : 
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the delete by query operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
  * : 
* Parameter: scroll_size
  * Type: number
  * Description: Size on the scroll request powering the delete by query
  * : 
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should the request should block until the delete by query is complete.
  * : 
* Parameter: requests_per_second
  * Type: number
  * Description: The throttle for this request in sub-requests per second. -1 means no throttle.
  * : 
* Parameter: slices
  * Type: number
  * Description: string
  * : The number of slices this task should be divided into. Defaults to 1, meaning the task isnt sliced into subtasks. Can be set to auto.


delete\_by\_query\_rethrottle
-----------------------------

Changes the number of requests per second for a particular Delete By Query operation.

```
POST _delete_by_query/{task_id}/_rethrottle

```


#### URL parameters



* Parameter: requests_per_second
  * Type: number
  * Description: The throttle to set on this request in floating sub-requests per second. -1 means set no throttle.


delete\_script
--------------

Deletes a script.

#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit operation timeout              |
|master_timeout|time|Specify timeout for connection to master|


exists
------

Returns information about whether a document exists in an index.

#### URL parameters



* Parameter: stored_fields
  * Type: list
  * Description: A comma-separated list of stored fields to return in the response
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: realtime
  * Type: boolean
  * Description: Specify whether to perform the operation in realtime or search mode
* Parameter: refresh
  * Type: boolean
  * Description: Refresh the shard containing the document before performing the operation
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


exists\_source
--------------

Returns information about whether a document source exists in an index.

```
HEAD {index}/_source/{id}

```


```
HEAD {index}/{type}/{id}/_source

```


#### URL parameters



* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: realtime
  * Type: boolean
  * Description: Specify whether to perform the operation in realtime or search mode
* Parameter: refresh
  * Type: boolean
  * Description: Refresh the shard containing the document before performing the operation
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


explain
-------

Returns information about why a specific matches (or doesnt match) a query.

```
GET {index}/_explain/{id}
POST {index}/_explain/{id}

```


```
GET {index}/{type}/{id}/_explain
POST {index}/{type}/{id}/_explain

```


#### HTTP request body

The query definition using the Query DSL

#### URL parameters



* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcards and prefix queries in the query string query should be analyzed (default: false)
* Parameter: analyzer
  * Type: string
  * Description: The analyzer for the query string query
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
* Parameter: df
  * Type: string
  * Description: The default field for query string query (default: _all)
* Parameter: stored_fields
  * Type: list
  * Description: A comma-separated list of stored fields to return in the response
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field


field\_caps
-----------

Returns the information about the capabilities of fields among multiple indices.

```
GET _field_caps
POST _field_caps

```


```
GET {index}/_field_caps
POST {index}/_field_caps

```


#### HTTP request body

An index filter specified with the Query DSL

#### URL parameters



* Parameter: fields
  * Type: list
  * Description: A comma-separated list of field names
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: include_unmapped
  * Type: boolean
  * Description: Indicates whether unmapped fields should be included in the response.


get
---

Returns a document.

#### URL parameters



* Parameter: stored_fields
  * Type: list
  * Description: A comma-separated list of stored fields to return in the response
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: realtime
  * Type: boolean
  * Description: Specify whether to perform the operation in realtime or search mode
* Parameter: refresh
  * Type: boolean
  * Description: Refresh the shard containing the document before performing the operation
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


get\_script
-----------

Returns a script.

#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|master_timeout|time|Specify timeout for connection to master|


get\_script\_context
--------------------

Returns all script contexts.

get\_script\_languages
----------------------

Returns available script types, languages and contexts

get\_source
-----------

Returns the source of a document.

```
GET {index}/{type}/{id}/_source

```


#### URL parameters



* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: realtime
  * Type: boolean
  * Description: Specify whether to perform the operation in realtime or search mode
* Parameter: refresh
  * Type: boolean
  * Description: Refresh the shard containing the document before performing the operation
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


index
-----

Creates or updates a document in an index.

```
PUT {index}/_doc/{id}
POST {index}/_doc/{id}

```


```
PUT {index}/{type}/{id}
POST {index}/{type}/{id}

```


#### HTTP request body

The document

**Required**: True

#### URL parameters



* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the index operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
* Parameter: op_type
  * Type: enum
  * Description: Explicit operation type. Defaults to index for requests with an explicit document ID, and to createfor requests without an explicit document ID
* Parameter: refresh
  * Type: enum
  * Description: If true then refresh the affected shards to make this operation visible to search, if wait_for then wait for a refresh to make this operation visible to search, if false (the default) then do nothing with refreshes.
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type
* Parameter: if_seq_no
  * Type: number
  * Description: only perform the index operation if the last operation that has changed the document has the specified sequence number
* Parameter: if_primary_term
  * Type: number
  * Description: only perform the index operation if the last operation that has changed the document has the specified primary term
* Parameter: pipeline
  * Type: string
  * Description: The pipeline id to preprocess incoming documents with
* Parameter: require_alias
  * Type: boolean
  * Description: When true, requires destination to be an alias. Default is false


indices.add\_block
------------------

Adds a block to an index.

```
PUT {index}/_block/{block}

```


#### URL parameters



* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.analyze
---------------

Performs the analysis process on a text and return the tokens breakdown of the text.

```
GET _analyze
POST _analyze

```


```
GET {index}/_analyze
POST {index}/_analyze

```


#### HTTP request body

Define analyzer/tokenizer parameters and the text on which the analysis should be performed

#### URL parameters


|Parameter|Type  |Description                                 |
|---------|------|--------------------------------------------|
|index    |string|The name of the index to scope the operation|


indices.clear\_cache
--------------------

Clears all or specific caches for one or more indices.

```
POST {index}/_cache/clear

```


#### URL parameters



* Parameter: fielddata
  * Type: boolean
  * Description: Clear field data
* Parameter: fields
  * Type: list
  * Description: A comma-separated list of fields to clear when using the fielddata parameter (default: all)
* Parameter: query
  * Type: boolean
  * Description: Clear query caches
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: index
  * Type: list
  * Description: A comma-separated list of index name to limit the operation
* Parameter: request
  * Type: boolean
  * Description: Clear request cache


indices.clone
-------------

Clones an index

```
PUT {index}/_clone/{target}
POST {index}/_clone/{target}

```


#### HTTP request body

The configuration for the target index (`settings` and `aliases`)

#### URL parameters



* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Set the number of active shards to wait for on the cloned index before the operation returns.


indices.close
-------------

Closes an index.

#### URL parameters



* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of active shards to wait for before the operation returns.


indices.create
--------------

Creates an index with optional settings and mappings.

#### HTTP request body

The configuration for the index (`settings` and `mappings`)

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be expected in the body of the mappings.
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Set the number of active shards to wait for before the operation returns.
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.delete
--------------

Deletes an index.

#### URL parameters



* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Ignore unavailable indexes (default: false)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Ignore if a wildcard expression resolves to no concrete indices (default: false)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether wildcard expressions should get expanded to open or closed indices (default: open)


indices.delete\_alias
---------------------

Deletes an alias.

```
DELETE {index}/_alias/{name}

```


```
DELETE {index}/_aliases/{name}

```


#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit timestamp for the document     |
|master_timeout|time|Specify timeout for connection to master|


indices.delete\_index\_template
-------------------------------

Deletes an index template.

```
DELETE _index_template/{name}

```


#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit operation timeout              |
|master_timeout|time|Specify timeout for connection to master|


indices.delete\_template
------------------------

Deletes an index template.

#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit operation timeout              |
|master_timeout|time|Specify timeout for connection to master|


indices.exists
--------------

Returns information about whether a particular index exists.

#### URL parameters



* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Ignore unavailable indexes (default: false)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Ignore if a wildcard expression resolves to no concrete indices (default: false)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether wildcard expressions should get expanded to open or closed indices (default: open)
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: include_defaults
  * Type: boolean
  * Description: Whether to return all default setting for each of the indices.


indices.exists\_alias
---------------------

Returns information about whether a particular alias exists.

```
HEAD {index}/_alias/{name}

```


#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.exists\_index\_template
-------------------------------

Returns information about whether a particular index template exists.

```
HEAD _index_template/{name}

```


#### URL parameters



* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.exists\_template
------------------------

Returns information about whether a particular index template exists.

#### URL parameters



* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.exists\_type
--------------------

Returns information about whether a particular document type exists. (DEPRECATED)

```
HEAD {index}/_mapping/{type}

```


#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.flush
-------------

Performs the flush operation on one or more indices.

```
POST {index}/_flush
GET {index}/_flush

```


#### URL parameters



* Parameter: force
  * Type: boolean
  * Description: Whether a flush should be forced even if it is not necessarily needed ie. if no changes will be committed to the index. This is useful if transaction log IDs should be incremented even if no uncommitted changes are present. (This setting can be considered as internal)
* Parameter: wait_if_ongoing
  * Type: boolean
  * Description: If set to true the flush operation will block until the flush can be executed if another flush operation is already executing. The default is true. If set to false the flush will be skipped iff if another flush operation is already running.
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.flush\_synced
---------------------

Performs a synced flush operation on one or more indices. Synced flush is deprecated and will be removed in 8.0. Use flush instead

```
POST _flush/synced
GET _flush/synced

```


```
POST {index}/_flush/synced
GET {index}/_flush/synced

```


#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.forcemerge
------------------

Performs the force merge operation on one or more indices.

#### URL parameters



* Parameter: flush
  * Type: boolean
  * Description: Specify whether the index should be flushed after performing the operation (default: true)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: max_num_segments
  * Type: number
  * Description: The number of segments the index should be merged into (default: dynamic)
* Parameter: only_expunge_deletes
  * Type: boolean
  * Description: Specify whether the operation should only expunge deleted documents


indices.get
-----------

Returns information about one or more indices.

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether to add the type name to the response (default: false)
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Ignore unavailable indexes (default: false)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Ignore if a wildcard expression resolves to no concrete indices (default: false)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether wildcard expressions should get expanded to open or closed indices (default: open)
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: include_defaults
  * Type: boolean
  * Description: Whether to return all default setting for each of the indices.
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.get\_alias
------------------

Returns an alias.

```
GET {index}/_alias/{name}

```


#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.get\_field\_mapping
---------------------------

Returns mapping for one or more fields.

```
GET _mapping/field/{fields}

```


```
GET {index}/_mapping/field/{fields}

```


```
GET _mapping/{type}/field/{fields}

```


```
GET {index}/_mapping/{type}/field/{fields}

```


#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be returned in the body of the mappings.
* Parameter: include_defaults
  * Type: boolean
  * Description: Whether the default mapping values should be returned as well
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.get\_index\_template
----------------------------

Returns an index template.

```
GET _index_template/{name}

```


#### URL parameters



* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.get\_mapping
--------------------

Returns mappings for one or more indices.

```
GET {index}/_mapping/{type}

```


#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether to add the type name to the response (default: false)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.get\_settings
---------------------

Returns settings for one or more indices.

```
GET {index}/_settings/{name}

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: include_defaults
  * Type: boolean
  * Description: Whether to return all default setting for each of the indices.


indices.get\_template
---------------------

Returns an index template.

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be returned in the body of the mappings.
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)
* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


indices.get\_upgrade
--------------------

The \_upgrade API is no longer useful and will be removed.

#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.open
------------

Opens an index.

#### URL parameters



* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of active shards to wait for before the operation returns.


indices.put\_alias
------------------

Creates or updates an alias.

```
PUT {index}/_alias/{name}
POST {index}/_alias/{name}

```


```
PUT {index}/_aliases/{name}
POST {index}/_aliases/{name}

```


#### HTTP request body

The settings for the alias, such as `routing` or `filter`

**Required**: False

#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Explicit timestamp for the document     |
|master_timeout|time|Specify timeout for connection to master|


indices.put\_index\_template
----------------------------

Creates or updates an index template.

```
PUT _index_template/{name}
POST _index_template/{name}

```


#### HTTP request body

The template definition

**Required**: True

#### URL parameters



* Parameter: create
  * Type: boolean
  * Description: Whether the index template should only be added if new or can also replace an existing one
* Parameter: cause
  * Type: string
  * Description: User defined reason for creating/updating the index template
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.put\_mapping
--------------------

Updates the index mappings.

```
PUT {index}/_mapping
POST {index}/_mapping

```


```
PUT {index}/{type}/_mapping
POST {index}/{type}/_mapping

```


```
PUT {index}/_mapping/{type}
POST {index}/_mapping/{type}

```


```
PUT {index}/{type}/_mappings
POST {index}/{type}/_mappings

```


```
PUT {index}/_mappings/{type}
POST {index}/_mappings/{type}

```


```
PUT _mappings/{type}
POST _mappings/{type}

```


```
PUT {index}/_mappings
POST {index}/_mappings

```


```
PUT _mapping/{type}
POST _mapping/{type}

```


#### HTTP request body

The mapping definition

**Required**: True

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be expected in the body of the mappings.
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: write_index_only
  * Type: boolean
  * Description: When true, applies mappings only to the write index of an alias or data stream


indices.put\_settings
---------------------

Updates the index settings.

#### HTTP request body

The index settings to be updated

**Required**: True

#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: preserve_existing
  * Type: boolean
  * Description: Whether to update existing settings. If set to true existing settings on an index remain unchanged, the default is false
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: flat_settings
  * Type: boolean
  * Description: Return settings in flat format (default: false)


indices.put\_template
---------------------

Creates or updates an index template.

```
PUT _template/{name}
POST _template/{name}

```


#### HTTP request body

The template definition

**Required**: True

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be returned in the body of the mappings.
* Parameter: order
  * Type: number
  * Description: The order for this template when merging multiple matching ones (higher numbers are merged later, overriding the lower numbers)
* Parameter: create
  * Type: boolean
  * Description: Whether the index template should only be added if new or can also replace an existing one
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.recovery
----------------

Returns information about ongoing index shard recoveries.

#### URL parameters


|Parameter  |Type   |Description                                                 |
|-----------|-------|------------------------------------------------------------|
|detailed   |boolean|Whether to display detailed information about shard recovery|
|active_only|boolean|Display only those recoveries that are currently on-going   |


indices.refresh
---------------

Performs the refresh operation in one or more indices.

```
POST _refresh
GET _refresh

```


```
POST {index}/_refresh
GET {index}/_refresh

```


#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.resolve\_index
----------------------

Returns information about any matching indices, aliases, and data streams

```
GET _resolve/index/{name}

```


#### URL parameters



* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether wildcard expressions should get expanded to open or closed indices (default: open)


indices.rollover
----------------

Updates an alias to point to a new index when the existing index is considered to be too large or too old.

```
POST {alias}/_rollover/{new_index}

```


#### HTTP request body

The conditions that needs to be met for executing rollover

#### URL parameters



* Parameter: include_type_name
  * Type: boolean
  * Description: Whether a type should be included in the body of the mappings.
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: dry_run
  * Type: boolean
  * Description: If set to true the rollover action will only be validated but not actually performed even if a condition matches. The default is false
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Set the number of active shards to wait for on the newly created rollover index before the operation returns.


indices.segments
----------------

Provides low-level information about segments in a Lucene index.

#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: verbose
  * Type: boolean
  * Description: Includes detailed memory usage by Lucene.


indices.shard\_stores
---------------------

Provides store information for shard copies of indices.

```
GET {index}/_shard_stores

```


#### URL parameters



* Parameter: status
  * Type: list
  * Description: A comma-separated list of statuses used to filter on shards to get store information for
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


indices.shrink
--------------

Allow to shrink an existing index into a new index with fewer primary shards.

```
PUT {index}/_shrink/{target}
POST {index}/_shrink/{target}

```


#### HTTP request body

The configuration for the target index (`settings` and `aliases`)

#### URL parameters



* Parameter: copy_settings
  * Type: boolean
  * Description: whether or not to copy settings from the source index (defaults to false)
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Set the number of active shards to wait for on the shrunken index before the operation returns.


indices.simulate\_index\_template
---------------------------------

Simulate matching the given index name against the index templates in the system

```
POST _index_template/_simulate_index/{name}

```


#### HTTP request body

New index template definition, which will be included in the simulation, as if it already exists in the system

**Required**: False

#### URL parameters



* Parameter: create
  * Type: boolean
  * Description: Whether the index template we optionally defined in the body should only be dry-run added if new or can also replace an existing one
* Parameter: cause
  * Type: string
  * Description: User defined reason for dry-run creating the new template for simulation purposes
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.simulate\_template
--------------------------

Simulate resolving the given template name or body

```
POST _index_template/_simulate

```


```
POST _index_template/_simulate/{name}

```


#### HTTP request body

New index template definition to be simulated, if no index template name is specified

**Required**: False

#### URL parameters



* Parameter: create
  * Type: boolean
  * Description: Whether the index template we optionally defined in the body should only be dry-run added if new or can also replace an existing one
* Parameter: cause
  * Type: string
  * Description: User defined reason for dry-run creating the new template for simulation purposes
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master


indices.split
-------------

Allows you to split an existing index into a new index with more primary shards.

```
PUT {index}/_split/{target}
POST {index}/_split/{target}

```


#### HTTP request body

The configuration for the target index (`settings` and `aliases`)

#### URL parameters



* Parameter: copy_settings
  * Type: boolean
  * Description: whether or not to copy settings from the source index (defaults to false)
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: master_timeout
  * Type: time
  * Description: Specify timeout for connection to master
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Set the number of active shards to wait for on the shrunken index before the operation returns.


indices.stats
-------------

Provides statistics on operations happening in an index.

```
GET {index}/_stats/{metric}

```


#### URL parameters



* Parameter: completion_fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata and suggest index metric (supports wildcards)
* Parameter: fielddata_fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata index metric (supports wildcards)
* Parameter: fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata and completion index metric (supports wildcards)
* Parameter: groups
  * Type: list
  * Description: A comma-separated list of search groups for search index metric
* Parameter: level
  * Type: enum
  * Description: Return stats aggregated at cluster, index or shard level
* Parameter: types
  * Type: list
  * Description: A comma-separated list of document types for the indexing index metric
* Parameter: include_segment_file_sizes
  * Type: boolean
  * Description: Whether to report the aggregated disk usage of each one of the Lucene index files (only applies if segment stats are requested)
* Parameter: include_unloaded_segments
  * Type: boolean
  * Description: If set to true segment stats will include stats for segments that are not currently loaded into memory
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: forbid_closed_indices
  * Type: boolean
  * Description: If set to false stats will also collected from closed indices if explicitly specified or if expand_wildcards expands to closed indices


indices.update\_aliases
-----------------------

Updates index aliases.

#### HTTP request body

The definition of `actions` to perform

**Required**: True

#### URL parameters


|Parameter     |Type|Description                             |
|--------------|----|----------------------------------------|
|timeout       |time|Request timeout                         |
|master_timeout|time|Specify timeout for connection to master|


indices.upgrade
---------------

The \_upgrade API is no longer useful and will be removed.

#### URL parameters



* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Specify whether the request should block until the all segments are upgraded (default: false)
* Parameter: only_ancient_segments
  * Type: boolean
  * Description: If true, only ancient (an older Lucene major release) segments will be upgraded


indices.validate\_query
-----------------------

Allows a user to validate a potentially expensive query without executing it.

```
GET _validate/query
POST _validate/query

```


```
GET {index}/_validate/query
POST {index}/_validate/query

```


```
GET {index}/{type}/_validate/query
POST {index}/{type}/_validate/query

```


#### HTTP request body

The query definition specified with the Query DSL

#### URL parameters



* Parameter: explain
  * Type: boolean
  * Description: Return detailed information about the error
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
* Parameter: analyzer
  * Type: string
  * Description: The analyzer to use for the query string
* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcard and prefix queries should be analyzed (default: false)
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
* Parameter: df
  * Type: string
  * Description: The field to use as default where no field prefix is given in the query string
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
* Parameter: rewrite
  * Type: boolean
  * Description: Provide a more detailed explanation showing the actual Lucene query that will be executed.
* Parameter: all_shards
  * Type: boolean
  * Description: Execute validation on all shards instead of one random shard per index


info
----

Returns basic information about the cluster.

ingest.delete\_pipeline
-----------------------

Deletes a pipeline.

```
DELETE _ingest/pipeline/{id}

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|
|timeout       |time|Explicit operation timeout                              |


ingest.get\_pipeline
--------------------

Returns a pipeline.

```
GET _ingest/pipeline/{id}

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|


ingest.processor\_grok
----------------------

Returns a list of the built-in patterns.

```
GET _ingest/processor/grok

```


ingest.put\_pipeline
--------------------

Creates or updates a pipeline.

```
PUT _ingest/pipeline/{id}

```


#### HTTP request body

The ingest definition

**Required**: True

#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|
|timeout       |time|Explicit operation timeout                              |


ingest.simulate
---------------

Allows to simulate a pipeline with example documents.

```
GET _ingest/pipeline/_simulate
POST _ingest/pipeline/_simulate

```


```
GET _ingest/pipeline/{id}/_simulate
POST _ingest/pipeline/{id}/_simulate

```


#### HTTP request body

The simulate definition

**Required**: True

#### URL parameters


|Parameter|Type   |Description                                                              |
|---------|-------|-------------------------------------------------------------------------|
|verbose  |boolean|Verbose mode. Display data output for each processor in executed pipeline|


mget
----

Allows to get multiple documents in one request.

```
GET {index}/_mget
POST {index}/_mget

```


```
GET {index}/{type}/_mget
POST {index}/{type}/_mget

```


#### HTTP request body

Document identifiers; can be either `docs` (containing full document information) or `ids` (when index and type is provided in the URL.

**Required**: True

#### URL parameters



* Parameter: stored_fields
  * Type: list
  * Description: A comma-separated list of stored fields to return in the response
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: realtime
  * Type: boolean
  * Description: Specify whether to perform the operation in realtime or search mode
* Parameter: refresh
  * Type: boolean
  * Description: Refresh the shard containing the document before performing the operation
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field


msearch
-------

Allows to execute several search operations in one request.

```
GET _msearch
POST _msearch

```


```
GET {index}/_msearch
POST {index}/_msearch

```


```
GET {index}/{type}/_msearch
POST {index}/{type}/_msearch

```


#### HTTP request body

The request definitions (metadata-search request definition pairs), separated by newlines

**Required**: True

#### URL parameters



* Parameter: search_type
  * Type: enum
  * Description: Search operation type
* Parameter: max_concurrent_searches
  * Type: number
  * Description: Controls the maximum number of concurrent searches the multi search api will execute
* Parameter: typed_keys
  * Type: boolean
  * Description: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
* Parameter: pre_filter_shard_size
  * Type: number
  * Description: A threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if thenumber of shards the search request expands to exceeds the threshold. This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method ie. if date filters are mandatory to match but the shard bounds and the query are disjoint.
* Parameter: max_concurrent_shard_requests
  * Type: number
  * Description: The number of concurrent shard requests each sub search executes concurrently per node. This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests
* Parameter: rest_total_hits_as_int
  * Type: boolean
  * Description: Indicates whether hits.total should be rendered as an integer or an object in the rest search response
* Parameter: ccs_minimize_roundtrips
  * Type: boolean
  * Description: Indicates whether network round-trips should be minimized as part of cross-cluster search requests execution


msearch\_template
-----------------

Allows to execute several search template operations in one request.

```
GET _msearch/template
POST _msearch/template

```


```
GET {index}/_msearch/template
POST {index}/_msearch/template

```


```
GET {index}/{type}/_msearch/template
POST {index}/{type}/_msearch/template

```


#### HTTP request body

The request definitions (metadata-search request definition pairs), separated by newlines

**Required**: True

#### URL parameters



* Parameter: search_type
  * Type: enum
  * Description: Search operation type
* Parameter: typed_keys
  * Type: boolean
  * Description: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
* Parameter: max_concurrent_searches
  * Type: number
  * Description: Controls the maximum number of concurrent searches the multi search api will execute
* Parameter: rest_total_hits_as_int
  * Type: boolean
  * Description: Indicates whether hits.total should be rendered as an integer or an object in the rest search response
* Parameter: ccs_minimize_roundtrips
  * Type: boolean
  * Description: Indicates whether network round-trips should be minimized as part of cross-cluster search requests execution


mtermvectors
------------

Returns multiple termvectors in one request.

```
GET _mtermvectors
POST _mtermvectors

```


```
GET {index}/_mtermvectors
POST {index}/_mtermvectors

```


```
GET {index}/{type}/_mtermvectors
POST {index}/{type}/_mtermvectors

```


#### HTTP request body

Define ids, documents, parameters or a list of parameters per document here. You must at least provide a list of document ids. See documentation.

**Required**: False

#### URL parameters



* Parameter: ids
  * Type: list
  * Description: A comma-separated list of documents ids. You must define ids as parameter or set ids or docs in the request body
* Parameter: term_statistics
  * Type: boolean
  * Description: Specifies if total term frequency and document frequency should be returned. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: field_statistics
  * Type: boolean
  * Description: Specifies if document count, sum of document frequencies and sum of total term frequencies should be returned. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: fields
  * Type: list
  * Description: A comma-separated list of fields to return. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: offsets
  * Type: boolean
  * Description: Specifies if term offsets should be returned. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: positions
  * Type: boolean
  * Description: Specifies if term positions should be returned. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: payloads
  * Type: boolean
  * Description: Specifies if term payloads should be returned. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random) .Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: routing
  * Type: string
  * Description: Specific routing value. Applies to all returned documents unless otherwise specified in body params or docs.
* Parameter: realtime
  * Type: boolean
  * Description: Specifies if requests are real-time as opposed to near-real-time (default: true).
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


nodes.hot\_threads
------------------

Returns information about hot threads on each node in the cluster.

```
GET _nodes/{node_id}/hot_threads

```


```
GET _cluster/nodes/hotthreads

```


```
GET _cluster/nodes/{node_id}/hotthreads

```


```
GET _nodes/{node_id}/hotthreads

```


```
GET _cluster/nodes/hot_threads

```


```
GET _cluster/nodes/{node_id}/hot_threads

```


#### URL parameters



* Parameter: interval
  * Type: time
  * Description: The interval for the second sampling of threads
* Parameter: snapshots
  * Type: number
  * Description: Number of samples of thread stacktrace (default: 10)
* Parameter: threads
  * Type: number
  * Description: Specify the number of threads to provide information for (default: 3)
* Parameter: ignore_idle_threads
  * Type: boolean
  * Description: Dont show threads that are in known-idle places, such as waiting on a socket select or pulling from an empty task queue (default: true)
* Parameter: type
  * Type: enum
  * Description: The type to sample (default: cpu)
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout


nodes.info
----------

Returns information about nodes in the cluster.

```
GET _nodes/{node_id}/{metric}

```


#### URL parameters


|Parameter    |Type   |Description                                    |
|-------------|-------|-----------------------------------------------|
|flat_settings|boolean|Return settings in flat format (default: false)|
|timeout      |time   |Explicit operation timeout                     |


nodes.reload\_secure\_settings
------------------------------

Reloads secure settings.

```
POST _nodes/reload_secure_settings

```


```
POST _nodes/{node_id}/reload_secure_settings

```


#### HTTP request body

An object containing the password for the elasticsearch keystore

**Required**: False

#### URL parameters


|Parameter|Type|Description               |
|---------|----|--------------------------|
|timeout  |time|Explicit operation timeout|


nodes.stats
-----------

Returns statistical information about nodes in the cluster.

```
GET _nodes/{node_id}/stats

```


```
GET _nodes/stats/{metric}

```


```
GET _nodes/{node_id}/stats/{metric}

```


```
GET _nodes/stats/{metric}/{index_metric}

```


```
GET _nodes/{node_id}/stats/{metric}/{index_metric}

```


#### URL parameters



* Parameter: completion_fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata and suggest index metric (supports wildcards)
* Parameter: fielddata_fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata index metric (supports wildcards)
* Parameter: fields
  * Type: list
  * Description: A comma-separated list of fields for fielddata and completion index metric (supports wildcards)
* Parameter: groups
  * Type: boolean
  * Description: A comma-separated list of search groups for search index metric
* Parameter: level
  * Type: enum
  * Description: Return indices stats aggregated at index, node or shard level
* Parameter: types
  * Type: list
  * Description: A comma-separated list of document types for the indexing index metric
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: include_segment_file_sizes
  * Type: boolean
  * Description: Whether to report the aggregated disk usage of each one of the Lucene index files (only applies if segment stats are requested)


nodes.usage
-----------

Returns low-level information about REST actions usage on nodes.

```
GET _nodes/{node_id}/usage

```


```
GET _nodes/usage/{metric}

```


```
GET _nodes/{node_id}/usage/{metric}

```


#### URL parameters


|Parameter|Type|Description               |
|---------|----|--------------------------|
|timeout  |time|Explicit operation timeout|


ping
----

Returns whether the cluster is running.

put\_script
-----------

Creates or updates a script.

```
PUT _scripts/{id}
POST _scripts/{id}

```


```
PUT _scripts/{id}/{context}
POST _scripts/{id}/{context}

```


#### HTTP request body

The document

**Required**: True

#### URL parameters


|Parameter     |Type  |Description                             |
|--------------|------|----------------------------------------|
|timeout       |time  |Explicit operation timeout              |
|master_timeout|time  |Specify timeout for connection to master|
|context       |string|Context name to compile script against  |


rank\_eval
----------

Allows to evaluate the quality of ranked search results over a set of typical search queries

```
GET _rank_eval
POST _rank_eval

```


```
GET {index}/_rank_eval
POST {index}/_rank_eval

```


#### HTTP request body

The ranking evaluation search definition, including search requests, document ratings and ranking metric definition.

**Required**: True

#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: search_type
  * Type: enum
  * Description: Search operation type


reindex
-------

Allows to copy documents from one index to another, optionally filtering the source documents by a query, changing the destination index settings, or fetching the documents from a remote cluster.

#### HTTP request body

The search definition using the Query DSL and the prototype for the index request.

**Required**: True

#### URL parameters



* Parameter: refresh
  * Type: boolean
  * Description: Should the affected indexes be refreshed?
  * : 
* Parameter: timeout
  * Type: time
  * Description: Time each individual bulk request should wait for shards that are unavailable.
  * : 
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the reindex operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
  * : 
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should the request should block until the reindex is complete.
  * : 
* Parameter: requests_per_second
  * Type: number
  * Description: The throttle to set on this request in sub-requests per second. -1 means no throttle.
  * : 
* Parameter: scroll
  * Type: time
  * Description: Control how long to keep the search context alive
  * : 
* Parameter: slices
  * Type: number
  * Description: string
  * : The number of slices this task should be divided into. Defaults to 1, meaning the task isnt sliced into subtasks. Can be set to auto.
* Parameter: max_docs
  * Type: number
  * Description: Maximum number of documents to process (default: all documents)
  * : 


reindex\_rethrottle
-------------------

Changes the number of requests per second for a particular Reindex operation.

```
POST _reindex/{task_id}/_rethrottle

```


#### URL parameters



* Parameter: requests_per_second
  * Type: number
  * Description: The throttle to set on this request in floating sub-requests per second. -1 means set no throttle.


render\_search\_template
------------------------

Allows to use the Mustache language to pre-render a search definition.

```
GET _render/template
POST _render/template

```


```
GET _render/template/{id}
POST _render/template/{id}

```


#### HTTP request body

The search definition template and its params

scripts\_painless\_execute
--------------------------

Allows an arbitrary script to be executed and a result to be returned

```
GET _scripts/painless/_execute
POST _scripts/painless/_execute

```


#### HTTP request body

The script to execute

Allows to retrieve a large numbers of results from a single search request.

```
GET _search/scroll
POST _search/scroll

```


```
GET _search/scroll/{scroll_id}
POST _search/scroll/{scroll_id}

```


#### HTTP request body

The scroll ID if not passed by URL or query parameter.

#### URL parameters



* Parameter: scroll
  * Type: time
  * Description: Specify how long a consistent view of the index should be maintained for scrolled search
* Parameter: scroll_id
  * Type: string
  * Description: The scroll ID for scrolled search
* Parameter: rest_total_hits_as_int
  * Type: boolean
  * Description: Indicates whether hits.total should be rendered as an integer or an object in the rest search response


search
------

Returns results matching a query.

```
GET {index}/_search
POST {index}/_search

```


```
GET {index}/{type}/_search
POST {index}/{type}/_search

```


#### HTTP request body

The search definition using the Query DSL

#### URL parameters



* Parameter: analyzer
  * Type: string
  * Description: The analyzer to use for the query string
* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcard and prefix queries should be analyzed (default: false)
* Parameter: ccs_minimize_roundtrips
  * Type: boolean
  * Description: Indicates whether network round-trips should be minimized as part of cross-cluster search requests execution
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
* Parameter: df
  * Type: string
  * Description: The field to use as default where no field prefix is given in the query string
* Parameter: explain
  * Type: boolean
  * Description: Specify whether to return detailed information about score computation as part of a hit
* Parameter: stored_fields
  * Type: list
  * Description: A comma-separated list of stored fields to return as part of a hit
* Parameter: docvalue_fields
  * Type: list
  * Description: A comma-separated list of fields to return as the docvalue representation of a field for each hit
* Parameter: from
  * Type: number
  * Description: Starting offset (default: 0)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: ignore_throttled
  * Type: boolean
  * Description: Whether specified concrete, expanded or aliased indices should be ignored when throttled
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
* Parameter: routing
  * Type: list
  * Description: A comma-separated list of specific routing values
* Parameter: scroll
  * Type: time
  * Description: Specify how long a consistent view of the index should be maintained for scrolled search
* Parameter: search_type
  * Type: enum
  * Description: Search operation type
* Parameter: size
  * Type: number
  * Description: Number of hits to return (default: 10)
* Parameter: sort
  * Type: list
  * Description: A comma-separated list of : pairs
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: terminate_after
  * Type: number
  * Description: The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate early.
* Parameter: stats
  * Type: list
  * Description: Specific tag of the request for logging and statistical purposes
* Parameter: suggest_field
  * Type: string
  * Description: Specify which field to use for suggestions
* Parameter: suggest_mode
  * Type: enum
  * Description: Specify suggest mode
* Parameter: suggest_size
  * Type: number
  * Description: How many suggestions to return in response
* Parameter: suggest_text
  * Type: string
  * Description: The source text for which the suggestions should be returned
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: track_scores
  * Type: boolean
  * Description: Whether to calculate and return scores even if they are not used for sorting
* Parameter: track_total_hits
  * Type: boolean
  * Description: Indicate if the number of documents that match the query should be tracked
* Parameter: allow_partial_search_results
  * Type: boolean
  * Description: Indicate if an error should be returned if there is a partial search failure or timeout
* Parameter: typed_keys
  * Type: boolean
  * Description: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
* Parameter: version
  * Type: boolean
  * Description: Specify whether to return document version as part of a hit
* Parameter: seq_no_primary_term
  * Type: boolean
  * Description: Specify whether to return sequence number and primary term of the last modification of each hit
* Parameter: request_cache
  * Type: boolean
  * Description: Specify if request cache should be used for this request or not, defaults to index level setting
* Parameter: batched_reduce_size
  * Type: number
  * Description: The number of shard results that should be reduced at once on the coordinating node. This value should be used as a protection mechanism to reduce the memory overhead per search request if the potential number of shards in the request can be large.
* Parameter: max_concurrent_shard_requests
  * Type: number
  * Description: The number of concurrent shard requests per node this search executes concurrently. This value should be used to limit the impact of the search on the cluster in order to limit the number of concurrent shard requests
* Parameter: pre_filter_shard_size
  * Type: number
  * Description: A threshold that enforces a pre-filter roundtrip to prefilter search shards based on query rewriting if thenumber of shards the search request expands to exceeds the threshold. This filter roundtrip can limit the number of shards significantly if for instance a shard can not match any documents based on its rewrite method ie. if date filters are mandatory to match but the shard bounds and the query are disjoint.
* Parameter: rest_total_hits_as_int
  * Type: boolean
  * Description: Indicates whether hits.total should be rendered as an integer or an object in the rest search response


search\_shards
--------------

Returns information about the indices and shards that a search request would be executed against.

```
GET _search_shards
POST _search_shards

```


```
GET {index}/_search_shards
POST {index}/_search_shards

```


#### URL parameters



* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.


search\_template
----------------

Allows to use the Mustache language to pre-render a search definition.

```
GET _search/template
POST _search/template

```


```
GET {index}/_search/template
POST {index}/_search/template

```


```
GET {index}/{type}/_search/template
POST {index}/{type}/_search/template

```


#### HTTP request body

The search definition template and its params

**Required**: True

#### URL parameters



* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
* Parameter: ignore_throttled
  * Type: boolean
  * Description: Whether specified concrete, expanded or aliased indices should be ignored when throttled
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
* Parameter: routing
  * Type: list
  * Description: A comma-separated list of specific routing values
* Parameter: scroll
  * Type: time
  * Description: Specify how long a consistent view of the index should be maintained for scrolled search
* Parameter: search_type
  * Type: enum
  * Description: Search operation type
* Parameter: explain
  * Type: boolean
  * Description: Specify whether to return detailed information about score computation as part of a hit
* Parameter: profile
  * Type: boolean
  * Description: Specify whether to profile the query execution
* Parameter: typed_keys
  * Type: boolean
  * Description: Specify whether aggregation and suggester names should be prefixed by their respective types in the response
* Parameter: rest_total_hits_as_int
  * Type: boolean
  * Description: Indicates whether hits.total should be rendered as an integer or an object in the rest search response
* Parameter: ccs_minimize_roundtrips
  * Type: boolean
  * Description: Indicates whether network round-trips should be minimized as part of cross-cluster search requests execution


snapshot.cleanup\_repository
----------------------------

Removes stale data from repository.

```
POST _snapshot/{repository}/_cleanup

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|
|timeout       |time|Explicit operation timeout                              |


snapshot.clone
--------------

Clones indices from one snapshot into another snapshot in the same repository.

```
PUT _snapshot/{repository}/{snapshot}/_clone/{target_snapshot}

```


#### HTTP request body

The snapshot clone definition

**Required**: True

#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|


snapshot.create
---------------

Creates a snapshot in a repository.

```
PUT _snapshot/{repository}/{snapshot}
POST _snapshot/{repository}/{snapshot}

```


#### HTTP request body

The snapshot definition

**Required**: False

#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should this request wait until the operation has completed before returning


snapshot.create\_repository
---------------------------

Creates a repository.

```
PUT _snapshot/{repository}
POST _snapshot/{repository}

```


#### HTTP request body

The repository definition

**Required**: True

#### URL parameters


|Parameter     |Type   |Description                                             |
|--------------|-------|--------------------------------------------------------|
|master_timeout|time   |Explicit operation timeout for connection to master node|
|timeout       |time   |Explicit operation timeout                              |
|verify        |boolean|Whether to verify the repository after creation         |


snapshot.delete
---------------

Deletes a snapshot.

```
DELETE _snapshot/{repository}/{snapshot}

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|


snapshot.delete\_repository
---------------------------

Deletes a repository.

```
DELETE _snapshot/{repository}

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|
|timeout       |time|Explicit operation timeout                              |


snapshot.get
------------

Returns information about a snapshot.

```
GET _snapshot/{repository}/{snapshot}

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether to ignore unavailable snapshots, defaults to false which means a SnapshotMissingException is thrown
* Parameter: verbose
  * Type: boolean
  * Description: Whether to show verbose snapshot info or only show the basic info found in the repository index blob


snapshot.get\_repository
------------------------

Returns information about a repository.

```
GET _snapshot/{repository}

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: local
  * Type: boolean
  * Description: Return local information, do not retrieve the state from master node (default: false)


snapshot.restore
----------------

Restores a snapshot.

```
POST _snapshot/{repository}/{snapshot}/_restore

```


#### HTTP request body

Details of what to restore

**Required**: False

#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should this request wait until the operation has completed before returning


snapshot.status
---------------

Returns information about the status of a snapshot.

```
GET _snapshot/{repository}/_status

```


```
GET _snapshot/{repository}/{snapshot}/_status

```


#### URL parameters



* Parameter: master_timeout
  * Type: time
  * Description: Explicit operation timeout for connection to master node
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether to ignore unavailable snapshots, defaults to false which means a SnapshotMissingException is thrown


snapshot.verify\_repository
---------------------------

Verifies a repository.

```
POST _snapshot/{repository}/_verify

```


#### URL parameters


|Parameter     |Type|Description                                             |
|--------------|----|--------------------------------------------------------|
|master_timeout|time|Explicit operation timeout for connection to master node|
|timeout       |time|Explicit operation timeout                              |


tasks.cancel
------------

Cancels a task, if it can be cancelled through an API.

```
POST _tasks/{task_id}/_cancel

```


#### URL parameters



* Parameter: nodes
  * Type: list
  * Description: A comma-separated list of node IDs or names to limit the returned information; use _local to return information from the node youre connecting to, leave empty to get information from all nodes
* Parameter: actions
  * Type: list
  * Description: A comma-separated list of actions that should be cancelled. Leave empty to cancel all.
* Parameter: parent_task_id
  * Type: string
  * Description: Cancel tasks with specified parent task id (node_id:task_number). Set to -1 to cancel all.
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should the request block until the cancellation of the task and its descendant tasks is completed. Defaults to false


tasks.get
---------

Returns information about a task.

#### URL parameters


|Parameter          |Type   |Description                                             |
|-------------------|-------|--------------------------------------------------------|
|wait_for_completion|boolean|Wait for the matching tasks to complete (default: false)|
|timeout            |time   |Explicit operation timeout                              |


tasks.list
----------

Returns a list of tasks.

#### URL parameters



* Parameter: nodes
  * Type: list
  * Description: A comma-separated list of node IDs or names to limit the returned information; use _local to return information from the node youre connecting to, leave empty to get information from all nodes
* Parameter: actions
  * Type: list
  * Description: A comma-separated list of actions that should be returned. Leave empty to return all.
* Parameter: detailed
  * Type: boolean
  * Description: Return detailed task information (default: false)
* Parameter: parent_task_id
  * Type: string
  * Description: Return tasks with specified parent task id (node_id:task_number). Set to -1 to return all.
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Wait for the matching tasks to complete (default: false)
* Parameter: group_by
  * Type: enum
  * Description: Group tasks by nodes or parent/child relationships
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout


termvectors
-----------

Returns information and statistics about terms in the fields of a particular document.

```
GET {index}/_termvectors/{id}
POST {index}/_termvectors/{id}

```


```
GET {index}/_termvectors
POST {index}/_termvectors

```


```
GET {index}/{type}/{id}/_termvectors
POST {index}/{type}/{id}/_termvectors

```


```
GET {index}/{type}/_termvectors
POST {index}/{type}/_termvectors

```


#### HTTP request body

Define parameters and or supply a document to get termvectors for. See documentation.

**Required**: False

#### URL parameters



* Parameter: term_statistics
  * Type: boolean
  * Description: Specifies if total term frequency and document frequency should be returned.
* Parameter: field_statistics
  * Type: boolean
  * Description: Specifies if document count, sum of document frequencies and sum of total term frequencies should be returned.
* Parameter: fields
  * Type: list
  * Description: A comma-separated list of fields to return.
* Parameter: offsets
  * Type: boolean
  * Description: Specifies if term offsets should be returned.
* Parameter: positions
  * Type: boolean
  * Description: Specifies if term positions should be returned.
* Parameter: payloads
  * Type: boolean
  * Description: Specifies if term payloads should be returned.
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random).
* Parameter: routing
  * Type: string
  * Description: Specific routing value.
* Parameter: realtime
  * Type: boolean
  * Description: Specifies if request is real-time as opposed to near-real-time (default: true).
* Parameter: version
  * Type: number
  * Description: Explicit version number for concurrency control
* Parameter: version_type
  * Type: enum
  * Description: Specific version type


update
------

Updates a document with a script or partial document.

```
POST {index}/_update/{id}

```


```
POST {index}/{type}/{id}/_update

```


#### HTTP request body

The request definition requires either `script` or partial `doc`

**Required**: True

#### URL parameters



* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the update operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
* Parameter: lang
  * Type: string
  * Description: The script language (default: painless)
* Parameter: refresh
  * Type: enum
  * Description: If true then refresh the affected shards to make this operation visible to search, if wait_for then wait for a refresh to make this operation visible to search, if false (the default) then do nothing with refreshes.
* Parameter: retry_on_conflict
  * Type: number
  * Description: Specify how many times should the operation be retried when a conflict occurs (default: 0)
* Parameter: routing
  * Type: string
  * Description: Specific routing value
* Parameter: timeout
  * Type: time
  * Description: Explicit operation timeout
* Parameter: if_seq_no
  * Type: number
  * Description: only perform the update operation if the last operation that has changed the document has the specified sequence number
* Parameter: if_primary_term
  * Type: number
  * Description: only perform the update operation if the last operation that has changed the document has the specified primary term
* Parameter: require_alias
  * Type: boolean
  * Description: When true, requires destination is an alias. Default is false


update\_by\_query
-----------------

Performs an update on every document in the index without changing the source, for example to pick up a mapping change.

```
POST {index}/_update_by_query

```


```
POST {index}/{type}/_update_by_query

```


#### HTTP request body

The search definition using the Query DSL

#### URL parameters



* Parameter: analyzer
  * Type: string
  * Description: The analyzer to use for the query string
  * : 
* Parameter: analyze_wildcard
  * Type: boolean
  * Description: Specify whether wildcard and prefix queries should be analyzed (default: false)
  * : 
* Parameter: default_operator
  * Type: enum
  * Description: The default operator for query string query (AND or OR)
  * : 
* Parameter: df
  * Type: string
  * Description: The field to use as default where no field prefix is given in the query string
  * : 
* Parameter: from
  * Type: number
  * Description: Starting offset (default: 0)
  * : 
* Parameter: ignore_unavailable
  * Type: boolean
  * Description: Whether specified concrete indices should be ignored when unavailable (missing or closed)
  * : 
* Parameter: allow_no_indices
  * Type: boolean
  * Description: Whether to ignore if a wildcard indices expression resolves into no concrete indices. (This includes _all string or when no indices have been specified)
  * : 
* Parameter: conflicts
  * Type: enum
  * Description: What to do when the update by query hits version conflicts?
  * : 
* Parameter: expand_wildcards
  * Type: enum
  * Description: Whether to expand wildcard expression to concrete indices that are open, closed or both.
  * : 
* Parameter: lenient
  * Type: boolean
  * Description: Specify whether format-based query failures (such as providing text to a numeric field) should be ignored
  * : 
* Parameter: pipeline
  * Type: string
  * Description: Ingest pipeline to set on index requests made by this action. (default: none)
  * : 
* Parameter: preference
  * Type: string
  * Description: Specify the node or shard the operation should be performed on (default: random)
  * : 
* Parameter: q
  * Type: string
  * Description: Query in the Lucene query string syntax
  * : 
* Parameter: routing
  * Type: list
  * Description: A comma-separated list of specific routing values
  * : 
* Parameter: scroll
  * Type: time
  * Description: Specify how long a consistent view of the index should be maintained for scrolled search
  * : 
* Parameter: search_type
  * Type: enum
  * Description: Search operation type
  * : 
* Parameter: search_timeout
  * Type: time
  * Description: Explicit timeout for each search request. Defaults to no timeout.
  * : 
* Parameter: size
  * Type: number
  * Description: Deprecated, please use max_docs instead
  * : 
* Parameter: max_docs
  * Type: number
  * Description: Maximum number of documents to process (default: all documents)
  * : 
* Parameter: sort
  * Type: list
  * Description: A comma-separated list of : pairs
  * : 
* Parameter: _source
  * Type: list
  * Description: True or false to return the _source field or not, or a list of fields to return
  * : 
* Parameter: _source_excludes
  * Type: list
  * Description: A list of fields to exclude from the returned _source field
  * : 
* Parameter: _source_includes
  * Type: list
  * Description: A list of fields to extract and return from the _source field
  * : 
* Parameter: terminate_after
  * Type: number
  * Description: The maximum number of documents to collect for each shard, upon reaching which the query execution will terminate early.
  * : 
* Parameter: stats
  * Type: list
  * Description: Specific tag of the request for logging and statistical purposes
  * : 
* Parameter: version
  * Type: boolean
  * Description: Specify whether to return document version as part of a hit
  * : 
* Parameter: version_type
  * Type: boolean
  * Description: Should the document increment the version number (internal) on hit or not (reindex)
  * : 
* Parameter: request_cache
  * Type: boolean
  * Description: Specify if request cache should be used for this request or not, defaults to index level setting
  * : 
* Parameter: refresh
  * Type: boolean
  * Description: Should the affected indexes be refreshed?
  * : 
* Parameter: timeout
  * Type: time
  * Description: Time each individual bulk request should wait for shards that are unavailable.
  * : 
* Parameter: wait_for_active_shards
  * Type: string
  * Description: Sets the number of shard copies that must be active before proceeding with the update by query operation. Defaults to 1, meaning the primary shard only. Set to all for all shard copies, otherwise set to any non-negative value less than or equal to the total number of copies for the shard (number of replicas + 1)
  * : 
* Parameter: scroll_size
  * Type: number
  * Description: Size on the scroll request powering the update by query
  * : 
* Parameter: wait_for_completion
  * Type: boolean
  * Description: Should the request should block until the update by query operation is complete.
  * : 
* Parameter: requests_per_second
  * Type: number
  * Description: The throttle to set on this request in sub-requests per second. -1 means no throttle.
  * : 
* Parameter: slices
  * Type: number
  * Description: string
  * : The number of slices this task should be divided into. Defaults to 1, meaning the task isnt sliced into subtasks. Can be set to auto.


update\_by\_query\_rethrottle
-----------------------------

Changes the number of requests per second for a particular Update By Query operation.

```
POST _update_by_query/{task_id}/_rethrottle

```


#### URL parameters



* Parameter: requests_per_second
  * Type: number
  * Description: The throttle to set on this request in floating sub-requests per second. -1 means set no throttle.

================
File: fix_tests.sh
================
#!/bin/bash

# Run `go test ./...`
go test ./...
exit_code=$?

# Check if the exit code is not 0
if [ $exit_code -ne 0 ]; then
  # Loop up to a maximum of 25 times
  for i in {1..25}; do
    echo "Repomix..."
    repomix

    echo "Attempt $i: Running aider to fix the error..."

    # Run the aider command with the specified options
    OUTPUT=$(go test ./...) # Capture the output from `go test ./...`
    aider --sonnet --dark-mode --no-cache-prompts --no-auto-commits \
          --no-suggest-shell-commands --yes --yes-always --read repomix-output.txt\
          --message "Fix Error: $OUTPUT"

    # Re-run the tests after fixing
    go test ./...
    exit_code=$?

    # If the test passes, break out of the loop
    if [ $exit_code -eq 0 ]; then
      echo "Tests passed on attempt $i."
      break
    fi

    # If maximum attempts are reached, exit the loop
    if [ $i -eq 25 ]; then
      echo "Maximum attempts reached. Tests still failing."
      exit 1
    fi
  done
else
  echo "Tests passed successfully."
fi

================
File: go.mod
================
module my-indexer

go 1.21

================
File: main.go
================
package main

import (
	"context"
	"log"
	"net/http"
	"os"
	"os/signal"
	"syscall"
	"time"

	"my-indexer/router"
)

func main() {
	// Get port from environment variable or use default
	port := os.Getenv("PORT")
	if port == "" {
		port = "8080"
	}

	r := router.NewRouter()
	
	// Configure server
	srv := &http.Server{
		Addr:    ":" + port,
		Handler: r,
	}

	// Server run context
	srvCtx, srvCancel := context.WithCancel(context.Background())
	defer srvCancel()

	// Channel to listen for errors coming from the listener
	serverErrors := make(chan error, 1)

	// Start server
	go func() {
		log.Printf("Starting server on port %s", port)
		if err := srv.ListenAndServe(); err != nil && err != http.ErrServerClosed {
			serverErrors <- err
		}
	}()

	// Listen for syscall signals for process to interrupt/quit
	sig := make(chan os.Signal, 1)
	signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)

	// Wait for signals
	select {
	case err := <-serverErrors:
		log.Printf("Server error: %v", err)
	case s := <-sig:
		log.Printf("Server shutdown initiated by %v signal", s)
	}

	// Shutdown signal with grace period of 30 seconds
	shutdownCtx, shutdownCancel := context.WithTimeout(srvCtx, 30*time.Second)
	defer shutdownCancel()

	// Trigger graceful shutdown
	if err := srv.Shutdown(shutdownCtx); err != nil {
		log.Printf("Graceful shutdown failed: %v", err)
	} else {
		log.Println("Server shutdown completed")
	}
}

================
File: Makefile
================
.PHONY: build test run stop

# Build the docker image
build:
	docker build -t my-indexer .

# Run tests in docker container
test: build
	docker run --rm my-indexer

# Run the application
run: build
	docker run -d --name my-indexer-app my-indexer

# Stop the running container
stop:
	docker stop my-indexer-app || true
	docker rm my-indexer-app || true

================
File: README.md
================
# My-Indexer: A High-Performance Full-Text Search Engine in Go

![Robot Indexer](robot-indexer.jpg)

My-Indexer is a lightweight, concurrent full-text search engine written in Go. It provides fast and efficient text indexing and searching capabilities with support for complex queries, making it ideal for applications that need embedded search functionality.

## Features

- **Full-Text Search**: Support for term, phrase, and field-specific queries
- **Complex Query Support**: Boolean operators (AND, OR) for advanced search combinations
- **Concurrent Operations**: Thread-safe design supporting multiple simultaneous readers and writers
- **Transaction Support**: ACID-compliant operations with rollback capability
- **Crash Recovery**: Built-in transaction logging for durability
- **Custom Analyzers**: Flexible text analysis with customizable filters
- **Memory Efficient**: Optimized for low memory footprint
- **Pure Go Implementation**: No external dependencies required

## Comparison with SQLite FTS

While SQLite FTS is an excellent choice for many applications, My-Indexer offers several advantages:

| Feature | My-Indexer | SQLite FTS |
|---------|------------|------------|
| Memory Usage | Optimized in-memory index | Disk-based storage |
| Concurrency | Native Go concurrency with multiple readers/writers | Single writer, multiple readers |
| Query Types | Custom query language with field-specific searches | SQL-based queries |
| Customization | Extensible analyzers and filters | Limited customization options |
| Integration | Native Go API | Requires SQLite bindings |
| Dependencies | Zero external dependencies | Requires SQLite library |

## Installation

```bash
go get github.com/yourusername/my-indexer
```

## Quick Start

```go
package main

import (
    "fmt"
    "github.com/yourusername/my-indexer/index"
)

func main() {
    // Create a new index
    idx := index.NewIndex()

    // Add a document
    doc := index.NewDocument()
    doc.AddField("title", "Go Programming")
    doc.AddField("content", "Go is a statically typed, compiled programming language.")
    idx.AddDocument(doc)

    // Search the index
    query := "programming language"
    results, err := idx.Search(query)
    if err != nil {
        panic(err)
    }

    // Print results
    for _, result := range results {
        fmt.Printf("Found document: %v\n", result)
    }
}
```

## Query Syntax

My-Indexer supports a rich query syntax:

- **Term Query**: `programming`
- **Phrase Query**: `"go programming"`
- **Field Query**: `title:golang`
- **Boolean Queries**:
  - AND: `go AND programming`
  - OR: `golang OR rust`

## Building from Source

1. Clone the repository:
```bash
git clone https://github.com/yourusername/my-indexer.git
cd my-indexer
```

2. Build the project:
```bash
make build
```

3. Run tests:
```bash
make test
```

## Project Structure

```
my-indexer/
 analysis/       # Text analysis and tokenization
 document/      # Document representation
 index/         # Core indexing functionality
 query/         # Query parsing and execution
 search/        # Search implementation
 storage/       # Storage management
 txlog/         # Transaction logging
```

## Performance

My-Indexer is designed for high performance:

- Concurrent read/write operations
- Optimized in-memory index structure
- Efficient query execution
- Low memory footprint

## Contributing

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add some amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Acknowledgments

- Inspired by Lucene's architecture and SQLite FTS
- Built with Go's excellent concurrency primitives
- Special thanks to all contributors

## Contact

If you have any questions or suggestions, please open an issue on GitHub.

================
File: steps.txt
================
Step 1: Analyze ElasticSearch API

		Task: ElasticSearch APIs to support:
   Endpoints: create document, update document, get document, delete document, search documents, 
  * Bulk operations: batch indexing, batch updating, batch deleting.
  * list indexes
  * multi-search
  * scroll api
		Implementation:
	1.	Read ElasticSearch documentation for detailed API contracts and expected behavior from elasticsearch_api_ref.md in this project.
	2.	Define these APIs in your project as Go interfaces, aligning with the ElasticSearch standards.
		Verification:
		Write unit tests to validate API endpoint registration.
		Mock an HTTP server to validate basic request/response flow.

Step 2: Design an ElasticSearch-Compatible Router

		Task: Create a routing layer to support both your existing APIs and ElasticSearch-compatible endpoints.
		Implementation:
	1.	Use Gos http.ServeMux or a third-party router (if allowed).
	2.	Define handlers for ElasticSearch-compatible endpoints.
	3.	Ensure handlers validate requests according to ElasticSearch API.
  run 'make test' to validate changes

func setupRouter() *http.ServeMux {
    mux := http.NewServeMux()
    // Existing APIs
    mux.HandleFunc("/api/document", documentHandler)
    // ElasticSearch APIs
    mux.HandleFunc("/_search", elasticSearchHandler)
    mux.HandleFunc("/_index", elasticIndexHandler)
    return mux
}

		Unit Test:
		Mock HTTP requests for /api/document and ElasticSearch endpoints (/_search, /_index).
		Assert correct routing and response codes.
		Explanation: The router ensures seamless integration of new endpoints without affecting existing ones.

  Step 3: Implement ElasticSearch-Compatible Indexing (/_index)

		Task: Create a handler for the _index API to index documents in ElasticSearchs JSON structure.
		Implementation:
	1.	Parse the ElasticSearch document format.
	2.	Convert and add the document to your existing indexing system.
run 'make test' to validate changes
		Code:

func elasticIndexHandler(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodPut && r.Method != http.MethodPost {
        http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
        return
    }
    var doc map[string]interface{}
    if err := json.NewDecoder(r.Body).Decode(&doc); err != nil {
        http.Error(w, "Invalid JSON", http.StatusBadRequest)
        return
    }
    // Convert and index the document
    if err := indexDocument(doc); err != nil {
        http.Error(w, err.Error(), http.StatusInternalServerError)
        return
    }
    w.WriteHeader(http.StatusCreated)
}

	Unit Test:
		Test valid and invalid requests (e.g., valid JSON, missing fields).
		Assert document is correctly indexed into the system.
		Explanation:
This handler allows documents in ElasticSearch format to be ingested into your custom indexer.

Step 4: Implement ElasticSearch-Compatible Search (/_search)
Step 4.1: Parse the DSL Query

Task:

		Implement logic to parse ElasticSearchs Query DSL (Domain-Specific Language) format into a Go struct.

Implementation:

	1.	Define a struct to represent supported query types (e.g., match, term, range, etc.).
	2.	Parse incoming JSON into this struct.
  This step ensures the handler can interpret queries in ElasticSearch DSL format.
  Run 'make test' to validate changes


Step 4.2: Map DSL Components to Internal Query Structures

Task:

		Translate parsed DSL queries into your internal query representation used by the indexer.

Implementation:

	1.	Define an internal query struct.
	2.	Map supported DSL queries (match, term) to your query struct.



Step 4.3: Execute the Internal Query

Task:

		Perform a search using your indexers query engine.

Implementation:

	1.	Use the internal query struct to retrieve results from the index.
	2.	Return a result set compatible with ElasticSearch.
		This step integrates with the indexers existing query engine to retrieve relevant documents.


Step 4.4: Format Results in ElasticSearch-Compatible JSON

Task:

		Convert query results into a JSON format expected by ElasticSearch clients.

Implementation:

	1.	Define a struct for the response format.
	2.	Populate the response with the query results.
		This ensures clients receive results formatted as if from ElasticSearch.


Step 4.5: Integrate Everything in the Handler

Task:

		Combine parsing, mapping, executing, and formatting in the /search endpoint.
func elasticSearchHandler(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodPost {
        http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
        return
    }
    body, err := io.ReadAll(r.Body)
    if err != nil {
        http.Error(w, "Failed to read body", http.StatusBadRequest)
        return
    }
    searchRequest, err := parseDSLQuery(body)
    if err != nil {
        http.Error(w, "Invalid JSON", http.StatusBadRequest)
        return
    }
    internalQuery := mapToInternalQuery(searchRequest)
    results, err := executeInternalQuery(internalQuery)
    if err != nil {
        http.Error(w, "Search execution failed", http.StatusInternalServerError)
        return
    }
    response := formatElasticSearchResponse(results)
    json.NewEncoder(w).Encode(response)
}

Unit Test:

		Mock a full /search request with JSON input and validate the response.





==============

Step 5: Add _bulk API Support

		Task: Implement the _bulk API for batch operations (indexing, updating, deleting).
		Implementation:
	1.	Parse the bulk operation payload.
	2.	Process each operation sequentially or concurrently.
run 'make test' to validate changes
		Code:

  func elasticBulkHandler(w http.ResponseWriter, r *http.Request) {
    if r.Method != http.MethodPost {
        http.Error(w, "Invalid method", http.StatusMethodNotAllowed)
        return
    }
    operations := parseBulkPayload(r.Body)
    results := processBulkOperations(operations)
    json.NewEncoder(w).Encode(results)
}

	Unit Test:
		Validate bulk payload parsing.
		Assert batch operations (index, delete, update) work correctly.
		Test edge cases (e.g., empty payload, invalid operation).
		Explanation:
This feature allows efficient batch processing, mirroring ElasticSearch behavior.

Step 6: Extend Unit Tests for API Contracts

		Task: Validate compatibility with ElasticSearch using comprehensive unit and integration tests.
		Implementation:
		Write tests to cover all API functionality and edge cases.
		Mock ElasticSearch-compatible clients to simulate real-world usage.
		Test concurrency, large payloads, and malformed requests.
run 'make test' to validate changes
		Code:

  func TestElasticSearchAPI(t *testing.T) {
    // Test cases for each API (_index, _search, _bulk)
    // Assert expected response formats and status codes
}

Step 7: Document API Extensions

		Task: Update project documentation to include ElasticSearch-compatible APIs.
		Implementation:
		Provide examples for using the new APIs.
		Document request/response formats and expected behavior.
run 'make test' to validate changes

  Step 8: Dockerize and Validate

		Task: Update the Docker setup to run the extended system.
		Implementation:
		Add scripts to the Makefile for starting the API server and running tests.
run 'make test' to validate changes

  Additional Considerations:

		Performance: Test throughput with concurrent requests to new APIs.
		Error Handling: Ensure comprehensive validation and error responses.
		Future Proofing: Use modular code to easily add more ElasticSearch features.
